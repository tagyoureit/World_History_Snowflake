{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041fa73c-d568-4e2c-ad88-35100435348b",
   "metadata": {
    "language": "python",
    "name": "Setup"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Configuration\n",
    "import io\n",
    "import os\n",
    "import tempfile\n",
    "import json\n",
    "import requests\n",
    "import pypdf\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.exceptions import SnowparkSQLException\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# --- Configuration ---\n",
    "SNOWFLAKE_DATABASE = \"WORLD_HISTORY\"\n",
    "SNOWFLAKE_SCHEMA = \"public\"\n",
    "SNOWFLAKE_ROLE = \"SYSADMIN\"\n",
    "TARGET_WEBSITE_URL = \"https://glhssocialstudies.weebly.com/world-history-textbook---pdf-copy.html\"\n",
    "\n",
    "# Define stage names WITHOUT a leading '@'\n",
    "SOURCE_STAGE_NAME = \"pdf_documents\"\n",
    "\n",
    "# Dynamic names based on database\n",
    "EXTERNAL_ACCESS_INTEGRATION_NAME = f\"{SNOWFLAKE_DATABASE}_WEB_ACCESS\"\n",
    "NETWORK_RULE_NAME = f\"{SNOWFLAKE_DATABASE}_WEBSITE_ACCESS\"\n",
    "\n",
    "ADAPTIVE_SPLIT_TARGET_PATH = f\"{SOURCE_STAGE_NAME}/parts\"\n",
    "SINGLE_PAGE_TARGET_PATH = f\"{SOURCE_STAGE_NAME}/pages\"\n",
    "\n",
    "# --- Session Initialization ---\n",
    "session = get_active_session()\n",
    "session.sql(f\"CREATE DATABASE IF NOT EXISTS {SNOWFLAKE_DATABASE};\").collect()\n",
    "session.use_database(SNOWFLAKE_DATABASE)\n",
    "session.use_schema(SNOWFLAKE_SCHEMA)\n",
    "\n",
    "print(f\"‚úÖ Setup complete.\")\n",
    "print(f\"  - Current Role: {session.get_current_role()}\")\n",
    "print(f\"  - Configured Role: {SNOWFLAKE_ROLE}\")\n",
    "print(f\"  - Database: {session.get_current_database()}\")\n",
    "print(f\"  - Schema: {session.get_current_schema()}\")\n",
    "print(f\"  - Target Website: {TARGET_WEBSITE_URL}\")\n",
    "print(f\"  - Source Stage: @{SOURCE_STAGE_NAME}\")\n",
    "print(f\"  - External Access Integration: {EXTERNAL_ACCESS_INTEGRATION_NAME}\")\n",
    "print(f\"  - Network Rule: {NETWORK_RULE_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5896eafa-ef42-44e6-9704-009bd22391f2",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "# Cell 2: External Access Setup\n",
    "print(\"üîê Setting up external access with proper role management...\")\n",
    "\n",
    "# Switch to ACCOUNTADMIN for setup\n",
    "session.use_role(\"ACCOUNTADMIN\")\n",
    "print(f\"   Switched to role: {session.get_current_role()}\")\n",
    "\n",
    "# Extract domain from the configured URL\n",
    "target_domain = urlparse(TARGET_WEBSITE_URL).netloc\n",
    "print(f\"   Target domain: {target_domain}\")\n",
    "\n",
    "# IMPORTANT: Also allow icomets.org where the PDFs are actually hosted\n",
    "pdf_domain = \"icomets.org\"\n",
    "print(f\"   PDF domain: {pdf_domain}\")\n",
    "\n",
    "try:\n",
    "    # Create network rule for BOTH the target website AND the PDF hosting domain\n",
    "    session.sql(f\"\"\"\n",
    "        CREATE OR REPLACE NETWORK RULE {NETWORK_RULE_NAME}\n",
    "        MODE = EGRESS\n",
    "        TYPE = HOST_PORT\n",
    "        VALUE_LIST = ('{target_domain}:443', '{target_domain}:80', '{pdf_domain}:443', '{pdf_domain}:80')\n",
    "    \"\"\").collect()\n",
    "    print(f\"‚úÖ Network rule created: {NETWORK_RULE_NAME} for {target_domain} + {pdf_domain}\")\n",
    "    \n",
    "    # Create external access integration using configured variables\n",
    "    session.sql(f\"\"\"\n",
    "        CREATE OR REPLACE EXTERNAL ACCESS INTEGRATION {EXTERNAL_ACCESS_INTEGRATION_NAME}\n",
    "        ALLOWED_NETWORK_RULES = ({NETWORK_RULE_NAME})\n",
    "        ENABLED = TRUE\n",
    "    \"\"\").collect()\n",
    "    print(f\"‚úÖ External access integration created: {EXTERNAL_ACCESS_INTEGRATION_NAME}\")\n",
    "\n",
    "    # Grant access for the role to use the access integration\n",
    "    session.sql(f\"\"\"\n",
    "        GRANT USAGE ON INTEGRATION {EXTERNAL_ACCESS_INTEGRATION_NAME} to {SNOWFLAKE_ROLE}\n",
    "    \"\"\").collect()\n",
    "    print(f\"‚úÖ Integration {EXTERNAL_ACCESS_INTEGRATION_NAME} access granted to {SNOWFLAKE_ROLE}\")\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during setup: {str(e)}\")\n",
    "\n",
    "finally:\n",
    "    # Switch back to configured role\n",
    "    try:\n",
    "        session.use_role(SNOWFLAKE_ROLE)\n",
    "        print(f\"‚úÖ Switched back to role: {session.get_current_role()}\")\n",
    "    except Exception as role_error:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Could not switch to {SNOWFLAKE_ROLE}: {role_error}\")\n",
    "\n",
    "print(f\"üîß External access setup completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c5d602",
   "metadata": {},
   "source": [
    "# Add external integration to the notebook\n",
    "\n",
    "Now that the external access integration has been created, you need to go to Notebook Settings -> External Access and enable \"World_History_Web_Access\" or the downloads will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a28882a-4dc5-416b-84ab-5be8d789625a",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "# Create the PDF documents stage if it doesn't exist\n",
    "session.sql(f\"\"\"\n",
    "    CREATE STAGE IF NOT EXISTS {SOURCE_STAGE_NAME}\n",
    "        ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE')\n",
    "        DIRECTORY = (ENABLE = TRUE)\n",
    "        COMMENT = 'Stage for PDF documents'\n",
    "\"\"\").collect()\n",
    "print(f\"‚úÖ Stage @{SOURCE_STAGE_NAME} created/verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe7da72-4ee0-4edc-86b5-cb23f4a3a1d4",
   "metadata": {
    "collapsed": false,
    "name": "cell2"
   },
   "source": [
    "# Upload your PDF Documents to the Stage\n",
    "\n",
    "You have the option of using Snowflake to download the PDF's which requires the AccountAdmin role to enable external access to the website containing the files.\n",
    "\n",
    "Alternatively, you can use Python locally on your machine and run `python3 pdf_downloader.py` (after installing dependencies) and then can manually upload the files to the stage @PDF_DOCUMENTS in the database you created.  If you go this route, skip to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f1003f-f459-4f8d-88af-e3a3f8342623",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE FUNCTION get_pdf_links_from_website(website_url STRING)\n",
    "RETURNS VARIANT\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = '3.12'\n",
    "EXTERNAL_ACCESS_INTEGRATIONS = (WORLD_HISTORY_WEB_ACCESS)\n",
    "PACKAGES = ('requests', 'beautifulsoup4', 'lxml')\n",
    "HANDLER = 'scrape_pdfs'\n",
    "AS\n",
    "$$\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_pdfs(website_url):\n",
    "    try:\n",
    "        # Fetch webpage\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(website_url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        pdf_links = []\n",
    "        \n",
    "        # Method 1: Direct PDF links\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.lower().endswith('.pdf'):\n",
    "                full_url = urljoin(website_url, href)\n",
    "                filename = urlparse(href).path.split('/')[-1]\n",
    "                text = link.get_text(strip=True)\n",
    "                \n",
    "                pdf_links.append({\n",
    "                    'url': full_url,\n",
    "                    'text': text,\n",
    "                    'filename': filename,\n",
    "                    'method': 'direct_link'\n",
    "                })\n",
    "        \n",
    "        # Method 2: Regex patterns if no direct links found\n",
    "        if len(pdf_links) == 0:\n",
    "            content = response.text\n",
    "            pdf_patterns = [\n",
    "                r'https?://[^\"\\s]+\\.pdf',\n",
    "                r'/files/[^\"\\s]+\\.pdf',\n",
    "                r'uploads/[^\"\\s]+\\.pdf'\n",
    "            ]\n",
    "            \n",
    "            found_urls = set()\n",
    "            for pattern in pdf_patterns:\n",
    "                matches = re.findall(pattern, content, re.IGNORECASE)\n",
    "                for match in matches:\n",
    "                    url = match.strip('\"\\'')\n",
    "                    if url.startswith('/'):\n",
    "                        url = urljoin(website_url, url)\n",
    "                    found_urls.add(url)\n",
    "            \n",
    "            for i, url in enumerate(found_urls, 1):\n",
    "                filename = urlparse(url).path.split('/')[-1]\n",
    "                if not filename or not filename.endswith('.pdf'):\n",
    "                    filename = f\"chapter_{i:02d}.pdf\"\n",
    "                \n",
    "                pdf_links.append({\n",
    "                    'url': url,\n",
    "                    'text': f'Chapter {i}',\n",
    "                    'filename': filename,\n",
    "                    'method': 'regex_pattern'\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"website_url\": website_url,\n",
    "            \"pdf_count\": len(pdf_links),\n",
    "            \"pdf_links\": pdf_links,\n",
    "            \"scraped_at\": str(response.headers.get('date', 'unknown'))\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"website_url\": website_url,\n",
    "            \"pdf_count\": 0,\n",
    "            \"pdf_links\": []\n",
    "        }\n",
    "$$;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d489c5b9-f6b3-448b-8b46-d0abcf75e42b",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Test PDF Discovery\n",
    "print(f\"üåê Testing PDF discovery from: {TARGET_WEBSITE_URL}\")\n",
    "\n",
    "try:\n",
    "    query = f\"SELECT get_pdf_links_from_website('{TARGET_WEBSITE_URL}') as result\"\n",
    "    result = session.sql(query).collect()\n",
    "    \n",
    "    if result:\n",
    "        result_raw = result[0]['RESULT']\n",
    "        print(f\"üîç Raw result type: {type(result_raw)}\")\n",
    "        print(f\"üîç Raw result preview: {str(result_raw)[:100]}...\")\n",
    "        \n",
    "        # Handle both string and dict results\n",
    "        if isinstance(result_raw, str):\n",
    "            try:\n",
    "                result_data = json.loads(result_raw)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ùå Failed to parse result as JSON: {result_raw}\")\n",
    "                result_data = None\n",
    "        else:\n",
    "            result_data = result_raw\n",
    "        \n",
    "        if result_data and result_data.get('success', False):\n",
    "            pdf_links = result_data.get('pdf_links', [])\n",
    "            pdf_count = result_data.get('pdf_count', 0)\n",
    "            \n",
    "            print(f\"‚úÖ Found {pdf_count} PDF links\")\n",
    "            \n",
    "            if pdf_links:\n",
    "                print(f\"üìÑ Available PDFs:\")\n",
    "                for i, pdf_info in enumerate(pdf_links[:10], 1):  # Show first 10\n",
    "                    filename = pdf_info.get('filename', 'unknown')\n",
    "                    text = pdf_info.get('text', 'No description')[:50]\n",
    "                    print(f\"   {i:2d}. {text:<50} ‚Üí {filename}\")\n",
    "                \n",
    "                if len(pdf_links) > 10:\n",
    "                    print(f\"   ... and {len(pdf_links) - 10} more PDFs\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  No PDF links found\")\n",
    "        else:\n",
    "            error_msg = result_data.get('error', 'Unknown error') if result_data else 'No result data'\n",
    "            print(f\"‚ùå PDF discovery failed: {error_msg}\")\n",
    "    else:\n",
    "        print(\"‚ùå No result returned from PDF discovery\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during PDF discovery: {str(e)}\")\n",
    "    import traceback\n",
    "    print(f\"üîç Full traceback:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"üìù Next Steps:\")\n",
    "print(f\"1. If PDFs were found above, continue with remaining cells\")\n",
    "print(f\"2. Cell 5 will handle download and upload to @{SOURCE_STAGE_NAME}\")\n",
    "print(f\"3. Run all cells sequentially for complete processing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88860793-b7f0-470c-9997-f340cf61324d",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": [
    "# Cell 6: Diagnose Disk Space Issues\n",
    "print(\"üîç Diagnosing disk space and temp file issues...\")\n",
    "\n",
    "import shutil\n",
    "import tempfile\n",
    "import gc\n",
    "\n",
    "# Check available disk space\n",
    "try:\n",
    "    # Get temp directory info\n",
    "    temp_dir = tempfile.gettempdir()\n",
    "    print(f\"üìÅ Temp directory: {temp_dir}\")\n",
    "    \n",
    "    # Check disk usage\n",
    "    total, used, free = shutil.disk_usage(temp_dir)\n",
    "    print(f\"üíæ Disk usage in temp directory:\")\n",
    "    print(f\"   Total: {total / (1024**3):.2f} GB\")\n",
    "    print(f\"   Used:  {used / (1024**3):.2f} GB\") \n",
    "    print(f\"   Free:  {free / (1024**3):.2f} GB\")\n",
    "    print(f\"   Free:  {free / (1024**2):.1f} MB\")\n",
    "    \n",
    "    if free < 1024**3:  # Less than 1GB\n",
    "        print(f\"‚ö†Ô∏è  WARNING: Only {free / (1024**2):.1f} MB free space!\")\n",
    "        print(f\"üîß Need to optimize approach for limited disk space\")\n",
    "    \n",
    "    # Check current working directory space too\n",
    "    cwd_total, cwd_used, cwd_free = shutil.disk_usage(\".\")\n",
    "    print(f\"\\nüíæ Disk usage in current directory:\")\n",
    "    print(f\"   Free:  {cwd_free / (1024**2):.1f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error checking disk space: {e}\")\n",
    "\n",
    "# Check for any leftover temp files\n",
    "try:\n",
    "    import os\n",
    "    temp_files = []\n",
    "    for root, dirs, files in os.walk(temp_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.pdf') and 'tmp' in file:\n",
    "                filepath = os.path.join(root, file)\n",
    "                size = os.path.getsize(filepath)\n",
    "                temp_files.append((filepath, size))\n",
    "    \n",
    "    if temp_files:\n",
    "        print(f\"\\nüóÇÔ∏è  Found {len(temp_files)} leftover PDF temp files:\")\n",
    "        total_temp_size = 0\n",
    "        for filepath, size in temp_files:\n",
    "            print(f\"   üìÑ {os.path.basename(filepath)}: {size / (1024**2):.1f} MB\")\n",
    "            total_temp_size += size\n",
    "        print(f\"   üì¶ Total temp files: {total_temp_size / (1024**2):.1f} MB\")\n",
    "        \n",
    "        # Clean up old temp files\n",
    "        print(f\"üßπ Cleaning up leftover temp files...\")\n",
    "        for filepath, _ in temp_files:\n",
    "            try:\n",
    "                os.unlink(filepath)\n",
    "                print(f\"   ‚úÖ Deleted {os.path.basename(filepath)}\")\n",
    "            except Exception as cleanup_error:\n",
    "                print(f\"   ‚ùå Failed to delete {os.path.basename(filepath)}: {cleanup_error}\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ No leftover PDF temp files found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error checking temp files: {e}\")\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "print(f\"\\nüßπ Forced garbage collection\")\n",
    "\n",
    "print(f\"\\nüí° Recommendations:\")\n",
    "if free < 500 * 1024**2:  # Less than 500MB\n",
    "    print(f\"   1. Use streaming approach to avoid saving full files to disk\")\n",
    "    print(f\"   2. Process files in smaller batches\")\n",
    "    print(f\"   3. Force cleanup after each file\")\n",
    "else:\n",
    "    print(f\"   1. Should have enough space - check for cleanup issues\")\n",
    "    print(f\"   2. Monitor temp file accumulation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf42af-37b0-42ff-b01f-731b44c46adc",
   "metadata": {
    "language": "python",
    "name": "download_pdfs_to_stage"
   },
   "outputs": [],
   "source": [
    "# Cell 5: Download PDFs with Optimizations\n",
    "print(f\"üì• Downloading PDFs to @{SOURCE_STAGE_NAME}\")\n",
    "\n",
    "# Clean up existing stage files first\n",
    "try:\n",
    "    session.sql(f\"REMOVE @{SOURCE_STAGE_NAME}\").collect()\n",
    "    print(f\"üßπ Cleaned up existing stage files\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Get PDF links\n",
    "try:\n",
    "    result = session.sql(f\"SELECT get_pdf_links_from_website('{TARGET_WEBSITE_URL}') as result\").collect()\n",
    "    \n",
    "    if result:\n",
    "        result_raw = result[0]['RESULT']\n",
    "        if isinstance(result_raw, str):\n",
    "            result_data = json.loads(result_raw)\n",
    "        else:\n",
    "            result_data = result_raw\n",
    "        \n",
    "        if result_data and result_data.get('success', False):\n",
    "            pdf_links = result_data.get('pdf_links', [])\n",
    "            total_pdfs = len(pdf_links)\n",
    "            \n",
    "            print(f\"üìä Found {total_pdfs} PDFs to download\")\n",
    "            \n",
    "            success_count = 0\n",
    "            error_count = 0\n",
    "            \n",
    "            for i, pdf_info in enumerate(pdf_links, 1):\n",
    "                pdf_url = pdf_info['url']\n",
    "                filename = pdf_info['filename']\n",
    "                \n",
    "                print(f\"\\nüìÑ {i}/{total_pdfs}: {filename}\")\n",
    "                \n",
    "                # Check available space\n",
    "                try:\n",
    "                    temp_dir = tempfile.gettempdir()\n",
    "                    _, _, free_space = shutil.disk_usage(temp_dir)\n",
    "                    free_mb = free_space / (1024**2)\n",
    "                    \n",
    "                    if free_mb < 150:\n",
    "                        print(f\"   ‚ö†Ô∏è  Low space ({free_mb:.1f} MB) - cleaning up...\")\n",
    "                        # Clean up any leftover temp files\n",
    "                        for temp_file in os.listdir(temp_dir):\n",
    "                            if temp_file.endswith('.pdf') and 'tmp' in temp_file:\n",
    "                                try:\n",
    "                                    os.unlink(os.path.join(temp_dir, temp_file))\n",
    "                                except:\n",
    "                                    pass\n",
    "                        \n",
    "                        if free_mb < 100:\n",
    "                            print(f\"   ‚ùå Insufficient space - skipping\")\n",
    "                            error_count += 1\n",
    "                            continue\n",
    "                            \n",
    "                except Exception:\n",
    "                    pass\n",
    "                \n",
    "                temp_path = None\n",
    "                try:\n",
    "                    headers = {\n",
    "                        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "                    }\n",
    "                    \n",
    "                    # Streaming download\n",
    "                    with requests.get(pdf_url, headers=headers, timeout=90, stream=True) as response:\n",
    "                        response.raise_for_status()\n",
    "                        \n",
    "                        # Create temp file with correct name for proper upload\n",
    "                        temp_dir = tempfile.gettempdir()\n",
    "                        temp_path = os.path.join(temp_dir, filename)\n",
    "                        \n",
    "                        try:\n",
    "                            total_size = 0\n",
    "                            with open(temp_path, 'wb') as temp_file:\n",
    "                                for chunk in response.iter_content(chunk_size=8192):\n",
    "                                    if chunk:\n",
    "                                        temp_file.write(chunk)\n",
    "                                        total_size += len(chunk)\n",
    "                            \n",
    "                            print(f\"   üì¶ Downloaded {total_size:,} bytes ({total_size/1024/1024:.1f} MB)\")\n",
    "                            \n",
    "                            if total_size < 10000:\n",
    "                                raise Exception(f\"File too small: {total_size} bytes\")\n",
    "                            \n",
    "                            # Upload with proper filename and no compression\n",
    "                            put_result = session.file.put(\n",
    "                                local_file_name=temp_path,\n",
    "                                stage_location=f\"@{SOURCE_STAGE_NAME}\",\n",
    "                                auto_compress=False,\n",
    "                                overwrite=True\n",
    "                            )\n",
    "                            \n",
    "                            if put_result and put_result[0].status == 'UPLOADED':\n",
    "                                print(f\"   ‚úÖ Uploaded {filename}\")\n",
    "                                success_count += 1\n",
    "                            else:\n",
    "                                print(f\"   ‚ö†Ô∏è  Upload failed\")\n",
    "                                error_count += 1\n",
    "                            \n",
    "                        finally:\n",
    "                            if temp_path and os.path.exists(temp_path):\n",
    "                                os.unlink(temp_path)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ùå Error: {str(e)}\")\n",
    "                    error_count += 1\n",
    "                    if temp_path and os.path.exists(temp_path):\n",
    "                        try:\n",
    "                            os.unlink(temp_path)\n",
    "                        except:\n",
    "                            pass\n",
    "                \n",
    "                # Progress every 5 files\n",
    "                if i % 5 == 0 or i == total_pdfs:\n",
    "                    print(f\"\\nüìà Progress: {i}/{total_pdfs}, ‚úÖ{success_count} success, ‚ùå{error_count} errors\")\n",
    "            \n",
    "            # Final verification  \n",
    "            print(f\"\\nüìÅ Stage verification...\")\n",
    "            stage_files = session.sql(f\"LIST @{SOURCE_STAGE_NAME}\").collect()\n",
    "            \n",
    "            if stage_files:\n",
    "                print(f\"‚úÖ {len(stage_files)} files in @{SOURCE_STAGE_NAME}:\")\n",
    "                total_size = 0\n",
    "                \n",
    "                # Sort files by name for better display\n",
    "                sorted_files = sorted(stage_files, key=lambda x: x['name'])\n",
    "                \n",
    "                for file_info in sorted_files:\n",
    "                    # Get the name field and clean it up\n",
    "                    full_path = file_info['name']\n",
    "                    \n",
    "                    # Handle different path formats from LIST command\n",
    "                    if full_path.startswith('pdf_documents/'):\n",
    "                        name = full_path.replace('pdf_documents/', '')\n",
    "                    elif '/' in full_path:\n",
    "                        name = full_path.split('/')[-1]\n",
    "                    else:\n",
    "                        name = full_path\n",
    "                    \n",
    "                    size = file_info['size']\n",
    "                    total_size += size\n",
    "                    print(f\"   üìÑ {name} - {size:,} bytes ({size/1024/1024:.1f} MB)\")\n",
    "                \n",
    "                print(f\"\\nüéâ SUCCESS! Total: {total_size:,} bytes ({total_size/1024/1024:.1f} MB)\")\n",
    "            else:\n",
    "                print(f\"‚ùå No files in stage\")\n",
    "                \n",
    "        else:\n",
    "            print(\"‚ùå Failed to get PDF links\")\n",
    "    else:\n",
    "        print(\"‚ùå No result from discovery\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Download error: {str(e)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Download completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d32b3-ec94-4ca5-b577-5d988839a46b",
   "metadata": {
    "language": "python",
    "name": "filename_chapter_associations"
   },
   "outputs": [],
   "source": [
    "# Create Filename-Chapter Associations Table\n",
    "import re\n",
    "print(\"üìã Creating filename-chapter associations table...\")\n",
    "\n",
    "# Create the table with separate chapter number and title\n",
    "try:\n",
    "    session.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TABLE FILENAME_CHAPTER_ASSOCIATIONS (\n",
    "            FILENAME VARCHAR(100),\n",
    "            CHAPTER_NUMBER INTEGER,\n",
    "            CHAPTER_TITLE VARCHAR(500),\n",
    "            ORIGINAL_TEXT VARCHAR(500),\n",
    "            UPLOAD_TIMESTAMP TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),\n",
    "            PRIMARY KEY (FILENAME)\n",
    "        )\n",
    "    \"\"\").collect()\n",
    "    print(\"‚úÖ Table FILENAME_CHAPTER_ASSOCIATIONS created/verified\")\n",
    "    \n",
    "    # Get PDF links to extract titles\n",
    "    result = session.sql(f\"SELECT get_pdf_links_from_website('{TARGET_WEBSITE_URL}') as result\").collect()\n",
    "    \n",
    "    if result:\n",
    "        result_raw = result[0]['RESULT']\n",
    "        if isinstance(result_raw, str):\n",
    "            result_data = json.loads(result_raw)\n",
    "        else:\n",
    "            result_data = result_raw\n",
    "        \n",
    "        if result_data and result_data.get('success', False):\n",
    "            pdf_links = result_data.get('pdf_links', [])\n",
    "            \n",
    "            print(f\"üì• Parsing and inserting {len(pdf_links)} filename-chapter associations...\")\n",
    "            \n",
    "            # Insert each filename-title mapping with parsed data\n",
    "            for pdf_info in pdf_links:\n",
    "                filename = pdf_info['filename']\n",
    "                original_text = pdf_info.get('text', filename)\n",
    "                \n",
    "                # Parse chapter number and title\n",
    "                # Pattern: \"Chapter X: Title (size)\" or similar\n",
    "                chapter_number = None\n",
    "                chapter_title = original_text\n",
    "                \n",
    "                # Try to extract chapter number\n",
    "                chapter_match = re.search(r'Chapter\\s+(\\d+)', original_text, re.IGNORECASE)\n",
    "                if chapter_match:\n",
    "                    chapter_number = int(chapter_match.group(1))\n",
    "                    \n",
    "                    # Extract title after the colon, before any parentheses\n",
    "                    title_match = re.search(r'Chapter\\s+\\d+:\\s*([^(]+)', original_text, re.IGNORECASE)\n",
    "                    if title_match:\n",
    "                        chapter_title = title_match.group(1).strip()\n",
    "                \n",
    "                # If no \"Chapter X:\" pattern, try to extract number from filename\n",
    "                if chapter_number is None:\n",
    "                    filename_match = re.search(r'chap(\\d+)', filename, re.IGNORECASE)\n",
    "                    if filename_match:\n",
    "                        chapter_number = int(filename_match.group(1))\n",
    "                \n",
    "                # Clean up chapter title (remove extra spaces, size info)\n",
    "                chapter_title = re.sub(r'\\s*\\(\\d+[A-Za-z]*\\)\\s*$', '', chapter_title).strip()\n",
    "                \n",
    "                try:\n",
    "                    session.sql(f\"\"\"\n",
    "                        INSERT INTO FILENAME_CHAPTER_ASSOCIATIONS \n",
    "                        (FILENAME, CHAPTER_NUMBER, CHAPTER_TITLE, ORIGINAL_TEXT)\n",
    "                        VALUES ('{filename}', {chapter_number or 'NULL'}, '{chapter_title.replace(\"'\", \"''\")}', '{original_text.replace(\"'\", \"''\")}')\n",
    "                    \"\"\").collect()\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Error inserting {filename}: {str(e)}\")\n",
    "            \n",
    "            # Verify the data\n",
    "            associations = session.sql(\"SELECT * FROM FILENAME_CHAPTER_ASSOCIATIONS ORDER BY CHAPTER_NUMBER\").collect()\n",
    "            \n",
    "            print(f\"\\n‚úÖ {len(associations)} associations created:\")\n",
    "            for assoc in associations[:10]:  # Show first 10\n",
    "                filename = assoc['FILENAME']\n",
    "                chapter_num = assoc['CHAPTER_NUMBER']\n",
    "                title = assoc['CHAPTER_TITLE'][:50] + \"...\" if len(assoc['CHAPTER_TITLE']) > 50 else assoc['CHAPTER_TITLE']\n",
    "                print(f\"   üìÑ {filename:<15} ‚Üí Ch.{chapter_num:2d}: {title}\")\n",
    "            \n",
    "            if len(associations) > 10:\n",
    "                print(f\"   ... and {len(associations) - 10} more associations\")\n",
    "                \n",
    "            print(f\"\\nüéØ Query examples:\")\n",
    "            print(f\"   ‚Ä¢ SELECT * FROM FILENAME_CHAPTER_ASSOCIATIONS WHERE CHAPTER_NUMBER = 1\")\n",
    "            print(f\"   ‚Ä¢ SELECT FILENAME, CHAPTER_TITLE FROM FILENAME_CHAPTER_ASSOCIATIONS ORDER BY CHAPTER_NUMBER\")\n",
    "                \n",
    "        else:\n",
    "            print(\"‚ùå Failed to get PDF links for associations\")\n",
    "    else:\n",
    "        print(\"‚ùå No result from PDF discovery\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating associations: {str(e)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Filename-chapter associations completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b9688e-9b33-4f92-9363-5d071df094b5",
   "metadata": {
    "collapsed": false,
    "name": "cell8"
   },
   "source": [
    "# Note to self... \n",
    "do something with the above associations; add to metadata..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596f4c95-b1e5-430d-bb5e-55fe4c0dae32",
   "metadata": {
    "collapsed": false,
    "name": "cell14"
   },
   "source": [
    "# Done with file upload!\n",
    "\n",
    "If you manually uploaded files to the Snowflake stage, continue processing the files starting from the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "Helper_Functions_Adaptive_Chapter_Split"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Helper Function (Adaptive Splitting) (Updated)\n",
    "#\n",
    "def get_page_range_desc(page_labels, start_idx, end_idx):\n",
    "    start_label = page_labels[start_idx] if page_labels and start_idx < len(page_labels) else start_idx + 1\n",
    "    end_label = page_labels[end_idx] if page_labels and end_idx < len(page_labels) else end_idx + 1\n",
    "    clean_desc = f\"pages{start_label}to{end_label}\".replace('/', '_').replace(' ', '')\n",
    "    readable_desc = f\"pages {start_label}-{end_label}\"\n",
    "    return clean_desc, readable_desc\n",
    "\n",
    "def split_large_pdf_on_stage(session, file_content_stream, original_filename, target_stage_path, max_size_mb=25):\n",
    "    original_size_mb = file_content_stream.getbuffer().nbytes / (1024 * 1024)\n",
    "    print(f\"\\nüìÑ Processing {original_filename} ({original_size_mb:.1f} MB)\")\n",
    "\n",
    "    # --- MODIFIED: DO NOT COPY SMALL FILES ---\n",
    "    if original_size_mb <= 25:\n",
    "        print(\"‚úÖ File is under 25MB and in the correct stage. No action needed.\")\n",
    "        return [{'filename': original_filename, 'page_range': 'all', 'size_mb': original_size_mb}]\n",
    "\n",
    "    # --- Logic for splitting large files remains the same ---\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        reader = pypdf.PdfReader(file_content_stream)\n",
    "        total_pages, page_labels = len(reader.pages), reader.page_labels\n",
    "        print(f\"Total pages: {total_pages}. Starting adaptive split...\")\n",
    "        \n",
    "        output_parts, start_page_of_chunk, part_num = [], 0, 1\n",
    "        \n",
    "        while start_page_of_chunk < total_pages:\n",
    "            size_test_writer, end_page_of_chunk = pypdf.PdfWriter(), start_page_of_chunk - 1\n",
    "            for i in range(start_page_of_chunk, total_pages):\n",
    "                size_test_writer.add_page(reader.pages[i])\n",
    "                with io.BytesIO() as buffer:\n",
    "                    size_test_writer.write(buffer)\n",
    "                    if buffer.tell() / (1024 * 1024) > max_size_mb and i > start_page_of_chunk: break\n",
    "                end_page_of_chunk = i\n",
    "            \n",
    "            final_writer = pypdf.PdfWriter()\n",
    "            for i in range(start_page_of_chunk, end_page_of_chunk + 1):\n",
    "                final_writer.add_page(reader.pages[i])\n",
    "\n",
    "            base_name = original_filename.rsplit('.', 1)[0]\n",
    "            clean_desc, readable_desc = get_page_range_desc(page_labels, start_page_of_chunk, end_page_of_chunk)\n",
    "            output_filename = f\"{base_name}_{clean_desc}.pdf\"\n",
    "            \n",
    "            temp_part_path = os.path.join(temp_dir, output_filename)\n",
    "            with open(temp_part_path, \"wb\") as f: final_writer.write(f)\n",
    "\n",
    "            final_size_mb = os.path.getsize(temp_part_path) / (1024 * 1024)\n",
    "            session.file.put(\n",
    "                local_file_name=temp_part_path,\n",
    "                #stage_location=f\"{target_stage_path}/{output_filename}\",\n",
    "                stage_location=f\"{target_stage_path}\",\n",
    "                auto_compress=False, overwrite=True\n",
    "            )\n",
    "            print(f\"  - Part {part_num}: Uploaded {output_filename} ({final_size_mb:.1f} MB, {readable_desc})\")\n",
    "            \n",
    "            output_parts.append({'filename': output_filename, 'page_range': readable_desc, 'size_mb': final_size_mb})\n",
    "            part_num += 1\n",
    "            start_page_of_chunk = end_page_of_chunk + 1\n",
    "            \n",
    "    return output_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "Split_Large_Files"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Main Execution (Adaptive Splitting for Large Files)\n",
    "#\n",
    "print(\"--- Starting Task 1: Adaptive splitting for files > 25MB ---\")\n",
    "all_results = {}\n",
    "try:\n",
    "    # pattern selects only the root directory so we don't also do anything recursive by selecting sub-directories\n",
    "    staged_files = session.sql(f\"LS @{SOURCE_STAGE_NAME} PATTERN='[^/]+'\").collect()\n",
    "    files_to_process = [f[\"name\"] for f in staged_files if f[\"name\"].lower().endswith('.pdf') and 'pages' not in f[\"name\"]]\n",
    "\n",
    "    if not files_to_process:\n",
    "        print(\"No matching PDF files found in the stage to process.\")\n",
    "    else:\n",
    "        print(f\"Found {len(files_to_process)} PDF(s) to check...\")\n",
    "        for file_path_on_stage in sorted(files_to_process):\n",
    "            try:\n",
    "                stage_file_path = f\"@{file_path_on_stage}\"\n",
    "                file_name_only = file_path_on_stage.split('/')[-1]\n",
    "                \n",
    "                with session.file.get_stream(stage_file_path) as instream:\n",
    "                    pdf_bytes_io = io.BytesIO(instream.read())\n",
    "                    parts = split_large_pdf_on_stage(\n",
    "                        session=session,\n",
    "                        file_content_stream=pdf_bytes_io,\n",
    "                        original_filename=file_name_only,\n",
    "                        target_stage_path=ADAPTIVE_SPLIT_TARGET_PATH, # Use correct target\n",
    "                    )\n",
    "                    if len(parts) > 1:\n",
    "                        all_results[file_name_only.rsplit('.', 1)[0]] = parts\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error processing {file_path_on_stage}: {e}\")\n",
    "except SnowparkSQLException as e:\n",
    "    print(f\"‚ùå SQL Error: Could not list files in stage '@{SOURCE_STAGE_NAME}'. Please check permissions.\")\n",
    "    print(e)\n",
    "print(\"\\n--- Task 1 Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f83429-288b-47d5-b86d-b2a6f105a8be",
   "metadata": {
    "language": "python",
    "name": "Helper_Function_Individual_Files"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Cell 4: Helper Functions (Single-Page Splitting) (New)\n",
    "#\n",
    "def get_page_label(pdf_reader, page_num):\n",
    "    \"\"\"Extracts the page label for a given page number.\"\"\"\n",
    "    try:\n",
    "        return pdf_reader.page_labels[page_num]\n",
    "    except (IndexError, KeyError):\n",
    "        return None\n",
    "\n",
    "def split_pdf_to_single_pages_on_stage(session, file_content_stream, original_filename, target_stage_path):\n",
    "    \"\"\"Splits a PDF from a stream into single pages and uploads them to a stage directory.\"\"\"\n",
    "    print(f\"\\nüìÑ Splitting {original_filename} into single pages...\")\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        reader = pypdf.PdfReader(file_content_stream)\n",
    "        total_pages = len(reader.pages)\n",
    "        print(f\"Total pages: {total_pages}\")\n",
    "\n",
    "        base_name = original_filename.rsplit('.', 1)[0]\n",
    "        page_upload_info = []\n",
    "\n",
    "        for page_num in range(total_pages):\n",
    "            pdf_writer = pypdf.PdfWriter()\n",
    "            pdf_writer.add_page(reader.pages[page_num])\n",
    "            \n",
    "            page_label = get_page_label(reader, page_num)\n",
    "            \n",
    "            if page_label:\n",
    "                clean_label = str(page_label).replace('/', '_').replace('\\\\', '_')\n",
    "                try:\n",
    "                    # Attempt to convert and format\n",
    "                    page_number_str = f\"{int(clean_label):04d}\"\n",
    "                    output_filename = f\"{base_name}_page{page_number_str}.pdf\"\n",
    "                except ValueError:\n",
    "                    # Handle cases where clean_label is not a valid number\n",
    "                    output_filename = f\"{base_name}_page{clean_label}.pdf\" \n",
    "            else:\n",
    "                output_filename = f\"{base_name}_page{page_num+1:04d}_nolabel.pdf\"\n",
    "            \n",
    "            temp_page_path = os.path.join(temp_dir, output_filename)\n",
    "            with open(temp_page_path, \"wb\") as f:\n",
    "                pdf_writer.write(f)\n",
    "\n",
    "            session.file.put(\n",
    "                local_file_name=temp_page_path,\n",
    "                #stage_location=f\"{target_stage_path}/{output_filename}\",\n",
    "                stage_location=f\"{target_stage_path}\",\n",
    "                auto_compress=False, overwrite=True\n",
    "            )\n",
    "            page_upload_info.append({'filename': output_filename, 'page_label': page_label})\n",
    "            \n",
    "            if (page_num + 1) % 20 == 0 or page_num == total_pages - 1:\n",
    "                print(f\"  ...uploaded {page_num+1}/{total_pages} pages\")\n",
    "    \n",
    "    print(f\"‚úÖ Split into {len(page_upload_info)} individual pages in @{target_stage_path}\")\n",
    "    return page_upload_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b980e-c5d2-4e9f-a156-33954d1ff1c8",
   "metadata": {
    "language": "python",
    "name": "Single_Page_Split"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Cell 5: Main Execution (Single-Page Splitting) (Updated)\n",
    "#\n",
    "print(\"\\n--- Starting Task 2: Splitting all original files into single pages ---\")\n",
    "single_page_results = {} # To store results for the summary\n",
    "\n",
    "try:\n",
    "    # the pattern select only from the root; otherwise we get recursive splitting and sub-directory files\n",
    "    staged_files = session.sql(f\"LS @{SOURCE_STAGE_NAME} PATTERN='[^/]+'\").collect()\n",
    "    # Find original PDFs, excluding any adaptively split parts\n",
    "    files_to_process = [f[\"name\"] for f in staged_files if f[\"name\"].lower().endswith('.pdf') and '_pages' not in f[\"name\"]]\n",
    "\n",
    "    if not files_to_process:\n",
    "        print(\"No original PDF files found in the stage to process.\")\n",
    "    else:\n",
    "        print(f\"Found {len(files_to_process)} original PDF(s) to split into single pages...\")\n",
    "        for file_path_on_stage in sorted(files_to_process):\n",
    "            try:\n",
    "                stage_file_path = f\"@{file_path_on_stage}\"\n",
    "                file_name_only = file_path_on_stage.split('/')[-1]\n",
    "                \n",
    "                with session.file.get_stream(stage_file_path) as instream:\n",
    "                    pdf_bytes_io = io.BytesIO(instream.read())\n",
    "                    # Capture the returned info\n",
    "                    page_upload_info = split_pdf_to_single_pages_on_stage(\n",
    "                        session=session,\n",
    "                        file_content_stream=pdf_bytes_io,\n",
    "                        original_filename=file_name_only,\n",
    "                        target_stage_path=SINGLE_PAGE_TARGET_PATH,\n",
    "                    )\n",
    "                    single_page_results[file_name_only] = page_upload_info\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error splitting {file_path_on_stage} into single pages: {e}\")\n",
    "except SnowparkSQLException as e:\n",
    "    print(f\"‚ùå SQL Error: Could not list files in stage '@{SOURCE_STAGE_NAME}'. Please check permissions.\")\n",
    "    print(e)\n",
    "print(\"\\n--- Task 2 Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "Summary"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Cell 6: Final Summary\n",
    "#\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ All Tasks Complete\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --- Summary for Task 1: Adaptive Splitting ---\n",
    "print(\"\\nüìã Summary of Adaptive Splitting (Task 1)\")\n",
    "if all_results:\n",
    "    total_parts = sum(len(parts) for parts in all_results.values())\n",
    "    print(f\"  - Total: {len(all_results)} large PDF(s) were split into {total_parts} parts.\")\n",
    "    print(f\"  - Destination: @{ADAPTIVE_SPLIT_TARGET_PATH}/\")\n",
    "else:\n",
    "    print(\"  - No files required adaptive splitting.\")\n",
    "\n",
    "# --- Summary for Task 2: Single-Page Splitting ---\n",
    "print(\"\\nüìã Summary of Single-Page Splitting (Task 2)\")\n",
    "if single_page_results:\n",
    "    total_pages_created = sum(len(pages) for pages in single_page_results.values())\n",
    "    print(f\"  - Total: {len(single_page_results)} original PDF(s) were split into {total_pages_created} single pages.\")\n",
    "    print(f\"  - Destination: @{SINGLE_PAGE_TARGET_PATH}/\")\n",
    "else:\n",
    "    print(\"  - Single-page splitting was not run or no files were processed.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de915c2-44f4-4586-aaf7-aa6b5dfc5b92",
   "metadata": {
    "collapsed": false,
    "name": "cell4"
   },
   "source": [
    "# Start Table Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f970f-2ae0-4026-9ed1-1cda6a00c17d",
   "metadata": {
    "language": "sql",
    "name": "create_document_processing_tables"
   },
   "outputs": [],
   "source": [
    "-- Main table for individual document pages\n",
    "CREATE OR REPLACE TABLE DOCUMENT_PAGES (\n",
    "    PAGE_ID STRING PRIMARY KEY,\n",
    "    CHAPTER_NUMBER INTEGER,\n",
    "    PAGE_NUMBER INTEGER,                    -- Actual page number from PDF\n",
    "    PART_IDENTIFIER STRING,                 -- Links pages to their parent parts (e.g., 'pages64to76')\n",
    "    CHAPTER_TITLE STRING,\n",
    "    -- UNIT_NUMBER INTEGER,\n",
    "    PAGE_CONTENT_RAW TEXT,\n",
    "    PAGE_CONTENT TEXT,                      -- Content for this specific page\n",
    "    PAGE_SUMMARY TEXT,                      -- AI-generated page summary\n",
    "    PAGE_KEYWORDS ARRAY,                    -- Key terms on this page\n",
    "    --CONTENT_VECTOR VECTOR(FLOAT, 768),     -- Page embedding for search\n",
    "    \n",
    "    -- Content metadata\n",
    "    CONTENT_TYPE STRING,                    -- 'text_heavy', 'visual_heavy', 'mixed'\n",
    "    WORD_COUNT INTEGER,\n",
    "    CHAR_COUNT INTEGER,\n",
    "    \n",
    "    -- URL and citation\n",
    "    CHAPTER_PDF_URL STRING,                 -- Presigned URL to PDF\n",
    "    SOURCE_FILENAME STRING,                 -- Original source filename (RELATIVE_PATH)\n",
    "    ENHANCED_CITATION STRING,               -- \"Chapter Title, Chapter X, Page Y\"\n",
    "    \n",
    "    -- Processing metadata\n",
    "    PROCESSING_STATUS STRING DEFAULT 'PENDING',\n",
    "    PROCESSING_TIMESTAMP TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()\n",
    ");\n",
    "\n",
    "-- ================================================================================\n",
    "-- HIERARCHICAL RAG STAGING TABLE - For chapter and part-level content\n",
    "-- ================================================================================\n",
    "\n",
    "-- Staging table for chapter and multi-page part PDFs\n",
    "CREATE OR REPLACE TABLE DOCUMENT_PARTS (\n",
    "    CHAPTER_NUMBER INTEGER,\n",
    "    PART_IDENTIFIER STRING,      -- e.g., 'pages64to76', 'full' for complete chapters\n",
    "    PART_CONTENT_RAW STRING,     -- Raw text extracted from PDF\n",
    "    SOURCE_FILENAME STRING,      -- Original filename\n",
    "    PDF_URL STRING,              -- Presigned URL to the source PDF\n",
    "    \n",
    "    -- Processing metadata\n",
    "    WORD_COUNT INTEGER,\n",
    "    CHAR_COUNT INTEGER,\n",
    "    ENHANCED_CITATION STRING,\n",
    "    PROCESSING_STATUS STRING DEFAULT 'PENDING',\n",
    "    CREATED_TIMESTAMP TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()\n",
    ")\n",
    "COMMENT = 'Staging table for chapter-level and multi-page part PDFs used in hierarchical RAG pipeline';\n",
    "\n",
    "-- AI analysis results for cross-reference extraction\n",
    "CREATE OR REPLACE TABLE DOCUMENT_ANALYSIS (\n",
    "    PAGE_ID STRING PRIMARY KEY,\n",
    "    CHAPTER_NUMBER INTEGER,\n",
    "    PAGE_NUMBER INTEGER,\n",
    "    PAGE_CONTENT TEXT,\n",
    "    EXTRACTED_REFERENCES VARIANT,          -- JSON array of cross-references\n",
    "    AI_MODEL STRING,\n",
    "    PROCESSING_TIMESTAMP TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()\n",
    ");\n",
    "\n",
    "-- Knowledge graph edges between pages/chapters\n",
    "CREATE OR REPLACE TABLE DOCUMENT_EDGES (\n",
    "    EDGE_ID STRING DEFAULT CONCAT('EDGE_', UNIFORM(1, 999999999, RANDOM())) PRIMARY KEY,\n",
    "    \n",
    "    -- Source page\n",
    "    SRC_PAGE_ID STRING,\n",
    "    SRC_CHAPTER_NUMBER INTEGER,\n",
    "    SRC_PAGE_NUMBER INTEGER,\n",
    "    \n",
    "    -- Destination page (may not exist yet)\n",
    "    DST_PAGE_ID STRING,                     -- NULL if referenced page doesn't exist\n",
    "    DST_CHAPTER_NUMBER INTEGER,\n",
    "    DST_PAGE_NUMBER INTEGER,\n",
    "    \n",
    "    -- Reference details\n",
    "    REFERENCE_TYPE STRING,                  -- 'page_reference', 'chapter_reference', 'figure_reference'\n",
    "    REFERENCE_CONTEXT STRING,              -- \"see page 759\", \"discussed in Chapter 24\"\n",
    "    REFERENCE_EXPLANATION STRING,          -- AI explanation of the connection\n",
    "    CONFIDENCE_SCORE FLOAT,\n",
    "    \n",
    "    PROCESSING_TIMESTAMP TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()\n",
    ");\n",
    "\n",
    "-- ================================================================================\n",
    "-- HIERARCHICAL RAG TABLE - Final unified table for multi-granularity RAG\n",
    "-- ================================================================================\n",
    "\n",
    "-- Final table for hierarchical multi-hop RAG - stores content at all granularity levels\n",
    "CREATE OR REPLACE TABLE WORLD_HISTORY_RAG (\n",
    "    CHAPTER_NUMBER INTEGER,\n",
    "    PART_IDENTIFIER STRING,     -- e.g., 'pages64to76', 'full', or NULL for chapter-wide summaries\n",
    "    PAGE_NUMBER INTEGER,        -- The specific page number, or NULL for part/chapter summaries\n",
    "    CONTENT_TYPE STRING,        -- 'ChapterSummary', 'PartSummary', 'PageSummary', 'RawText'\n",
    "    TEXT_CONTENT STRING,        -- The actual text or summary\n",
    "    SOURCE_FILENAME STRING,     -- The original file the content was extracted from\n",
    "    PDF_URL STRING,             -- A presigned URL to the source PDF\n",
    "    PAGE_ID STRING,             -- The page ID used in multi-hop search\n",
    "    ENHANCED_CITATION STRING,   -- \"Chapter Title, Chapter X, Page Y\"\n",
    "    -- Processing metadata\n",
    "    CREATED_TIMESTAMP TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()\n",
    ")    \n",
    "    COMMENT = 'Hierarchical RAG table storing content at multiple granularities: raw text, page summaries, part summaries, and chapter summaries';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b36759-2513-4a75-9fa8-4a3c9f88706e",
   "metadata": {
    "language": "sql",
    "name": "ingest_document_pages"
   },
   "outputs": [],
   "source": [
    "INSERT INTO DOCUMENT_PAGES (\n",
    "    PAGE_ID, CHAPTER_NUMBER, PAGE_NUMBER, PART_IDENTIFIER, CHAPTER_TITLE,\n",
    "     PAGE_CONTENT_RAW, PAGE_CONTENT,     CONTENT_TYPE, WORD_COUNT, CHAR_COUNT, CHAPTER_PDF_URL,\n",
    "    SOURCE_FILENAME, ENHANCED_CITATION, PROCESSING_STATUS\n",
    ")\n",
    "WITH page_processing AS (\n",
    "    SELECT\n",
    "        -- Extract chapter and page numbers using regex\n",
    "        ZEROIFNULL(REGEXP_SUBSTR(RELATIVE_PATH, 'chap(\\\\d+)', 1, 1, 'ie', 1))::INT AS chapter_number,\n",
    "        REGEXP_SUBSTR(RELATIVE_PATH, 'page[A-Z]*(\\\\d+)', 1, 1, 'ie', 1)::INT AS page_number,\n",
    "        \n",
    "        -- Extract part identifier if this page belongs to a multi-page chunk (usually NULL for individual pages)\n",
    "        REGEXP_SUBSTR(RELATIVE_PATH, 'pages(\\\\d+to\\\\d+)', 1, 1, 'ie', 1) as part_identifier,\n",
    "        \n",
    "        -- Generate consistent page ID\n",
    "        'CH' || LPAD(ZEROIFNULL(REGEXP_SUBSTR(RELATIVE_PATH, 'chap(\\\\d+)', 1, 1, 'ie', 1))::INT, 2, '0') || \n",
    "        '_P' || LPAD(REGEXP_SUBSTR(RELATIVE_PATH, 'page[A-Z]*(\\\\d+)', 1, 1, 'ie', 1)::INT, 4, '0') as page_id,\n",
    "        \n",
    "        -- Parse PDF content\n",
    "        SNOWFLAKE.CORTEX.PARSE_DOCUMENT('@PDF_DOCUMENTS', RELATIVE_PATH, {'mode': 'LAYOUT'}) as parse_result,\n",
    "        RELATIVE_PATH,\n",
    "        \n",
    "        -- Generate presigned URL\n",
    "        GET_PRESIGNED_URL('@PDF_DOCUMENTS', RELATIVE_PATH, 604800) as pdf_url\n",
    "    FROM\n",
    "        DIRECTORY(@PDF_DOCUMENTS)\n",
    "    WHERE\n",
    "        RELATIVE_PATH LIKE 'pages/%.pdf'\n",
    "        AND page_id\n",
    "            NOT IN (SELECT PAGE_ID FROM DOCUMENT_PAGES WHERE PAGE_ID IS NOT NULL)  -- Idempotency check\n",
    ")\n",
    "SELECT\n",
    "    -- Core identifiers\n",
    "    page_id,\n",
    "    chapter_number,\n",
    "    page_number,\n",
    "    part_identifier,\n",
    "    'Chapter ' || chapter_number as chapter_title,\n",
    "    \n",
    "    \n",
    "    -- Content fields\n",
    "    COALESCE(parse_result['content']::STRING, '') as page_content_raw,\n",
    "    COALESCE(parse_result['content']::STRING, '') as page_content,  -- Same as raw for now\n",
    "\n",
    "    \n",
    "    -- Content metadata\n",
    "    CASE \n",
    "        WHEN LENGTH(TRIM(COALESCE(parse_result['content']::STRING, ''))) < 100 THEN 'text_light'\n",
    "        WHEN LENGTH(TRIM(COALESCE(parse_result['content']::STRING, ''))) > 2000 THEN 'text_heavy'\n",
    "        ELSE 'mixed'\n",
    "    END as content_type,\n",
    "    \n",
    "    -- Counts\n",
    "    CASE \n",
    "        WHEN LENGTH(TRIM(COALESCE(parse_result['content']::STRING, ''))) = 0 THEN 0\n",
    "        ELSE ARRAY_SIZE(SPLIT(COALESCE(parse_result['content']::STRING, ''), ' '))\n",
    "    END as word_count,\n",
    "    LENGTH(COALESCE(parse_result['content']::STRING, '')) as char_count,\n",
    "    \n",
    "    -- URLs and citations\n",
    "    pdf_url as chapter_pdf_url,\n",
    "    RELATIVE_PATH as source_filename,\n",
    "    CONCAT(\n",
    "        'Chapter ',\n",
    "        chapter_number,\n",
    "        ', Pages ',\n",
    "        REPLACE(\n",
    "        REGEXP_SUBSTR(source_filename, '\\\\d+to\\\\d+'),\n",
    "        'to',\n",
    "        '-'\n",
    "        )\n",
    "     ) AS enhanced_citation,       \n",
    "    \n",
    "    -- Processing status\n",
    "    'PROCESSED' as processing_status\n",
    "\n",
    "FROM page_processing\n",
    "WHERE chapter_number IS NOT NULL \n",
    "  AND page_number IS NOT NULL\n",
    "  AND LENGTH(TRIM(COALESCE(parse_result['content']::STRING, ''))) > 50;  -- Quality filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5b20be-75d5-4b79-9d58-2ee95a2290d8",
   "metadata": {
    "language": "sql",
    "name": "ingest_document_parts"
   },
   "outputs": [],
   "source": [
    "INSERT INTO DOCUMENT_PARTS (CHAPTER_NUMBER, PART_IDENTIFIER, PART_CONTENT_RAW, SOURCE_FILENAME, PDF_URL, WORD_COUNT, CHAR_COUNT, ENHANCED_CITATION, PROCESSING_STATUS)\n",
    "WITH parsed_docs AS (\n",
    "  SELECT\n",
    "    RELATIVE_PATH,\n",
    "    SNOWFLAKE.CORTEX.PARSE_DOCUMENT('@PDF_DOCUMENTS', RELATIVE_PATH, {'mode': 'LAYOUT'})['content']::STRING AS content_raw\n",
    "  FROM DIRECTORY(@PDF_DOCUMENTS)\n",
    "  WHERE\n",
    "    RELATIVE_PATH LIKE 'parts/%.pdf' -- Only files in parts subdirectory\n",
    "    AND RELATIVE_PATH NOT IN (SELECT SOURCE_FILENAME FROM DOCUMENT_PARTS WHERE SOURCE_FILENAME IS NOT NULL) -- Idempotency check\n",
    ")\n",
    "SELECT\n",
    "    ZEROIFNULL(REGEXP_SUBSTR(pd.RELATIVE_PATH, 'chap(\\\\d+)', 1, 1, 'ie', 1))::INT as chapter_number,\n",
    "    REGEXP_SUBSTR(pd.RELATIVE_PATH, '(pages\\\\d+to\\\\d+)', 1, 1, 'ie', 1) as part_identifier,\n",
    "    pd.content_raw,\n",
    "    pd.RELATIVE_PATH,\n",
    "    GET_PRESIGNED_URL('@PDF_DOCUMENTS', pd.RELATIVE_PATH, 604800),\n",
    "    -- Calculate word and character counts from the content_raw field\n",
    "    ARRAY_SIZE(SPLIT(pd.content_raw, ' ')),\n",
    "    LENGTH(pd.content_raw),\n",
    "    'Chapter ' || chapter_number || ', Part ' || part_identifier as enhanced_citation,\n",
    "    'PROCESSED'\n",
    "FROM parsed_docs AS pd\n",
    "WHERE\n",
    "    ZEROIFNULL(REGEXP_SUBSTR(pd.RELATIVE_PATH, 'chap(\\\\d+)', 1, 1, 'ie', 1))::INT IS NOT NULL; -- Must have a valid chapter number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7607d3e-050f-431b-988f-57765727d36f",
   "metadata": {
    "language": "sql",
    "name": "process_page_rawtext"
   },
   "outputs": [],
   "source": [
    "INSERT INTO WORLD_HISTORY_RAG (CHAPTER_NUMBER, PART_IDENTIFIER, PAGE_NUMBER, CONTENT_TYPE, TEXT_CONTENT, SOURCE_FILENAME, PDF_URL, PAGE_ID, ENHANCED_CITATION)\n",
    "\n",
    "-- First, select and insert the raw text for each page\n",
    "SELECT\n",
    "    p.CHAPTER_NUMBER, \n",
    "    p.PART_IDENTIFIER, \n",
    "    p.PAGE_NUMBER, \n",
    "    'RawText' AS CONTENT_TYPE,\n",
    "    AI_COMPLETE(\n",
    "        'llama3.3-70b', \n",
    "        'Return the raw text of the page with obvious errors fixed.  This includes spelling errors, hyphenated words, combined words, etc.  If and only a part of the text is not clear you can try to figure out what it means, but if there is not enough context then just return the raw text.  If any historical names, dates, etc seem way off try to correct it but do not make up anything.  Do NOT add any additional text or commentary like \"Here is the text with obvious errors fixed\".  The text will be used later in a search engine so it should be as close to the original text as possible. :\\n' || p.PAGE_CONTENT_RAW\n",
    "    ) as PAGE_CONTENT_RAW,\n",
    "    p.SOURCE_FILENAME, \n",
    "    p.CHAPTER_PDF_URL,\n",
    "    'CH' || LPAD(p.CHAPTER_NUMBER, 2, '0') || '_P' || LPAD(p.PAGE_NUMBER, 4, '0') as PAGE_ID,\n",
    "    p.ENHANCED_CITATION\n",
    "FROM DOCUMENT_PAGES p\n",
    "WHERE p.PROCESSING_STATUS = 'PROCESSED'\n",
    "  AND p.PAGE_CONTENT_RAW IS NOT NULL\n",
    "  AND LENGTH(p.PAGE_CONTENT_RAW) > 50;  -- Only process pages with substantial content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa813e1-9f44-4a33-ad5f-19ad7b766a51",
   "metadata": {
    "language": "sql",
    "name": "process_page_summaries"
   },
   "outputs": [],
   "source": [
    "INSERT INTO WORLD_HISTORY_RAG (CHAPTER_NUMBER, PART_IDENTIFIER, PAGE_NUMBER, CONTENT_TYPE, TEXT_CONTENT, SOURCE_FILENAME, PDF_URL, PAGE_ID, ENHANCED_CITATION)\n",
    "SELECT\n",
    "    p.CHAPTER_NUMBER, \n",
    "    p.PART_IDENTIFIER, \n",
    "    p.PAGE_NUMBER, \n",
    "    'PageSummary' AS CONTENT_TYPE,\n",
    "    AI_COMPLETE(\n",
    "        'llama3.3-70b', \n",
    "        'Do not add any additional text or commentary like \"Here is the summary\".  The text will be used later in a search engine so it should be as close to the original text as possible.  Summarize this page content in 1-2 concise sentences:\\n' || p.PAGE_CONTENT_RAW\n",
    "    ),\n",
    "    p.SOURCE_FILENAME, \n",
    "    p.CHAPTER_PDF_URL,\n",
    "    'CH' || LPAD(p.CHAPTER_NUMBER, 2, '0') || '_P' || LPAD(p.PAGE_NUMBER, 4, '0') as PAGE_ID,\n",
    "    p.ENHANCED_CITATION\n",
    "FROM DOCUMENT_PAGES p\n",
    "WHERE p.PROCESSING_STATUS = 'PROCESSED'\n",
    "  AND p.PAGE_CONTENT_RAW IS NOT NULL\n",
    "  AND LENGTH(p.PAGE_CONTENT_RAW) > 50  -- Only process pages with substantial content\n",
    "  AND CONCAT(p.CHAPTER_NUMBER, '_', p.PAGE_NUMBER, '_PageSummary') NOT IN (\n",
    "      SELECT CONCAT(CHAPTER_NUMBER, '_', PAGE_NUMBER, '_', CONTENT_TYPE) \n",
    "      FROM WORLD_HISTORY_RAG \n",
    "      WHERE CONTENT_TYPE = 'PageSummary'\n",
    "      limit 10 \n",
    "  ); -- Idempotency check for page summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc8e53-3109-4506-8209-276c02568721",
   "metadata": {
    "language": "sql",
    "name": "process_part_rawtext"
   },
   "outputs": [],
   "source": [
    "INSERT INTO WORLD_HISTORY_RAG (CHAPTER_NUMBER, PART_IDENTIFIER, PAGE_NUMBER, CONTENT_TYPE, TEXT_CONTENT, SOURCE_FILENAME, PDF_URL, PAGE_ID, ENHANCED_CITATION)\n",
    "SELECT\n",
    "    p.CHAPTER_NUMBER,\n",
    "    p.PART_IDENTIFIER,\n",
    "    NULL AS PAGE_NUMBER,  -- Parts don't have specific page numbers\n",
    "    'RawText' AS CONTENT_TYPE,\n",
    "    AI_COMPLETE(\n",
    "      'llama3.3-70b', \n",
    "      'Return the raw text of the page with obvious errors fixed.  This includes spelling errors, hyphenated words, combined words, etc.  If and only a part of the text is not clear you can try to figure out what it means, but if there is not enough context then just return the raw text.  If any historical names, dates, etc seem way off try to correct it but do not make up anything.  Do NOT add any additional text or commentary like \"Here is the text with obvious errors fixed\".  The text will be used later in a search engine so it should be as close to the original text as possible. :\\n' ||  p.PART_CONTENT_RAW\n",
    "    ) as TEXT_CONTENT,\n",
    "    p.SOURCE_FILENAME,\n",
    "    p.PDF_URL,\n",
    "    'chap' || LPAD(p.CHAPTER_NUMBER, 2, '0') || '_part' || p.PART_IDENTIFIER as PAGE_ID,\n",
    "    p.ENHANCED_CITATION\n",
    "FROM DOCUMENT_PARTS p\n",
    "WHERE p.PART_CONTENT_RAW IS NOT NULL\n",
    "  -- AND LENGTH(TRIM(p.PART_CONTENT_RAW)) > 100  -- Quality filter\n",
    "  AND CONCAT(p.CHAPTER_NUMBER, '_', p.PART_IDENTIFIER, '_RawText') NOT IN (\n",
    "      SELECT CONCAT(CHAPTER_NUMBER, '_', PART_IDENTIFIER, '_', CONTENT_TYPE) \n",
    "      FROM WORLD_HISTORY_RAG \n",
    "      WHERE CONTENT_TYPE = 'RawText' AND PART_IDENTIFIER IS NOT NULL\n",
    "  ); -- Idempotency check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9498ef-c7a9-4c20-b9fe-7e3a95200fa4",
   "metadata": {
    "language": "sql",
    "name": "process_part_summary"
   },
   "outputs": [],
   "source": [
    "INSERT INTO WORLD_HISTORY_RAG (CHAPTER_NUMBER, PART_IDENTIFIER, PAGE_NUMBER, CONTENT_TYPE, TEXT_CONTENT, SOURCE_FILENAME, PDF_URL, PAGE_ID, ENHANCED_CITATION)\n",
    "SELECT\n",
    "    p.CHAPTER_NUMBER,\n",
    "    p.PART_IDENTIFIER,\n",
    "    NULL AS PAGE_NUMBER,  -- Parts don't have specific page numbers\n",
    "    'PartSummary' AS CONTENT_TYPE,\n",
    "    AI_COMPLETE(\n",
    "        'llama3.3-70b',\n",
    "        'Do not add any additional text or commentary like \"Here is the summary\".  If there is not enough context to summarize the part then just return the raw text.  Summarize the following history textbook section in 2-3 concise paragraphs. Focus on the main themes, key events, important people, and historical significance:\\n\\n' || \n",
    "        p.PART_CONTENT_RAW ||  \n",
    "        '\\n\\nProvide a clear, educational summary suitable for a history student.'\n",
    "    ) AS TEXT_CONTENT,\n",
    "    p.SOURCE_FILENAME,\n",
    "    p.PDF_URL,\n",
    "    'CH' || LPAD(p.CHAPTER_NUMBER, 2, '0') || '_' || p.PART_IDENTIFIER as PAGE_ID,\n",
    "    p.ENHANCED_CITATION\n",
    "FROM DOCUMENT_PARTS p\n",
    "WHERE p.PART_CONTENT_RAW IS NOT NULL\n",
    "  -- AND LENGTH(TRIM(p.PART_CONTENT_RAW)) > 100  -- Quality filter\n",
    "  AND CONCAT(p.CHAPTER_NUMBER, '_', p.PART_IDENTIFIER, '_PartSummary') NOT IN (\n",
    "      SELECT CONCAT(CHAPTER_NUMBER, '_', PART_IDENTIFIER, '_', CONTENT_TYPE) \n",
    "      FROM WORLD_HISTORY_RAG \n",
    "      WHERE CONTENT_TYPE = 'PartSummary' AND PART_IDENTIFIER IS NOT NULL\n",
    "  ); -- Idempotency check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfcd2e5-2835-4696-9584-2beb5fbcfc18",
   "metadata": {
    "language": "sql",
    "name": "process_chapter_summary"
   },
   "outputs": [],
   "source": [
    "INSERT INTO WORLD_HISTORY_RAG (CHAPTER_NUMBER, PART_IDENTIFIER, PAGE_NUMBER, CONTENT_TYPE, TEXT_CONTENT, SOURCE_FILENAME, PDF_URL)\n",
    "WITH aggregated_part_summaries AS (\n",
    "    SELECT\n",
    "        CHAPTER_NUMBER,\n",
    "        ANY_VALUE(PDF_URL) as PDF_URL,\n",
    "        ANY_VALUE(SOURCE_FILENAME) as SOURCE_FILENAME,\n",
    "        LISTAGG(TEXT_CONTENT, '\\n\\n---\\n\\n') WITHIN GROUP (ORDER BY PART_IDENTIFIER) as full_chapter_text,\n",
    "        COUNT(*) as part_count\n",
    "    FROM WORLD_HISTORY_RAG\n",
    "    WHERE CONTENT_TYPE = 'PartSummary'\n",
    "      AND CHAPTER_NUMBER NOT IN (\n",
    "          SELECT DISTINCT CHAPTER_NUMBER \n",
    "          FROM WORLD_HISTORY_RAG \n",
    "          WHERE CONTENT_TYPE = 'ChapterSummary'\n",
    "      ) -- Idempotency check for chapter summaries\n",
    "    GROUP BY CHAPTER_NUMBER\n",
    ")\n",
    "SELECT\n",
    "    a.CHAPTER_NUMBER, \n",
    "    NULL AS PART_IDENTIFIER, \n",
    "    NULL AS PAGE_NUMBER, \n",
    "    'ChapterSummary' AS CONTENT_TYPE,\n",
    "    AI_COMPLETE(\n",
    "        'llama3.3-70b', \n",
    "        'You are given several high-level summaries from different sections of a history chapter. Combine them into a single, comprehensive 2-3 paragraph summary of the entire chapter.  Do not add any additional text or commentary like \"Here is the summary\".  The text will be used later in a search engine so it should be as close to the original text as possible. :\\n\\n' || \n",
    "        a.full_chapter_text ||\n",
    "        '\\n\\nCreate a cohesive narrative that synthesizes the key themes, events, and historical significance covered throughout this chapter.'\n",
    "    ),\n",
    "    a.SOURCE_FILENAME, \n",
    "    a.PDF_URL\n",
    "FROM aggregated_part_summaries a\n",
    "WHERE a.part_count > 0;  -- Only process chapters that have part summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8f283-3a27-4ebb-a235-008d517cbbdd",
   "metadata": {
    "language": "sql",
    "name": "update_pdf_urls_daily"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TASK DAILY_PDF_URL_REFRESH\n",
    "    USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = 'XSMALL'\n",
    "    SCHEDULE = 'USING CRON 0 6 * * * UTC'  -- Daily at 6 AM UTC\n",
    "    COMMENT = 'Refresh presigned URLs that expire within 24 hours'\n",
    "AS\n",
    "    update world_history_rag\n",
    "    set pdf_url = GET_PRESIGNED_URL(@WORLD_HISTORY.PUBLIC.PDF_DOCUMENTS, source_filename, 604800);\n",
    "\n",
    "alter task daily_pdf_url_refresh resume;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6f6f8d-ab84-4a24-b113-c90875f9f011",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": [
    "# Create Cortex Search Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e598532c-8e53-4c5e-acd2-9ca6ae05d5f9",
   "metadata": {
    "language": "sql",
    "name": "cortex_search_service"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE CORTEX SEARCH SERVICE WORLD_HISTORY_RAG_SEARCH\n",
    "    ON TEXT_CONTENT\n",
    "    ATTRIBUTES \n",
    "        CHAPTER_NUMBER,\n",
    "        PAGE_NUMBER,\n",
    "        SOURCE_FILENAME,\n",
    "        CONTENT_TYPE\n",
    "    WAREHOUSE = WAREHOUSE_XL_G2\n",
    "    TARGET_LAG = '30 days'\n",
    "    EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\n",
    "    AS (\n",
    "        SELECT \n",
    "            wh.TEXT_CONTENT,\n",
    "            wh.CHAPTER_NUMBER,\n",
    "            wh.PAGE_NUMBER,\n",
    "            wh.PART_IDENTIFIER,\n",
    "            wh.CONTENT_TYPE,\n",
    "            wh.SOURCE_FILENAME,\n",
    "            wh.PDF_URL,\n",
    "            wh.PAGE_ID,\n",
    "            wh.ENHANCED_CITATION\n",
    "        FROM WORLD_HISTORY_RAG wh\n",
    "        WHERE wh.TEXT_CONTENT IS NOT NULL\n",
    "          AND LENGTH(TRIM(wh.TEXT_CONTENT)) > 50\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa513b-f73d-499d-8ba4-3694220b2f77",
   "metadata": {
    "language": "sql",
    "name": "multihop_view_and_fn"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE VIEW MULTIHOP_SEARCH_RESULTS AS\n",
    "WITH search_base AS (\n",
    "    -- All searchable content with page IDs\n",
    "    SELECT \n",
    "        page_id,\n",
    "        wh.CHAPTER_NUMBER,\n",
    "        wh.PAGE_NUMBER,\n",
    "        wh.CONTENT_TYPE,\n",
    "        'Chapter ' || wh.CHAPTER_NUMBER || COALESCE(', Page ' || wh.PAGE_NUMBER, '') || ' (' || wh.CONTENT_TYPE || ')' as citation,\n",
    "        LEFT(wh.TEXT_CONTENT, 500) as content_preview,\n",
    "        wh.PDF_URL\n",
    "    FROM WORLD_HISTORY_RAG wh\n",
    "    WHERE wh.TEXT_CONTENT IS NOT NULL\n",
    ")\n",
    "SELECT \n",
    "    sb.*,\n",
    "    COALESCE(connected.related_pages, ARRAY_CONSTRUCT()) as connected_pages,\n",
    "    COALESCE(connected.connection_count, 0) as connection_count\n",
    "FROM search_base sb\n",
    "LEFT JOIN (\n",
    "    SELECT \n",
    "        e.SRC_PAGE_ID,\n",
    "        ARRAY_AGG(OBJECT_CONSTRUCT(\n",
    "            'page_id', e.DST_PAGE_ID,\n",
    "            'citation', 'Chapter ' || e.DST_CHAPTER_NUMBER || ', Page ' || e.DST_PAGE_NUMBER,\n",
    "            'context', e.REFERENCE_CONTEXT,\n",
    "            'confidence', e.CONFIDENCE_SCORE\n",
    "        )) as related_pages,\n",
    "        COUNT(*) as connection_count\n",
    "    FROM DOCUMENT_EDGES e\n",
    "    WHERE e.CONFIDENCE_SCORE > 0.5\n",
    "    GROUP BY e.SRC_PAGE_ID\n",
    ") connected ON sb.page_id = connected.SRC_PAGE_ID;\n",
    "\n",
    "-- Function to be used as a tool in the ; only takes a single unique page id\n",
    "CREATE OR REPLACE FUNCTION MULTIHOP_SEARCH_RESULTS_FN(page_id_param STRING)\n",
    "RETURNS array\n",
    "LANGUAGE SQL\n",
    "AS\n",
    "$$\n",
    "    SELECT \n",
    "    ARRAY_AGG(OBJECT_CONSTRUCT(*))\n",
    "        FROM multihop_search_results\n",
    "        WHERE page_id = page_id_param\n",
    "$$;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ecf2a-0be8-4cff-8272-334e08144b5f",
   "metadata": {
    "language": "sql",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE FUNCTION FIND_CONNECTED_PAGES(starting_page_id STRING, max_hops INT)\n",
    "RETURNS array\n",
    "LANGUAGE SQL\n",
    "AS\n",
    "$$\n",
    "    WITH RECURSIVE page_traversal AS (\n",
    "        -- Starting page (hop 0)\n",
    "        SELECT \n",
    "            page_id as dest_page_id,\n",
    "            chapter_number as dest_chapter_number,\n",
    "            page_number as dest_page_number,\n",
    "            enhanced_citation,\n",
    "            ARRAY_CONSTRUCT('Starting page: ' || enhanced_citation) as connection_path,\n",
    "            0 as hop_count\n",
    "        FROM DOCUMENT_PAGES \n",
    "        WHERE page_id = starting_page_id\n",
    "\n",
    "        UNION ALL\n",
    "\n",
    "        -- Connected pages (hop 1+)\n",
    "        SELECT \n",
    "            COALESCE(e.DST_PAGE_ID, 'MISSING_CH' || e.DST_CHAPTER_NUMBER || '_P' || LPAD(e.DST_PAGE_NUMBER, 4, '0')) as dest_page_id,\n",
    "            e.DST_CHAPTER_NUMBER as dest_chapter_number,\n",
    "            e.DST_PAGE_NUMBER as dest_page_number,\n",
    "            COALESCE(dp.ENHANCED_CITATION, 'Referenced: Chapter ' || e.DST_CHAPTER_NUMBER || ', Page ' || e.DST_PAGE_NUMBER) as enhanced_citation,\n",
    "            ARRAY_APPEND(pt.connection_path, e.REFERENCE_EXPLANATION || ' (' || e.REFERENCE_CONTEXT || ')') as connection_path,\n",
    "            pt.hop_count + 1\n",
    "        FROM page_traversal pt\n",
    "        JOIN DOCUMENT_EDGES e ON pt.dest_page_id = e.SRC_PAGE_ID\n",
    "        LEFT JOIN DOCUMENT_PAGES dp ON e.DST_PAGE_ID = dp.PAGE_ID\n",
    "        WHERE pt.hop_count < max_hops\n",
    "          AND e.CONFIDENCE_SCORE > 0.5\n",
    "    )\n",
    "    SELECT \n",
    "        ARRAY_AGG(OBJECT_CONSTRUCT(*))\n",
    "    FROM page_traversal \n",
    "    WHERE hop_count > 0  -- Exclude starting page\n",
    "    ORDER BY hop_count, dest_chapter_number, dest_page_number\n",
    "$$;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41769bf-faaa-4227-a99f-dcae6ace06f4",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "russ.goldin@snowflake.com",
   "authorId": "298846609519",
   "authorName": "RGOLDIN",
   "lastEditTime": 1754404355684,
   "notebookId": "eloloxhad76ye2ogjsv6",
   "sessionId": "da876f99-2072-464b-8116-e0fc3b050751"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
