{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "runfopeppywhiou5fwbc",
   "authorId": "298846609519",
   "authorName": "RGOLDIN",
   "authorEmail": "russ.goldin@snowflake.com",
   "sessionId": "6fbb72b7-faf0-40eb-83b8-6ee250930efa",
   "lastEditTime": 1754495907717
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad9bbef-d2a5-4cc1-8558-e91f11e5111c",
   "metadata": {
    "name": "Instructions",
    "collapsed": false
   },
   "source": [
    "# World_History_Snowflake\n",
    "Project to create and populate a DB with both structured and unstructured data\n",
    "\n",
    "\n",
    "# Instructions\n",
    "\n",
    "First, set the timeout on this Notebook to 1 (or 2) hours.  The entire sql file takes a while to run and if the notebook times out you will be left figuring out where to resume it.\n",
    "\n",
    "There are two (2) notebooks that setup, process and create the necessary data.\n",
    "1.  `1-WORLD_HISTORY_Setup_and_PDF_Ingestion.ipynb` \n",
    "    - Creates the database, stage and tables for the unstructured PDF data and processing\n",
    "    - The first couple of cells setup the database, and then there are a series of cells that connect to an external website to download 32 chapters in PDF format.  These can be completely done in Snowflake, but require AccountAdmin access to enable an external integration and a restart of the Snowflake notebook.  \n",
    "    The alternative is to use the `pdf_downloader.py` to download the files to a local machine and then manually upload then to the Snowflake @PDF_DOCUMENTS stage either through Snowsight or other methods.\n",
    "    - The balance of the cells use either SQL or Python to process the files into smaller parts and pages, extract raw text, summarize the content at the page/part/chapter level and collate the data into the world_history_rag table.\n",
    "    - This notebook also creates a graph knowledge base of the content so the Agent can find and follow references (ie See Page 543) to additional content.\n",
    "    - A Cortex Search service is created to use vector embeddings to find content within the document.\n",
    "2. `2-WORLD_HISTORY_SCHOOL_DATA.ipynb` \n",
    "    - Creates a second schema, `Schools` with tables and data related to cities, school districts, schools, classes, students, test questions, chapter exams and realistic distribution of grades for exams.\n",
    "    - Exam questions are retrieved from the PDF documents.\n",
    "    - Correct and incorrect answers are generated for individual student test results.\n",
    "    - There are 5 cities and school districts, 15 high schools, 45 classes, 45 teachers, 1350 students, 526 test questions, 710k individual test question responses, 43.2k exam results.\n",
    "    - A YAML file defining a Cortex Agent is uploaded to a public.config_files stage\n",
    "3. This setup _does not_ cover creating an Agent.  There is no UI or API to currently create agents.  See below.\n",
    "\n",
    "\n",
    "\n",
    "# Agent Setup\n",
    "This is how you can setup an Agent to use all of the services and tools to come up with accurate answers to complicated questions.\n",
    "\n",
    "## About\n",
    "- Display Name: World History Agent\n",
    "- Description: This agent has access to both information about schools, tests, grades, test questions, student responses (structured data) and a World History Textbook (unstructured data).  The content is related in the fact that the exams and questions and responses are based on the content from the World History Textbook.  Anyone can ask questions about content in the textbook, relate that back to student performance, and seamlessly use the agent to go back and forth between the different modalities.\n",
    "\n",
    "## Instructions\n",
    "- Response Instructions: Show any percentages as 0.00%.  If the question isn't extremely clear, ask for clarification.  You are an expert professor in World History.  You have the knowledge of 1060 pages of World History and access to student exam performance data.  Your keen observations, suggestions and insights will be highly prized.  Don't be afraid to make suggestions for how tests can be improved or how individual teachers, or schools, can teach the content differently.  \n",
    "- Sample Questions:\n",
    "    - Which is the first page that has references to other pages about the enlightment. What pages does it reference and what content is on those pages?\n",
    "    - I want to compare the military of Classical Athens to that of the late Roman Republic. Your primary method for finding the Roman comparison must be to execute a search for explicit connections starting from the pages discussing the Athenian military during the Persian Wars. First, summarize the Athenian model, then use the connection-finding tool to locate the relevant Roman content and provide the comparison.\n",
    "    - Analyze the policy of 'War Communism' implemented by the Bolsheviks during the Russian Civil War. First, summarize the policy's immediate historical context. Then, trace its ideological foundation by following any explicit cross-references in that chapter back to the introduction of Marxist theory earlier in the textbook.\n",
    "    - Compare the citizen-soldier model of Classical Athens during the Persian Wars with the professionalized army of the late Roman Republic. Begin by finding the section on the Persian Wars, summarize the chapter's discussion of the Athenian state and its military, and then use any direct textual cross-references to locate and analyze the author's comparison with the Roman military system.\n",
    "    - What were the hardest questions about the Roman Empire, which answer was chosen wrong the most, and what pages should students study to get more familiar with the content?\n",
    "    - How closely does the exam for Emerging Europe and the Byzantine Empire follow the textbook material?\n",
    "\n",
    "## Tools\n",
    "- Cortex Analyst: Add the World_History.public.config_files/world_history_semantic_model.yaml.  Let Cortex create the description.\n",
    "- Cortex Search: Add WORLD_HISTORY_QA.PUBLIC.WORLD_HISTORY_RAG_SEARCH\n",
    "   - Description: Returns vector based searches on the world history returning either pages, parts, page summaries, part summaries, or chapter summaries.\n",
    "   - ID Column: PDF_URL\n",
    "   - Title Column: ENHANCED_CITATION\n",
    "\n",
    "- Custom Tools\n",
    "    - Multihop_Search_Results.  Add WORLD_HISTORY.PUBLIC.MULTIHOP_SEARCH_RESULTS as a function.  \n",
    "        - page_id_param description: This is the page_id param that needs to be passed in the format of CHxx_Pyyyy.  Example: SELECT WORLD_HISTORY_QA.PUBLIC.MULTIHOP_SEARCH_RESULTS_FN('CH23_P0777');\n",
    "        - description: Use this tool to enrich context for a known page ID. When you have a specific page from a vector search, use this tool to retrieve the page summary, part summary, and chapter summary. This is best for answering questions about the broader theme, context, or significance of information found on a specific page.  This tool returns the connected pages (hops) for references.  It should be used to find if there are any connected edges.  Then move to the find_connected_edges tool to recursively follow those edges in the knowledge graph.\n",
    "    -Find_Connected_Edges. Add WORLD_HISTORY.PUBLIC.FIND_CONNECTED_PAGES as a function.\n",
    "        - max_hops description: This is the number of connections from the source page.  If the source page is page 10 and has a reference to page 20, that would be the first hop.  If page 20 has a reference to page 30 that's the 2nd hop.  Default to 2.\n",
    "        - starting_page_id description: This is the starting page id in the format \"CHxx_Pyyyy\".  Example \"CH23_P0772\".  It is a combination chapter and page number that we will get from the prior steps.\n",
    "        - description: Always use this tool _after_ multihop_search_results.  That tool tells you _if_ there are connected graph edges.  This tool then allows you to recursively follow the relationships of the material.  Use this tool to answer questions about explicit connections, direct links, or tracing a topic's influence across the textbook. It traverses the book's graph of 'see page...' cross-references. Prioritize this tool when a user asks to 'trace the origins of,' 'find the connection to,' 'see what this is linked to,' or analyze how the author explicitly compares two disparate topics.\n",
    "\n",
    "## Orchestration\n",
    "Planning instructions\n",
    "```\n",
    "Step 1: Question Routing üö¶\n",
    "The router should prioritize tools in order from most specialized to most general.\n",
    "\n",
    "Is the user asking to trace a connection or find an explicit link? (e.g., using words like \"trace,\" \"connect,\" \"link,\" \"cross-reference,\" \"compare to what the author links\").\n",
    "\n",
    "If yes, prioritize the Multihop_Search_Results + Find_Connected_Pages tool path.  \n",
    "\n",
    "Is the user asking for the summary, context, or significance of a known topic? (e.g., \"Summarize the chapter on the Persian Wars\").\n",
    "\n",
    "If yes, use the Cortex Search + Multihop_Search_Results Path.\n",
    "\n",
    "Is it a general knowledge question about the text? (e.g., \"Tell me about the Roman military\").\n",
    "\n",
    "If yes, use the standard Cortex Search Path, potentially enriched with Multihop_Search_Results.\n",
    "\n",
    "Is it a question about structured data?\n",
    "\n",
    "If yes, use the Cortex Analyst Path.\n",
    "\n",
    "-- ANY TIME the Multihop_Search_Results comes back with connected_pages to get more information.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041fa73c-d568-4e2c-ad88-35100435348b",
   "metadata": {
    "language": "python",
    "name": "Setup"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Configuration\n",
    "import io\n",
    "import os\n",
    "import tempfile\n",
    "import json\n",
    "import requests\n",
    "import pypdf\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.exceptions import SnowparkSQLException\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# --- Configuration ---\n",
    "SNOWFLAKE_DATABASE = \"WORLD_HISTORY\"\n",
    "SNOWFLAKE_SCHEMA = \"public\"\n",
    "SNOWFLAKE_ROLE = \"SYSADMIN\"\n",
    "TARGET_WEBSITE_URL = \"https://glhssocialstudies.weebly.com/world-history-textbook---pdf-copy.html\"\n",
    "\n",
    "# Define stage names WITHOUT a leading '@'\n",
    "SOURCE_STAGE_NAME = \"pdf_documents\"\n",
    "\n",
    "# Dynamic names based on database\n",
    "EXTERNAL_ACCESS_INTEGRATION_NAME = f\"{SNOWFLAKE_DATABASE}_WEB_ACCESS\"\n",
    "NETWORK_RULE_NAME = f\"{SNOWFLAKE_DATABASE}_WEBSITE_ACCESS\"\n",
    "\n",
    "ADAPTIVE_SPLIT_TARGET_PATH = f\"{SOURCE_STAGE_NAME}/parts\"\n",
    "SINGLE_PAGE_TARGET_PATH = f\"{SOURCE_STAGE_NAME}/pages\"\n",
    "\n",
    "# --- Session Initialization ---\n",
    "session = get_active_session()\n",
    "session.sql(f\"CREATE DATABASE IF NOT EXISTS {SNOWFLAKE_DATABASE};\").collect()\n",
    "session.use_database(SNOWFLAKE_DATABASE)\n",
    "session.use_schema(SNOWFLAKE_SCHEMA)\n",
    "\n",
    "print(f\"‚úÖ Setup complete.\")\n",
    "print(f\"  - Current Role: {session.get_current_role()}\")\n",
    "print(f\"  - Configured Role: {SNOWFLAKE_ROLE}\")\n",
    "print(f\"  - Database: {session.get_current_database()}\")\n",
    "print(f\"  - Schema: {session.get_current_schema()}\")\n",
    "print(f\"  - Target Website: {TARGET_WEBSITE_URL}\")\n",
    "print(f\"  - Source Stage: @{SOURCE_STAGE_NAME}\")\n",
    "print(f\"  - External Access Integration: {EXTERNAL_ACCESS_INTEGRATION_NAME}\")\n",
    "print(f\"  - Network Rule: {NETWORK_RULE_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a28882a-4dc5-416b-84ab-5be8d789625a",
   "metadata": {
    "language": "python",
    "name": "create_stage"
   },
   "outputs": [],
   "source": "# Create the PDF documents stage if it doesn't exist\nsession.sql(f\"\"\"\n    CREATE STAGE IF NOT EXISTS {SOURCE_STAGE_NAME}\n        ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE')\n        DIRECTORY = (\n           ENABLE = TRUE\n           AUTO_REFRESH = TRUE\n        )\n        COMMENT = 'Stage for PDF documents'\n\"\"\").collect()\nprint(f\"‚úÖ Stage @{SOURCE_STAGE_NAME} created/verified\")"
  },
  {
   "cell_type": "markdown",
   "id": "23e5dfe7-b092-49b4-99e4-76397a802364",
   "metadata": {
    "name": "manual_file_upload_skip_instructions",
    "collapsed": false
   },
   "source": "# Manual File Upload - Skip to `file_upload_complete`\n\n## Download through Snowflake - Option 1\n\nContinue to run the cells if you want to download the PDF files through Snowflake.  This requires AccountAdmin access to create the network rule and integration.  \n\n## Download to a local machine and upload - Option 2\n\nTo download manually, download (from the sidebar) and run `python3 pdf_downloader.py` (after installing dependencies) and then upload the files to the stage @PDF_DOCUMENTS in the database.  After you complete the upload, continue at cell `file_upload_complete`."
  },
  {
   "cell_type": "markdown",
   "id": "03628a64-096c-49c3-b9e3-916955bee1e9",
   "metadata": {
    "name": "external_integration_md",
    "collapsed": false
   },
   "source": "#  üö® üö® Add external integration to the notebook  üö® üö®\n\nThe next cell will switch to AccountAdmin and create the proper network rule and access integration.\n\nFollow these instructions:\n1. Run the following cell, _and only the next cell_, to create the network access.\n2. Open Notebook Settings -> External Access and enable \"World_History_Web_Access\" (or the downloads will fail.)\n3. Run the `Setup` cell again\n4. Skip to the `continue_here_for_snowflake_upload` cell and continue running the notebook\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5896eafa-ef42-44e6-9704-009bd22391f2",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "external_access"
   },
   "outputs": [],
   "source": [
    "# Cell 2: External Access Setup\n",
    "print(\"üîê Setting up external access with proper role management...\")\n",
    "\n",
    "# Switch to ACCOUNTADMIN for setup\n",
    "session.use_role(\"ACCOUNTADMIN\")\n",
    "print(f\"   Switched to role: {session.get_current_role()}\")\n",
    "\n",
    "# Extract domain from the configured URL\n",
    "target_domain = urlparse(TARGET_WEBSITE_URL).netloc\n",
    "print(f\"   Target domain: {target_domain}\")\n",
    "\n",
    "# IMPORTANT: Also allow icomets.org where the PDFs are actually hosted\n",
    "pdf_domain = \"icomets.org\"\n",
    "print(f\"   PDF domain: {pdf_domain}\")\n",
    "\n",
    "try:\n",
    "    # Create network rule for BOTH the target website AND the PDF hosting domain\n",
    "    session.sql(f\"\"\"\n",
    "        CREATE OR REPLACE NETWORK RULE {NETWORK_RULE_NAME}\n",
    "        MODE = EGRESS\n",
    "        TYPE = HOST_PORT\n",
    "        VALUE_LIST = ('{target_domain}:443', '{target_domain}:80', '{pdf_domain}:443', '{pdf_domain}:80')\n",
    "    \"\"\").collect()\n",
    "    print(f\"‚úÖ Network rule created: {NETWORK_RULE_NAME} for {target_domain} + {pdf_domain}\")\n",
    "    \n",
    "    # Create external access integration using configured variables\n",
    "    session.sql(f\"\"\"\n",
    "        CREATE OR REPLACE EXTERNAL ACCESS INTEGRATION {EXTERNAL_ACCESS_INTEGRATION_NAME}\n",
    "        ALLOWED_NETWORK_RULES = ({NETWORK_RULE_NAME})\n",
    "        ENABLED = TRUE\n",
    "    \"\"\").collect()\n",
    "    print(f\"‚úÖ External access integration created: {EXTERNAL_ACCESS_INTEGRATION_NAME}\")\n",
    "\n",
    "    # Grant access for the role to use the access integration\n",
    "    session.sql(f\"\"\"\n",
    "        GRANT USAGE ON INTEGRATION {EXTERNAL_ACCESS_INTEGRATION_NAME} to {SNOWFLAKE_ROLE}\n",
    "    \"\"\").collect()\n",
    "    print(f\"‚úÖ Integration {EXTERNAL_ACCESS_INTEGRATION_NAME} access granted to {SNOWFLAKE_ROLE}\")\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during setup: {str(e)}\")\n",
    "\n",
    "finally:\n",
    "    # Switch back to configured role\n",
    "    try:\n",
    "        session.use_role(SNOWFLAKE_ROLE)\n",
    "        print(f\"‚úÖ Switched back to role: {session.get_current_role()}\")\n",
    "    except Exception as role_error:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Could not switch to {SNOWFLAKE_ROLE}: {role_error}\")\n",
    "\n",
    "print(f\"üîß External access setup completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe7da72-4ee0-4edc-86b5-cb23f4a3a1d4",
   "metadata": {
    "collapsed": false,
    "name": "continue_here_for_snowflake_upload"
   },
   "source": "#  üö® üö® Upload your PDF Documents to the Stage using Snowflake  üö® üö®\n\n\nContinue with this cell after enabling the External Access Integration and running the `Setup` cell again."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f1003f-f459-4f8d-88af-e3a3f8342623",
   "metadata": {
    "language": "sql",
    "name": "get_pdf_links_from_website_fn"
   },
   "outputs": [],
   "source": "-- Scrapes website to get PDF download links\nCREATE OR REPLACE FUNCTION get_pdf_links_from_website(website_url STRING)\nRETURNS VARIANT\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.12'\nEXTERNAL_ACCESS_INTEGRATIONS = (WORLD_HISTORY_WEB_ACCESS)\nPACKAGES = ('requests', 'beautifulsoup4', 'lxml')\nHANDLER = 'scrape_pdfs'\nAS\n$$\nimport requests\nimport re\nimport json\nfrom urllib.parse import urljoin, urlparse\nfrom bs4 import BeautifulSoup\n\ndef scrape_pdfs(website_url):\n    try:\n        # Fetch webpage\n        headers = {\n            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n        }\n        \n        response = requests.get(website_url, headers=headers, timeout=30)\n        response.raise_for_status()\n        \n        soup = BeautifulSoup(response.text, 'html.parser')\n        pdf_links = []\n        \n        # Method 1: Direct PDF links\n        for link in soup.find_all('a', href=True):\n            href = link['href']\n            if href.lower().endswith('.pdf'):\n                full_url = urljoin(website_url, href)\n                filename = urlparse(href).path.split('/')[-1]\n                text = link.get_text(strip=True)\n                \n                pdf_links.append({\n                    'url': full_url,\n                    'text': text,\n                    'filename': filename,\n                    'method': 'direct_link'\n                })\n        \n        # Method 2: Regex patterns if no direct links found\n        if len(pdf_links) == 0:\n            content = response.text\n            pdf_patterns = [\n                r'https?://[^\"\\s]+\\.pdf',\n                r'/files/[^\"\\s]+\\.pdf',\n                r'uploads/[^\"\\s]+\\.pdf'\n            ]\n            \n            found_urls = set()\n            for pattern in pdf_patterns:\n                matches = re.findall(pattern, content, re.IGNORECASE)\n                for match in matches:\n                    url = match.strip('\"\\'')\n                    if url.startswith('/'):\n                        url = urljoin(website_url, url)\n                    found_urls.add(url)\n            \n            for i, url in enumerate(found_urls, 1):\n                filename = urlparse(url).path.split('/')[-1]\n                if not filename or not filename.endswith('.pdf'):\n                    filename = f\"chapter_{i:02d}.pdf\"\n                \n                pdf_links.append({\n                    'url': url,\n                    'text': f'Chapter {i}',\n                    'filename': filename,\n                    'method': 'regex_pattern'\n                })\n        \n        return {\n            \"success\": True,\n            \"website_url\": website_url,\n            \"pdf_count\": len(pdf_links),\n            \"pdf_links\": pdf_links,\n            \"scraped_at\": str(response.headers.get('date', 'unknown'))\n        }\n        \n    except Exception as e:\n        return {\n            \"success\": False,\n            \"error\": str(e),\n            \"website_url\": website_url,\n            \"pdf_count\": 0,\n            \"pdf_links\": []\n        }\n$$;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf42af-37b0-42ff-b01f-731b44c46adc",
   "metadata": {
    "language": "python",
    "name": "download_pdfs_to_stage"
   },
   "outputs": [],
   "source": "# Python code to download source files\nprint(f\"üì• Downloading PDFs to @{SOURCE_STAGE_NAME}\")\n\n# Clean up existing stage files first\ntry:\n    session.sql(f\"REMOVE @{SOURCE_STAGE_NAME}\").collect()\n    print(f\"üßπ Cleaned up existing stage files\")\nexcept:\n    pass\n\n# Get PDF links\ntry:\n    result = session.sql(f\"SELECT get_pdf_links_from_website('{TARGET_WEBSITE_URL}') as result\").collect()\n    \n    if result:\n        result_raw = result[0]['RESULT']\n        if isinstance(result_raw, str):\n            result_data = json.loads(result_raw)\n        else:\n            result_data = result_raw\n        \n        if result_data and result_data.get('success', False):\n            pdf_links = result_data.get('pdf_links', [])\n            total_pdfs = len(pdf_links)\n            \n            print(f\"üìä Found {total_pdfs} PDFs to download\")\n            \n            success_count = 0\n            error_count = 0\n            \n            for i, pdf_info in enumerate(pdf_links, 1):\n                pdf_url = pdf_info['url']\n                filename = pdf_info['filename']\n                \n                print(f\"\\nüìÑ {i}/{total_pdfs}: {filename}\")\n                \n                # Check available space\n                try:\n                    temp_dir = tempfile.gettempdir()\n                    _, _, free_space = shutil.disk_usage(temp_dir)\n                    free_mb = free_space / (1024**2)\n                    \n                    if free_mb < 150:\n                        print(f\"   ‚ö†Ô∏è  Low space ({free_mb:.1f} MB) - cleaning up...\")\n                        # Clean up any leftover temp files\n                        for temp_file in os.listdir(temp_dir):\n                            if temp_file.endswith('.pdf') and 'tmp' in temp_file:\n                                try:\n                                    os.unlink(os.path.join(temp_dir, temp_file))\n                                except:\n                                    pass\n                        \n                        if free_mb < 100:\n                            print(f\"   ‚ùå Insufficient space - skipping\")\n                            error_count += 1\n                            continue\n                            \n                except Exception:\n                    pass\n                \n                temp_path = None\n                try:\n                    headers = {\n                        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n                    }\n                    \n                    # Streaming download\n                    with requests.get(pdf_url, headers=headers, timeout=90, stream=True) as response:\n                        response.raise_for_status()\n                        \n                        # Create temp file with correct name for proper upload\n                        temp_dir = tempfile.gettempdir()\n                        temp_path = os.path.join(temp_dir, filename)\n                        \n                        try:\n                            total_size = 0\n                            with open(temp_path, 'wb') as temp_file:\n                                for chunk in response.iter_content(chunk_size=8192):\n                                    if chunk:\n                                        temp_file.write(chunk)\n                                        total_size += len(chunk)\n                            \n                            print(f\"   üì¶ Downloaded {total_size:,} bytes ({total_size/1024/1024:.1f} MB)\")\n                            \n                            if total_size < 10000:\n                                raise Exception(f\"File too small: {total_size} bytes\")\n                            \n                            # Upload with proper filename and no compression\n                            put_result = session.file.put(\n                                local_file_name=temp_path,\n                                stage_location=f\"@{SOURCE_STAGE_NAME}\",\n                                auto_compress=False,\n                                overwrite=True\n                            )\n                            \n                            if put_result and put_result[0].status == 'UPLOADED':\n                                print(f\"   ‚úÖ Uploaded {filename}\")\n                                success_count += 1\n                            else:\n                                print(f\"   ‚ö†Ô∏è  Upload failed\")\n                                error_count += 1\n                            \n                        finally:\n                            if temp_path and os.path.exists(temp_path):\n                                os.unlink(temp_path)\n                    \n                except Exception as e:\n                    print(f\"   ‚ùå Error: {str(e)}\")\n                    error_count += 1\n                    if temp_path and os.path.exists(temp_path):\n                        try:\n                            os.unlink(temp_path)\n                        except:\n                            pass\n                \n                # Progress every 5 files\n                if i % 5 == 0 or i == total_pdfs:\n                    print(f\"\\nüìà Progress: {i}/{total_pdfs}, ‚úÖ{success_count} success, ‚ùå{error_count} errors\")\n            \n            # Final verification  \n            print(f\"\\nüìÅ Stage verification...\")\n            stage_files = session.sql(f\"LIST @{SOURCE_STAGE_NAME}\").collect()\n            \n            if stage_files:\n                print(f\"‚úÖ {len(stage_files)} files in @{SOURCE_STAGE_NAME}:\")\n                total_size = 0\n                \n                # Sort files by name for better display\n                sorted_files = sorted(stage_files, key=lambda x: x['name'])\n                \n                for file_info in sorted_files:\n                    # Get the name field and clean it up\n                    full_path = file_info['name']\n                    \n                    # Handle different path formats from LIST command\n                    if full_path.startswith('pdf_documents/'):\n                        name = full_path.replace('pdf_documents/', '')\n                    elif '/' in full_path:\n                        name = full_path.split('/')[-1]\n                    else:\n                        name = full_path\n                    \n                    size = file_info['size']\n                    total_size += size\n                    print(f\"   üìÑ {name} - {size:,} bytes ({size/1024/1024:.1f} MB)\")\n                \n                print(f\"\\nüéâ SUCCESS! Total: {total_size:,} bytes ({total_size/1024/1024:.1f} MB)\")\n            else:\n                print(f\"‚ùå No files in stage\")\n                \n        else:\n            print(\"‚ùå Failed to get PDF links\")\n    else:\n        print(\"‚ùå No result from discovery\")\n        \nexcept Exception as e:\n    print(f\"‚ùå Download error: {str(e)}\")\n\nprint(f\"\\n‚úÖ Download completed!\")\n"
  },
  {
   "cell_type": "code",
   "id": "b2467a81-d730-4555-b047-957c45b605d4",
   "metadata": {
    "language": "python",
    "name": "alter_stage_refresh"
   },
   "outputs": [],
   "source": "-- ensure Stage is refreshed with uploaded files\nsession.sql(f\"ALTER STAGE {SOURCE_STAGE_NAME} REFRESH\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d32b3-ec94-4ca5-b577-5d988839a46b",
   "metadata": {
    "language": "python",
    "name": "filename_chapter_associations"
   },
   "outputs": [],
   "source": "# Create Filename-Chapter Associations Table\n# Todo: These aren't used anywhere yet.  They could be added to the world_history_rag table and/or used as topics for the test questions\nimport re\nprint(\"üìã Creating filename-chapter associations table...\")\n\n# Create the table with separate chapter number and title\ntry:\n    session.sql(f\"\"\"\n        CREATE OR REPLACE TABLE FILENAME_CHAPTER_ASSOCIATIONS (\n            FILENAME VARCHAR(100),\n            CHAPTER_NUMBER INTEGER,\n            CHAPTER_TITLE VARCHAR(500),\n            ORIGINAL_TEXT VARCHAR(500),\n            UPLOAD_TIMESTAMP TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),\n            PRIMARY KEY (FILENAME)\n        )\n    \"\"\").collect()\n    print(\"‚úÖ Table FILENAME_CHAPTER_ASSOCIATIONS created/verified\")\n    \n    # Get PDF links to extract titles\n    result = session.sql(f\"SELECT get_pdf_links_from_website('{TARGET_WEBSITE_URL}') as result\").collect()\n    \n    if result:\n        result_raw = result[0]['RESULT']\n        if isinstance(result_raw, str):\n            result_data = json.loads(result_raw)\n        else:\n            result_data = result_raw\n        \n        if result_data and result_data.get('success', False):\n            pdf_links = result_data.get('pdf_links', [])\n            \n            print(f\"üì• Parsing and inserting {len(pdf_links)} filename-chapter associations...\")\n            \n            # Insert each filename-title mapping with parsed data\n            for pdf_info in pdf_links:\n                filename = pdf_info['filename']\n                original_text = pdf_info.get('text', filename)\n                \n                # Parse chapter number and title\n                # Pattern: \"Chapter X: Title (size)\" or similar\n                chapter_number = None\n                chapter_title = original_text\n                \n                # Try to extract chapter number\n                chapter_match = re.search(r'Chapter\\s+(\\d+)', original_text, re.IGNORECASE)\n                if chapter_match:\n                    chapter_number = int(chapter_match.group(1))\n                    \n                    # Extract title after the colon, before any parentheses\n                    title_match = re.search(r'Chapter\\s+\\d+:\\s*([^(]+)', original_text, re.IGNORECASE)\n                    if title_match:\n                        chapter_title = title_match.group(1).strip()\n                \n                # If no \"Chapter X:\" pattern, try to extract number from filename\n                if chapter_number is None:\n                    filename_match = re.search(r'chap(\\d+)', filename, re.IGNORECASE)\n                    if filename_match:\n                        chapter_number = int(filename_match.group(1))\n                \n                # Clean up chapter title (remove extra spaces, size info)\n                chapter_title = re.sub(r'\\s*\\(\\d+[A-Za-z]*\\)\\s*$', '', chapter_title).strip()\n                \n                try:\n                    session.sql(f\"\"\"\n                        INSERT INTO FILENAME_CHAPTER_ASSOCIATIONS \n                        (FILENAME, CHAPTER_NUMBER, CHAPTER_TITLE, ORIGINAL_TEXT)\n                        VALUES ('{filename}', {chapter_number or 'NULL'}, '{chapter_title.replace(\"'\", \"''\")}', '{original_text.replace(\"'\", \"''\")}')\n                    \"\"\").collect()\n                except Exception as e:\n                    print(f\"   ‚ö†Ô∏è  Error inserting {filename}: {str(e)}\")\n            \n            # Verify the data\n            associations = session.sql(\"SELECT * FROM FILENAME_CHAPTER_ASSOCIATIONS ORDER BY CHAPTER_NUMBER\").collect()\n            \n            print(f\"\\n‚úÖ {len(associations)} associations created:\")\n            for assoc in associations[:10]:  # Show first 10\n                filename = assoc['FILENAME']\n                chapter_num = assoc['CHAPTER_NUMBER']\n                title = assoc['CHAPTER_TITLE'][:50] + \"...\" if len(assoc['CHAPTER_TITLE']) > 50 else assoc['CHAPTER_TITLE']\n                print(f\"   üìÑ {filename:<15} ‚Üí Ch.{chapter_num:2d}: {title}\")\n            \n            if len(associations) > 10:\n                print(f\"   ... and {len(associations) - 10} more associations\")\n                \n            print(f\"\\nüéØ Query examples:\")\n            print(f\"   ‚Ä¢ SELECT * FROM FILENAME_CHAPTER_ASSOCIATIONS WHERE CHAPTER_NUMBER = 1\")\n            print(f\"   ‚Ä¢ SELECT FILENAME, CHAPTER_TITLE FROM FILENAME_CHAPTER_ASSOCIATIONS ORDER BY CHAPTER_NUMBER\")\n                \n        else:\n            print(\"‚ùå Failed to get PDF links for associations\")\n    else:\n        print(\"‚ùå No result from PDF discovery\")\n        \nexcept Exception as e:\n    print(f\"‚ùå Error creating associations: {str(e)}\")\n\nprint(f\"\\n‚úÖ Filename-chapter associations completed!\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "596f4c95-b1e5-430d-bb5e-55fe4c0dae32",
   "metadata": {
    "collapsed": false,
    "name": "file_upload_complete"
   },
   "source": [
    "# Done with file upload!\n",
    "\n",
    "If you manually uploaded files to the Snowflake stage, continue processing the files starting from the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "Helper_Functions_Adaptive_Chapter_Split"
   },
   "outputs": [],
   "source": "#\n# Helper Function (Adaptive Splitting) to chunk source files into ~25mb parts\n#\ndef get_page_range_desc(page_labels, start_idx, end_idx):\n    start_label = page_labels[start_idx] if page_labels and start_idx < len(page_labels) else start_idx + 1\n    end_label = page_labels[end_idx] if page_labels and end_idx < len(page_labels) else end_idx + 1\n    clean_desc = f\"pages{start_label}to{end_label}\".replace('/', '_').replace(' ', '')\n    readable_desc = f\"pages {start_label}-{end_label}\"\n    return clean_desc, readable_desc\n\ndef split_large_pdf_on_stage(session, file_content_stream, original_filename, target_stage_path, max_size_mb=25):\n    original_size_mb = file_content_stream.getbuffer().nbytes / (1024 * 1024)\n    print(f\"\\nüìÑ Processing {original_filename} ({original_size_mb:.1f} MB)\")\n\n    # --- MODIFIED: DO NOT COPY SMALL FILES ---\n    if original_size_mb <= 25:\n        print(\"‚úÖ File is under 25MB and in the correct stage. No action needed.\")\n        return [{'filename': original_filename, 'page_range': 'all', 'size_mb': original_size_mb}]\n\n    # --- Logic for splitting large files remains the same ---\n    with tempfile.TemporaryDirectory() as temp_dir:\n        reader = pypdf.PdfReader(file_content_stream)\n        total_pages, page_labels = len(reader.pages), reader.page_labels\n        print(f\"Total pages: {total_pages}. Starting adaptive split...\")\n        \n        output_parts, start_page_of_chunk, part_num = [], 0, 1\n        \n        while start_page_of_chunk < total_pages:\n            size_test_writer, end_page_of_chunk = pypdf.PdfWriter(), start_page_of_chunk - 1\n            for i in range(start_page_of_chunk, total_pages):\n                size_test_writer.add_page(reader.pages[i])\n                with io.BytesIO() as buffer:\n                    size_test_writer.write(buffer)\n                    if buffer.tell() / (1024 * 1024) > max_size_mb and i > start_page_of_chunk: break\n                end_page_of_chunk = i\n            \n            final_writer = pypdf.PdfWriter()\n            for i in range(start_page_of_chunk, end_page_of_chunk + 1):\n                final_writer.add_page(reader.pages[i])\n\n            base_name = original_filename.rsplit('.', 1)[0]\n            clean_desc, readable_desc = get_page_range_desc(page_labels, start_page_of_chunk, end_page_of_chunk)\n            output_filename = f\"{base_name}_{clean_desc}.pdf\"\n            \n            temp_part_path = os.path.join(temp_dir, output_filename)\n            with open(temp_part_path, \"wb\") as f: final_writer.write(f)\n\n            final_size_mb = os.path.getsize(temp_part_path) / (1024 * 1024)\n            session.file.put(\n                local_file_name=temp_part_path,\n                #stage_location=f\"{target_stage_path}/{output_filename}\",\n                stage_location=f\"{target_stage_path}\",\n                auto_compress=False, overwrite=True\n            )\n            print(f\"  - Part {part_num}: Uploaded {output_filename} ({final_size_mb:.1f} MB, {readable_desc})\")\n            \n            output_parts.append({'filename': output_filename, 'page_range': readable_desc, 'size_mb': final_size_mb})\n            part_num += 1\n            start_page_of_chunk = end_page_of_chunk + 1\n            \n    return output_parts"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "Split_Large_Files"
   },
   "outputs": [],
   "source": "#\n# Adaptive Splitting for Large Files Code\n#\nprint(\"--- Starting Task 1: Adaptive splitting for files > 25MB ---\")\nall_results = {}\ntry:\n    # pattern selects only the root directory so we don't also do anything recursive by selecting sub-directories\n    staged_files = session.sql(f\"LS @{SOURCE_STAGE_NAME} PATTERN='[^/]+'\").collect()\n    files_to_process = [f[\"name\"] for f in staged_files if f[\"name\"].lower().endswith('.pdf') and 'pages' not in f[\"name\"]]\n\n    if not files_to_process:\n        print(\"No matching PDF files found in the stage to process.\")\n    else:\n        print(f\"Found {len(files_to_process)} PDF(s) to check...\")\n        for file_path_on_stage in sorted(files_to_process):\n            try:\n                stage_file_path = f\"@{file_path_on_stage}\"\n                file_name_only = file_path_on_stage.split('/')[-1]\n                \n                with session.file.get_stream(stage_file_path) as instream:\n                    pdf_bytes_io = io.BytesIO(instream.read())\n                    parts = split_large_pdf_on_stage(\n                        session=session,\n                        file_content_stream=pdf_bytes_io,\n                        original_filename=file_name_only,\n                        target_stage_path=ADAPTIVE_SPLIT_TARGET_PATH, # Use correct target\n                    )\n                    if len(parts) > 1:\n                        all_results[file_name_only.rsplit('.', 1)[0]] = parts\n            except Exception as e:\n                print(f\"‚ùå Error processing {file_path_on_stage}: {e}\")\nexcept SnowparkSQLException as e:\n    print(f\"‚ùå SQL Error: Could not list files in stage '@{SOURCE_STAGE_NAME}'. Please check permissions.\")\n    print(e)\nprint(\"\\n--- Task 1 Complete ---\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f83429-288b-47d5-b86d-b2a6f105a8be",
   "metadata": {
    "language": "python",
    "name": "Helper_Function_Individual_Files"
   },
   "outputs": [],
   "source": "#\n# Single-Page Splitting Helper Function\n#\ndef get_page_label(pdf_reader, page_num):\n    \"\"\"Extracts the page label for a given page number.\"\"\"\n    try:\n        return pdf_reader.page_labels[page_num]\n    except (IndexError, KeyError):\n        return None\n\ndef split_pdf_to_single_pages_on_stage(session, file_content_stream, original_filename, target_stage_path):\n    \"\"\"Splits a PDF from a stream into single pages and uploads them to a stage directory.\"\"\"\n    print(f\"\\nüìÑ Splitting {original_filename} into single pages...\")\n    \n    with tempfile.TemporaryDirectory() as temp_dir:\n        reader = pypdf.PdfReader(file_content_stream)\n        total_pages = len(reader.pages)\n        print(f\"Total pages: {total_pages}\")\n\n        base_name = original_filename.rsplit('.', 1)[0]\n        page_upload_info = []\n\n        for page_num in range(total_pages):\n            pdf_writer = pypdf.PdfWriter()\n            pdf_writer.add_page(reader.pages[page_num])\n            \n            page_label = get_page_label(reader, page_num)\n            \n            if page_label:\n                clean_label = str(page_label).replace('/', '_').replace('\\\\', '_')\n                try:\n                    # Attempt to convert and format\n                    page_number_str = f\"{int(clean_label):04d}\"\n                    output_filename = f\"{base_name}_page{page_number_str}.pdf\"\n                except ValueError:\n                    # Handle cases where clean_label is not a valid number\n                    output_filename = f\"{base_name}_page{clean_label}.pdf\" \n            else:\n                output_filename = f\"{base_name}_page{page_num+1:04d}_nolabel.pdf\"\n            \n            temp_page_path = os.path.join(temp_dir, output_filename)\n            with open(temp_page_path, \"wb\") as f:\n                pdf_writer.write(f)\n\n            session.file.put(\n                local_file_name=temp_page_path,\n                #stage_location=f\"{target_stage_path}/{output_filename}\",\n                stage_location=f\"{target_stage_path}\",\n                auto_compress=False, overwrite=True\n            )\n            page_upload_info.append({'filename': output_filename, 'page_label': page_label})\n            \n            if (page_num + 1) % 20 == 0 or page_num == total_pages - 1:\n                print(f\"  ...uploaded {page_num+1}/{total_pages} pages\")\n    \n    print(f\"‚úÖ Split into {len(page_upload_info)} individual pages in @{target_stage_path}\")\n    return page_upload_info"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b980e-c5d2-4e9f-a156-33954d1ff1c8",
   "metadata": {
    "language": "python",
    "name": "Single_Page_Split"
   },
   "outputs": [],
   "source": "#\n# Single-Page Splitting Main Code\n#\nprint(\"\\n--- Starting Task 2: Splitting all original files into single pages ---\")\nsingle_page_results = {} # To store results for the summary\n\ntry:\n    # the pattern select only from the root; otherwise we get recursive splitting and sub-directory files\n    staged_files = session.sql(f\"LS @{SOURCE_STAGE_NAME} PATTERN='[^/]+'\").collect()\n    # Find original PDFs, excluding any adaptively split parts\n    files_to_process = [f[\"name\"] for f in staged_files if f[\"name\"].lower().endswith('.pdf') and '_pages' not in f[\"name\"]]\n\n    if not files_to_process:\n        print(\"No original PDF files found in the stage to process.\")\n    else:\n        print(f\"Found {len(files_to_process)} original PDF(s) to split into single pages...\")\n        for file_path_on_stage in sorted(files_to_process):\n            try:\n                stage_file_path = f\"@{file_path_on_stage}\"\n                file_name_only = file_path_on_stage.split('/')[-1]\n                \n                with session.file.get_stream(stage_file_path) as instream:\n                    pdf_bytes_io = io.BytesIO(instream.read())\n                    # Capture the returned info\n                    page_upload_info = split_pdf_to_single_pages_on_stage(\n                        session=session,\n                        file_content_stream=pdf_bytes_io,\n                        original_filename=file_name_only,\n                        target_stage_path=SINGLE_PAGE_TARGET_PATH,\n                    )\n                    single_page_results[file_name_only] = page_upload_info\n            except Exception as e:\n                print(f\"‚ùå Error splitting {file_path_on_stage} into single pages: {e}\")\nexcept SnowparkSQLException as e:\n    print(f\"‚ùå SQL Error: Could not list files in stage '@{SOURCE_STAGE_NAME}'. Please check permissions.\")\n    print(e)\nprint(\"\\n--- Task 2 Complete ---\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "Summary"
   },
   "outputs": [],
   "source": "#\n# Final Summary of Adaptive Page Split and Single Page Split\n#\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ All Tasks Complete\")\nprint(\"=\"*60)\n\n# --- Summary for Task 1: Adaptive Splitting ---\nprint(\"\\nüìã Summary of Adaptive Splitting (Task 1)\")\nif all_results:\n    total_parts = sum(len(parts) for parts in all_results.values())\n    print(f\"  - Total: {len(all_results)} large PDF(s) were split into {total_parts} parts.\")\n    print(f\"  - Destination: @{ADAPTIVE_SPLIT_TARGET_PATH}/\")\nelse:\n    print(\"  - No files required adaptive splitting.\")\n\n# --- Summary for Task 2: Single-Page Splitting ---\nprint(\"\\nüìã Summary of Single-Page Splitting (Task 2)\")\nif single_page_results:\n    total_pages_created = sum(len(pages) for pages in single_page_results.values())\n    print(f\"  - Total: {len(single_page_results)} original PDF(s) were split into {total_pages_created} single pages.\")\n    print(f\"  - Destination: @{SINGLE_PAGE_TARGET_PATH}/\")\nelse:\n    print(\"  - Single-page splitting was not run or no files were processed.\")\n\nprint(\"\\n\" + \"=\"*60)"
  },
  {
   "cell_type": "code",
   "id": "c7f61265-81fe-499f-968b-a29976ed76df",
   "metadata": {
    "language": "python",
    "name": "alter_stage_refresh_2"
   },
   "outputs": [],
   "source": "session.sql(f\"ALTER STAGE {SOURCE_STAGE_NAME} REFRESH\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6de915c2-44f4-4586-aaf7-aa6b5dfc5b92",
   "metadata": {
    "collapsed": false,
    "name": "document_tables_md"
   },
   "source": "# Start Table Creation for Document Analysis\n\n- Document_pages = information for each page\n- Document_parts = information on sections/chunks\n- Document_analysis = links to related nodes\n- Document_edges = graph of related nodes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f970f-2ae0-4026-9ed1-1cda6a00c17d",
   "metadata": {
    "language": "sql",
    "name": "create_document_processing_tables"
   },
   "outputs": [],
   "source": [
    "-- Main table for individual document pages\n",
    "CREATE OR REPLACE TABLE DOCUMENT_PAGES (\n",
    "    PAGE_ID STRING PRIMARY KEY,\n",
    "    CHAPTER_NUMBER INTEGER,\n",
    "    PAGE_NUMBER INTEGER,                    -- Actual page number from PDF\n",
    "    PART_IDENTIFIER STRING,                 -- Links pages to their parent parts (e.g., 'pages64to76')\n",
    "    CHAPTER_TITLE STRING,\n",
    "    -- UNIT_NUMBER INTEGER,\n",
    "    PAGE_CONTENT_RAW TEXT,\n",
    "    PAGE_CONTENT TEXT,                      -- Content for this specific page\n",
    "    PAGE_SUMMARY TEXT,                      -- AI-generated page summary\n",
    "    PAGE_KEYWORDS ARRAY,                    -- Key terms on this page\n",
    "    --CONTENT_VECTOR VECTOR(FLOAT, 768),     -- Page embedding for search\n",
    "    \n",
    "    -- Content metadata\n",
    "    CONTENT_TYPE STRING,                    -- 'text_heavy', 'visual_heavy', 'mixed'\n",
    "    WORD_COUNT INTEGER,\n",
    "    CHAR_COUNT INTEGER,\n",
    "    \n",
    "    -- URL and citation\n",
    "    CHAPTER_PDF_URL STRING,                 -- Presigned URL to PDF\n",
    "    SOURCE_FILENAME STRING,                 -- Original source filename (RELATIVE_PATH)\n",
    "    ENHANCED_CITATION STRING,               -- \"Chapter Title, Chapter X, Page Y\"\n",
    "    \n",
    "    -- Processing metadata\n",
    "    PROCESSING_STATUS STRING DEFAULT 'PENDING',\n",
    "    PROCESSING_TIMESTAMP TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()\n",
    ");\n",
    "\n",
    "-- ================================================================================\n",
    "-- HIERARCHICAL RAG STAGING TABLE - For chapter and part-level content\n",
    "-- ================================================================================\n",
    "\n",
    "-- Staging table for chapter and multi-page part PDFs\n",
    "CREATE OR REPLACE TABLE DOCUMENT_PARTS (\n",
    "    CHAPTER_NUMBER INTEGER,\n",
    "    PART_IDENTIFIER STRING,      -- e.g., 'pages64to76', 'full' for complete chapters\n",
    "    PART_CONTENT_RAW STRING,     -- Raw text extracted from PDF\n",
    "    SOURCE_FILENAME STRING,      -- Original filename\n",
    "    PDF_URL STRING,              -- Presigned URL to the source PDF\n",
    "    \n",
    "    -- Processing metadata\n",
    "    WORD_COUNT INTEGER,\n",
    "    CHAR_COUNT INTEGER,\n",
    "    ENHANCED_CITATION STRING,\n",
    "    PROCESSING_STATUS STRING DEFAULT 'PENDING',\n",
    "    CREATED_TIMESTAMP TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()\n",
    ")\n",
    "COMMENT = 'Staging table for chapter-level and multi-page part PDFs used in hierarchical RAG pipeline';\n",
    "\n",
    "-- AI analysis results for cross-reference extraction\n",
    "CREATE OR REPLACE TABLE DOCUMENT_ANALYSIS (\n",
    "    PAGE_ID STRING PRIMARY KEY,\n",
    "    CHAPTER_NUMBER INTEGER,\n",
    "    PAGE_NUMBER INTEGER,\n",
    "    PAGE_CONTENT TEXT,\n",
    "    EXTRACTED_REFERENCES VARIANT,          -- JSON array of cross-references\n",
    "    AI_MODEL STRING,\n",
    "    PROCESSING_TIMESTAMP TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()\n",
    ");\n",
    "\n",
    "-- Knowledge graph edges between pages/chapters\n",
    "CREATE OR REPLACE TABLE DOCUMENT_EDGES (\n",
    "    EDGE_ID STRING DEFAULT CONCAT('EDGE_', UNIFORM(1, 999999999, RANDOM())) PRIMARY KEY,\n",
    "    \n",
    "    -- Source page\n",
    "    SRC_PAGE_ID STRING,\n",
    "    SRC_CHAPTER_NUMBER INTEGER,\n",
    "    SRC_PAGE_NUMBER INTEGER,\n",
    "    \n",
    "    -- Destination page (may not exist yet)\n",
    "    DST_PAGE_ID STRING,                     -- NULL if referenced page doesn't exist\n",
    "    DST_CHAPTER_NUMBER INTEGER,\n",
    "    DST_PAGE_NUMBER INTEGER,\n",
    "    \n",
    "    -- Reference details\n",
    "    REFERENCE_TYPE STRING,                  -- 'page_reference', 'chapter_reference', 'figure_reference'\n",
    "    REFERENCE_CONTEXT STRING,              -- \"see page 759\", \"discussed in Chapter 24\"\n",
    "    REFERENCE_EXPLANATION STRING,          -- AI explanation of the connection\n",
    "    CONFIDENCE_SCORE FLOAT,\n",
    "    \n",
    "    PROCESSING_TIMESTAMP TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()\n",
    ");\n",
    "\n",
    "-- ================================================================================\n",
    "-- HIERARCHICAL RAG TABLE - Final unified table for multi-granularity RAG\n",
    "-- ================================================================================\n",
    "\n",
    "-- Final table for hierarchical multi-hop RAG - stores content at all granularity levels\n",
    "CREATE OR REPLACE TABLE WORLD_HISTORY_RAG (\n",
    "    CHAPTER_NUMBER INTEGER,\n",
    "    PART_IDENTIFIER STRING,     -- e.g., 'pages64to76', 'full', or NULL for chapter-wide summaries\n",
    "    PAGE_NUMBER INTEGER,        -- The specific page number, or NULL for part/chapter summaries\n",
    "    CONTENT_TYPE STRING,        -- 'ChapterSummary', 'PartSummary', 'PageSummary', 'RawText'\n",
    "    TEXT_CONTENT STRING,        -- The actual text or summary\n",
    "    SOURCE_FILENAME STRING,     -- The original file the content was extracted from\n",
    "    PDF_URL STRING,             -- A presigned URL to the source PDF\n",
    "    PAGE_ID STRING,             -- The page ID used in multi-hop search\n",
    "    ENHANCED_CITATION STRING,   -- \"Chapter Title, Chapter X, Page Y\"\n",
    "    -- Processing metadata\n",
    "    CREATED_TIMESTAMP TIMESTAMP_LTZ DEFAULT CURRENT_TIMESTAMP()\n",
    ")    \n",
    "    COMMENT = 'Hierarchical RAG table storing content at multiple granularities: raw text, page summaries, part summaries, and chapter summaries';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b36759-2513-4a75-9fa8-4a3c9f88706e",
   "metadata": {
    "language": "sql",
    "name": "ingest_document_pages"
   },
   "outputs": [],
   "source": "-- Cortex Parse_Document to extract text from individual pages\nINSERT INTO DOCUMENT_PAGES (\n    PAGE_ID, CHAPTER_NUMBER, PAGE_NUMBER, PART_IDENTIFIER, CHAPTER_TITLE,\n     PAGE_CONTENT_RAW, PAGE_CONTENT, CONTENT_TYPE, WORD_COUNT, CHAR_COUNT, CHAPTER_PDF_URL,\n    SOURCE_FILENAME, ENHANCED_CITATION, PROCESSING_STATUS\n)\nWITH page_processing AS (\n    SELECT\n        -- Extract chapter and page numbers using regex\n        ZEROIFNULL(REGEXP_SUBSTR(RELATIVE_PATH, 'chap(\\\\d+)', 1, 1, 'ie', 1))::INT AS chapter_number,\n        REGEXP_SUBSTR(RELATIVE_PATH, 'page[A-Z]*(\\\\d+)', 1, 1, 'ie', 1)::INT AS page_number,\n        \n        -- Extract part identifier if this page belongs to a multi-page chunk (usually NULL for individual pages)\n        REGEXP_SUBSTR(RELATIVE_PATH, 'pages(\\\\d+to\\\\d+)', 1, 1, 'ie', 1) as part_identifier,\n        \n        -- Generate consistent page ID\n        'CH' || LPAD(ZEROIFNULL(REGEXP_SUBSTR(RELATIVE_PATH, 'chap(\\\\d+)', 1, 1, 'ie', 1))::INT, 2, '0') || \n        '_P' || LPAD(REGEXP_SUBSTR(RELATIVE_PATH, 'page[A-Z]*(\\\\d+)', 1, 1, 'ie', 1)::INT, 4, '0') as page_id,\n        \n        -- Parse PDF content\n        SNOWFLAKE.CORTEX.PARSE_DOCUMENT('@PDF_DOCUMENTS', RELATIVE_PATH, {'mode': 'LAYOUT'}) as parse_result,\n        RELATIVE_PATH,\n        \n        -- Generate presigned URL\n        GET_PRESIGNED_URL('@PDF_DOCUMENTS', RELATIVE_PATH, 604800) as pdf_url\n    FROM\n        DIRECTORY(@PDF_DOCUMENTS)\n    WHERE\n        RELATIVE_PATH LIKE 'pages/%.pdf'\n        AND page_id\n            NOT IN (SELECT PAGE_ID FROM DOCUMENT_PAGES WHERE PAGE_ID IS NOT NULL)  -- Idempotency check\n)\nSELECT\n    -- Core identifiers\n    page_id,\n    chapter_number,\n    page_number,\n    part_identifier,\n    'Chapter ' || chapter_number as chapter_title,\n    \n    \n    -- Content fields\n    COALESCE(parse_result['content']::STRING, '') as page_content_raw,\n    COALESCE(parse_result['content']::STRING, '') as page_content,  -- Same as raw for now\n\n    \n    -- Content metadata\n    CASE \n        WHEN LENGTH(TRIM(COALESCE(parse_result['content']::STRING, ''))) < 100 THEN 'text_light'\n        WHEN LENGTH(TRIM(COALESCE(parse_result['content']::STRING, ''))) > 2000 THEN 'text_heavy'\n        ELSE 'mixed'\n    END as content_type,\n    \n    -- Counts\n    CASE \n        WHEN LENGTH(TRIM(COALESCE(parse_result['content']::STRING, ''))) = 0 THEN 0\n        ELSE ARRAY_SIZE(SPLIT(COALESCE(parse_result['content']::STRING, ''), ' '))\n    END as word_count,\n    LENGTH(COALESCE(parse_result['content']::STRING, '')) as char_count,\n    \n    -- URLs and citations\n    pdf_url as chapter_pdf_url,\n    RELATIVE_PATH as source_filename,\n    CONCAT(\n        'Chapter ',\n        chapter_number,\n        ', Pages ',\n        REPLACE(\n        REGEXP_SUBSTR(source_filename, '\\\\d+to\\\\d+'),\n        'to',\n        '-'\n        )\n     ) AS enhanced_citation,       \n    \n    -- Processing status\n    'PROCESSED' as processing_status\n\nFROM page_processing\nWHERE chapter_number IS NOT NULL \n  AND page_number IS NOT NULL\n  AND LENGTH(TRIM(COALESCE(parse_result['content']::STRING, ''))) > 50;  -- Quality filter\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5b20be-75d5-4b79-9d58-2ee95a2290d8",
   "metadata": {
    "language": "sql",
    "name": "ingest_document_parts"
   },
   "outputs": [],
   "source": "-- Cortex Parse_Document to extract text from chunks/parts\nINSERT INTO DOCUMENT_PARTS (CHAPTER_NUMBER, PART_IDENTIFIER, PART_CONTENT_RAW, SOURCE_FILENAME, PDF_URL, WORD_COUNT, CHAR_COUNT, ENHANCED_CITATION, PROCESSING_STATUS)\nWITH parsed_docs AS (\n  SELECT\n    RELATIVE_PATH,\n    SNOWFLAKE.CORTEX.PARSE_DOCUMENT('@PDF_DOCUMENTS', RELATIVE_PATH, {'mode': 'LAYOUT'})['content']::STRING AS content_raw\n  FROM DIRECTORY(@PDF_DOCUMENTS)\n  WHERE\n    RELATIVE_PATH LIKE 'parts/%.pdf' -- Only files in parts subdirectory\n    AND RELATIVE_PATH NOT IN (SELECT SOURCE_FILENAME FROM DOCUMENT_PARTS WHERE SOURCE_FILENAME IS NOT NULL) -- Idempotency check\n)\nSELECT\n    ZEROIFNULL(REGEXP_SUBSTR(pd.RELATIVE_PATH, 'chap(\\\\d+)', 1, 1, 'ie', 1))::INT as chapter_number,\n    REGEXP_SUBSTR(pd.RELATIVE_PATH, '(pages\\\\d+to\\\\d+)', 1, 1, 'ie', 1) as part_identifier,\n    pd.content_raw,\n    pd.RELATIVE_PATH,\n    GET_PRESIGNED_URL('@PDF_DOCUMENTS', pd.RELATIVE_PATH, 604800),\n    -- Calculate word and character counts from the content_raw field\n    ARRAY_SIZE(SPLIT(pd.content_raw, ' ')),\n    LENGTH(pd.content_raw),\n    'Chapter ' || chapter_number || ', Part ' || part_identifier as enhanced_citation,\n    'PROCESSED'\nFROM parsed_docs AS pd\nWHERE\n    ZEROIFNULL(REGEXP_SUBSTR(pd.RELATIVE_PATH, 'chap(\\\\d+)', 1, 1, 'ie', 1))::INT IS NOT NULL; -- Must have a valid chapter number"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7607d3e-050f-431b-988f-57765727d36f",
   "metadata": {
    "language": "sql",
    "name": "process_page_rawtext"
   },
   "outputs": [],
   "source": "-- Insert the raw text from each page into world_history_rag\nINSERT INTO WORLD_HISTORY_RAG (CHAPTER_NUMBER, PART_IDENTIFIER, PAGE_NUMBER, CONTENT_TYPE, TEXT_CONTENT, SOURCE_FILENAME, PDF_URL, PAGE_ID, ENHANCED_CITATION)\n\n-- First, select and insert the raw text for each page\nSELECT\n    p.CHAPTER_NUMBER, \n    p.PART_IDENTIFIER, \n    p.PAGE_NUMBER, \n    'RawText' AS CONTENT_TYPE,\n    AI_COMPLETE(\n        'llama3.3-70b', \n        'Return the raw text of the page with obvious errors fixed.  This includes spelling errors, hyphenated words, combined words, etc.  If and only a part of the text is not clear you can try to figure out what it means, but if there is not enough context then just return the raw text.  If any historical names, dates, etc seem way off try to correct it but do not make up anything.  Do NOT add any additional text or commentary like \"Here is the text with obvious errors fixed\".  The text will be used later in a search engine so it should be as close to the original text as possible. :\\n' || p.PAGE_CONTENT_RAW\n    ) as PAGE_CONTENT_RAW,\n    p.SOURCE_FILENAME, \n    p.CHAPTER_PDF_URL,\n    'CH' || LPAD(p.CHAPTER_NUMBER, 2, '0') || '_P' || LPAD(p.PAGE_NUMBER, 4, '0') as PAGE_ID,\n    p.ENHANCED_CITATION\nFROM DOCUMENT_PAGES p\nWHERE p.PROCESSING_STATUS = 'PROCESSED'\n  AND p.PAGE_CONTENT_RAW IS NOT NULL\n  AND LENGTH(p.PAGE_CONTENT_RAW) > 50;  -- Only process pages with substantial content\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa813e1-9f44-4a33-ad5f-19ad7b766a51",
   "metadata": {
    "language": "sql",
    "name": "process_page_summaries"
   },
   "outputs": [],
   "source": "-- insert AI_COMPLETE page summaries to world_history_rag\nINSERT INTO WORLD_HISTORY_RAG (CHAPTER_NUMBER, PART_IDENTIFIER, PAGE_NUMBER, CONTENT_TYPE, TEXT_CONTENT, SOURCE_FILENAME, PDF_URL, PAGE_ID, ENHANCED_CITATION)\nSELECT\n    p.CHAPTER_NUMBER, \n    p.PART_IDENTIFIER, \n    p.PAGE_NUMBER, \n    'PageSummary' AS CONTENT_TYPE,\n    AI_COMPLETE(\n        'llama3.3-70b', \n        'Do not add any additional text or commentary like \"Here is the summary\".  The text will be used later in a search engine so it should be as close to the original text as possible.  Summarize this page content in 1-2 concise sentences:\\n' || p.PAGE_CONTENT_RAW\n    ),\n    p.SOURCE_FILENAME, \n    p.CHAPTER_PDF_URL,\n    'CH' || LPAD(p.CHAPTER_NUMBER, 2, '0') || '_P' || LPAD(p.PAGE_NUMBER, 4, '0') as PAGE_ID,\n    p.ENHANCED_CITATION\nFROM DOCUMENT_PAGES p\nWHERE p.PROCESSING_STATUS = 'PROCESSED'\n  AND p.PAGE_CONTENT_RAW IS NOT NULL\n  AND LENGTH(p.PAGE_CONTENT_RAW) > 50  -- Only process pages with substantial content\n  AND CONCAT(p.CHAPTER_NUMBER, '_', p.PAGE_NUMBER, '_PageSummary') NOT IN (\n      SELECT CONCAT(CHAPTER_NUMBER, '_', PAGE_NUMBER, '_', CONTENT_TYPE) \n      FROM WORLD_HISTORY_RAG \n      WHERE CONTENT_TYPE = 'PageSummary'\n      limit 10 \n  ); -- Idempotency check for page summaries"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc8e53-3109-4506-8209-276c02568721",
   "metadata": {
    "language": "sql",
    "name": "process_part_rawtext"
   },
   "outputs": [],
   "source": "-- Cortex AI_COMPLETE inserts cleaned up text from parts into world_history_rag\nINSERT INTO WORLD_HISTORY_RAG (CHAPTER_NUMBER, PART_IDENTIFIER, PAGE_NUMBER, CONTENT_TYPE, TEXT_CONTENT, SOURCE_FILENAME, PDF_URL, PAGE_ID, ENHANCED_CITATION)\nSELECT\n    p.CHAPTER_NUMBER,\n    p.PART_IDENTIFIER,\n    NULL AS PAGE_NUMBER,  -- Parts don't have specific page numbers\n    'RawText' AS CONTENT_TYPE,\n    AI_COMPLETE(\n      'llama3.3-70b', \n      'Return the raw text of the page with obvious errors fixed.  This includes spelling errors, hyphenated words, combined words, etc.  If and only a part of the text is not clear you can try to figure out what it means, but if there is not enough context then just return the raw text.  If any historical names, dates, etc seem way off try to correct it but do not make up anything.  Do NOT add any additional text or commentary like \"Here is the text with obvious errors fixed\".  The text will be used later in a search engine so it should be as close to the original text as possible. :\\n' ||  p.PART_CONTENT_RAW\n    ) as TEXT_CONTENT,\n    p.SOURCE_FILENAME,\n    p.PDF_URL,\n    'chap' || LPAD(p.CHAPTER_NUMBER, 2, '0') || '_part' || p.PART_IDENTIFIER as PAGE_ID,\n    p.ENHANCED_CITATION\nFROM DOCUMENT_PARTS p\nWHERE p.PART_CONTENT_RAW IS NOT NULL\n  -- AND LENGTH(TRIM(p.PART_CONTENT_RAW)) > 100  -- Quality filter\n  AND CONCAT(p.CHAPTER_NUMBER, '_', p.PART_IDENTIFIER, '_RawText') NOT IN (\n      SELECT CONCAT(CHAPTER_NUMBER, '_', PART_IDENTIFIER, '_', CONTENT_TYPE) \n      FROM WORLD_HISTORY_RAG \n      WHERE CONTENT_TYPE = 'RawText' AND PART_IDENTIFIER IS NOT NULL\n  ); -- Idempotency check\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9498ef-c7a9-4c20-b9fe-7e3a95200fa4",
   "metadata": {
    "language": "sql",
    "name": "process_part_summary"
   },
   "outputs": [],
   "source": "-- AI_COMPLETE chapter summaries inserted into world_history_rag\nINSERT INTO WORLD_HISTORY_RAG (CHAPTER_NUMBER, PART_IDENTIFIER, PAGE_NUMBER, CONTENT_TYPE, TEXT_CONTENT, SOURCE_FILENAME, PDF_URL, PAGE_ID, ENHANCED_CITATION)\nSELECT\n    p.CHAPTER_NUMBER,\n    p.PART_IDENTIFIER,\n    NULL AS PAGE_NUMBER,  -- Parts don't have specific page numbers\n    'PartSummary' AS CONTENT_TYPE,\n    AI_COMPLETE(\n        'llama3.3-70b',\n        'Do not add any additional text or commentary like \"Here is the summary\".  If there is not enough context to summarize the part then just return the raw text.  Summarize the following history textbook section in 2-3 concise paragraphs. Focus on the main themes, key events, important people, and historical significance:\\n\\n' || \n        p.PART_CONTENT_RAW ||  \n        '\\n\\nProvide a clear, educational summary suitable for a history student.'\n    ) AS TEXT_CONTENT,\n    p.SOURCE_FILENAME,\n    p.PDF_URL,\n    'CH' || LPAD(p.CHAPTER_NUMBER, 2, '0') || '_' || p.PART_IDENTIFIER as PAGE_ID,\n    p.ENHANCED_CITATION\nFROM DOCUMENT_PARTS p\nWHERE p.PART_CONTENT_RAW IS NOT NULL\n  -- AND LENGTH(TRIM(p.PART_CONTENT_RAW)) > 100  -- Quality filter\n  AND CONCAT(p.CHAPTER_NUMBER, '_', p.PART_IDENTIFIER, '_PartSummary') NOT IN (\n      SELECT CONCAT(CHAPTER_NUMBER, '_', PART_IDENTIFIER, '_', CONTENT_TYPE) \n      FROM WORLD_HISTORY_RAG \n      WHERE CONTENT_TYPE = 'PartSummary' AND PART_IDENTIFIER IS NOT NULL\n  ); -- Idempotency check"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfcd2e5-2835-4696-9584-2beb5fbcfc18",
   "metadata": {
    "language": "sql",
    "name": "process_chapter_summary"
   },
   "outputs": [],
   "source": "-- AI_COMPLETE to aggregate part summaries into chapter summaries and insert into world_history_rag\n-- Explore: AI_AGG and how it compares to this method\nINSERT INTO WORLD_HISTORY_RAG (CHAPTER_NUMBER, PART_IDENTIFIER, PAGE_NUMBER, CONTENT_TYPE, TEXT_CONTENT, SOURCE_FILENAME, PDF_URL)\nWITH aggregated_part_summaries AS (\n    SELECT\n        CHAPTER_NUMBER,\n        ANY_VALUE(PDF_URL) as PDF_URL,\n        ANY_VALUE(SOURCE_FILENAME) as SOURCE_FILENAME,\n        LISTAGG(TEXT_CONTENT, '\\n\\n---\\n\\n') WITHIN GROUP (ORDER BY PART_IDENTIFIER) as full_chapter_text,\n        COUNT(*) as part_count\n    FROM WORLD_HISTORY_RAG\n    WHERE CONTENT_TYPE = 'PartSummary'\n      AND CHAPTER_NUMBER NOT IN (\n          SELECT DISTINCT CHAPTER_NUMBER \n          FROM WORLD_HISTORY_RAG \n          WHERE CONTENT_TYPE = 'ChapterSummary'\n      ) -- Idempotency check for chapter summaries\n    GROUP BY CHAPTER_NUMBER\n)\nSELECT\n    a.CHAPTER_NUMBER, \n    NULL AS PART_IDENTIFIER, \n    NULL AS PAGE_NUMBER, \n    'ChapterSummary' AS CONTENT_TYPE,\n    AI_COMPLETE(\n        'llama3.3-70b', \n        'You are given several high-level summaries from different sections of a history chapter. Combine them into a single, comprehensive 2-3 paragraph summary of the entire chapter.  Do not add any additional text or commentary like \"Here is the summary\".  The text will be used later in a search engine so it should be as close to the original text as possible. :\\n\\n' || \n        a.full_chapter_text ||\n        '\\n\\nCreate a cohesive narrative that synthesizes the key themes, events, and historical significance covered throughout this chapter.'\n    ),\n    a.SOURCE_FILENAME, \n    a.PDF_URL\nFROM aggregated_part_summaries a\nWHERE a.part_count > 0;  -- Only process chapters that have part summaries"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a8f283-3a27-4ebb-a235-008d517cbbdd",
   "metadata": {
    "language": "sql",
    "name": "update_pdf_urls_daily"
   },
   "outputs": [],
   "source": "-- Presigned URL's only last 24 hours; this task updates them daily and Cortex Search will automatically update with the latest\n-- when it also refreshes\nCREATE OR REPLACE TASK DAILY_PDF_URL_REFRESH\n    USER_TASK_MANAGED_INITIAL_WAREHOUSE_SIZE = 'XSMALL'\n    SCHEDULE = 'USING CRON 0 6 * * * UTC'  -- Daily at 6 AM UTC\n    COMMENT = 'Refresh presigned URLs that expire within 24 hours'\nAS\n    update world_history_rag\n    set pdf_url = GET_PRESIGNED_URL(@WORLD_HISTORY.PUBLIC.PDF_DOCUMENTS, source_filename, 604800);\n\nalter task daily_pdf_url_refresh resume;"
  },
  {
   "cell_type": "markdown",
   "id": "9e6f6f8d-ab84-4a24-b113-c90875f9f011",
   "metadata": {
    "collapsed": false,
    "name": "cortex_search_service_md"
   },
   "source": [
    "# Create Cortex Search Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e598532c-8e53-4c5e-acd2-9ca6ae05d5f9",
   "metadata": {
    "language": "sql",
    "name": "cortex_search_service"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE CORTEX SEARCH SERVICE WORLD_HISTORY_RAG_SEARCH\n    ON TEXT_CONTENT\n    ATTRIBUTES \n        CHAPTER_NUMBER,\n        PAGE_NUMBER,\n        SOURCE_FILENAME,\n        CONTENT_TYPE\n    WAREHOUSE = WAREHOUSE_XL_G2\n    TARGET_LAG = '1 day' -- to pick up get_presigned_url changes\n    EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\n    AS (\n        SELECT \n            wh.TEXT_CONTENT,\n            wh.CHAPTER_NUMBER,\n            wh.PAGE_NUMBER,\n            wh.PART_IDENTIFIER,\n            wh.CONTENT_TYPE,\n            wh.SOURCE_FILENAME,\n            wh.PDF_URL,\n            wh.PAGE_ID,\n            wh.ENHANCED_CITATION\n        FROM WORLD_HISTORY_RAG wh\n        WHERE wh.TEXT_CONTENT IS NOT NULL\n          AND LENGTH(TRIM(wh.TEXT_CONTENT)) > 50\n    );"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa513b-f73d-499d-8ba4-3694220b2f77",
   "metadata": {
    "language": "sql",
    "name": "multihop_view_and_fn"
   },
   "outputs": [],
   "source": "-- View to retrieve connections for each page\nCREATE OR REPLACE VIEW MULTIHOP_SEARCH_RESULTS AS\nWITH search_base AS (\n    -- All searchable content with page IDs\n    SELECT \n        page_id,\n        wh.CHAPTER_NUMBER,\n        wh.PAGE_NUMBER,\n        wh.CONTENT_TYPE,\n        'Chapter ' || wh.CHAPTER_NUMBER || COALESCE(', Page ' || wh.PAGE_NUMBER, '') || ' (' || wh.CONTENT_TYPE || ')' as citation,\n        LEFT(wh.TEXT_CONTENT, 500) as content_preview,\n        wh.PDF_URL\n    FROM WORLD_HISTORY_RAG wh\n    WHERE wh.TEXT_CONTENT IS NOT NULL\n)\nSELECT \n    sb.*,\n    COALESCE(connected.related_pages, ARRAY_CONSTRUCT()) as connected_pages,\n    COALESCE(connected.connection_count, 0) as connection_count\nFROM search_base sb\nLEFT JOIN (\n    SELECT \n        e.SRC_PAGE_ID,\n        ARRAY_AGG(OBJECT_CONSTRUCT(\n            'page_id', e.DST_PAGE_ID,\n            'citation', 'Chapter ' || e.DST_CHAPTER_NUMBER || ', Page ' || e.DST_PAGE_NUMBER,\n            'context', e.REFERENCE_CONTEXT,\n            'confidence', e.CONFIDENCE_SCORE\n        )) as related_pages,\n        COUNT(*) as connection_count\n    FROM DOCUMENT_EDGES e\n    WHERE e.CONFIDENCE_SCORE > 0.5\n    GROUP BY e.SRC_PAGE_ID\n) connected ON sb.page_id = connected.SRC_PAGE_ID;\n\n-- Function to be used as a tool by Cortex Agent\n--   Cortex Agent needs functions or procedures to be considered a tool; \n--   This wraps the view and returns it as a JSON object which is easily consumed by the Agent\n-- only takes a single unique page id\nCREATE OR REPLACE FUNCTION MULTIHOP_SEARCH_RESULTS_FN(page_id_param STRING)\nRETURNS array\nLANGUAGE SQL\nAS\n$$\n    SELECT \n    ARRAY_AGG(OBJECT_CONSTRUCT(*))\n        FROM multihop_search_results\n        WHERE page_id = page_id_param\n$$;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ecf2a-0be8-4cff-8272-334e08144b5f",
   "metadata": {
    "language": "sql",
    "name": "Find_Connected_Pages_FN"
   },
   "outputs": [],
   "source": "-- Function to traverse multiple graph connections\n-- ie - 2 hops will follow \"See page 10\" -> \"See page 20\" -> \"See page 30\"\nCREATE OR REPLACE FUNCTION FIND_CONNECTED_PAGES(starting_page_id STRING, max_hops INT)\nRETURNS array\nLANGUAGE SQL\nAS\n$$\n    WITH RECURSIVE page_traversal AS (\n        -- Starting page (hop 0)\n        SELECT \n            page_id as dest_page_id,\n            chapter_number as dest_chapter_number,\n            page_number as dest_page_number,\n            enhanced_citation,\n            ARRAY_CONSTRUCT('Starting page: ' || enhanced_citation) as connection_path,\n            0 as hop_count\n        FROM DOCUMENT_PAGES \n        WHERE page_id = starting_page_id\n\n        UNION ALL\n\n        -- Connected pages (hop 1+)\n        SELECT \n            COALESCE(e.DST_PAGE_ID, 'MISSING_CH' || e.DST_CHAPTER_NUMBER || '_P' || LPAD(e.DST_PAGE_NUMBER, 4, '0')) as dest_page_id,\n            e.DST_CHAPTER_NUMBER as dest_chapter_number,\n            e.DST_PAGE_NUMBER as dest_page_number,\n            COALESCE(dp.ENHANCED_CITATION, 'Referenced: Chapter ' || e.DST_CHAPTER_NUMBER || ', Page ' || e.DST_PAGE_NUMBER) as enhanced_citation,\n            ARRAY_APPEND(pt.connection_path, e.REFERENCE_EXPLANATION || ' (' || e.REFERENCE_CONTEXT || ')') as connection_path,\n            pt.hop_count + 1\n        FROM page_traversal pt\n        JOIN DOCUMENT_EDGES e ON pt.dest_page_id = e.SRC_PAGE_ID\n        LEFT JOIN DOCUMENT_PAGES dp ON e.DST_PAGE_ID = dp.PAGE_ID\n        WHERE pt.hop_count < max_hops\n          AND e.CONFIDENCE_SCORE > 0.5\n    )\n    SELECT \n        ARRAY_AGG(OBJECT_CONSTRUCT(*))\n    FROM page_traversal \n    WHERE hop_count > 0  -- Exclude starting page\n    ORDER BY hop_count, dest_chapter_number, dest_page_number\n$$;"
  },
  {
   "cell_type": "markdown",
   "id": "b41769bf-faaa-4227-a99f-dcae6ace06f4",
   "metadata": {
    "name": "end_document_ingestion_md"
   },
   "source": "# End document ingestion and analysis"
  },
  {
   "cell_type": "markdown",
   "id": "02447845-b977-4efe-9734-bcef76cde43c",
   "metadata": {
    "name": "begin_school_data_md",
    "collapsed": false
   },
   "source": "# Setup School District Data\n\nThis section creates a new schema, `SCHOOLS`, for all data related to cities, schools, teachers, tests, test questions, etc."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5712f7d-3600-4e16-afb8-c694d29bd6fe",
   "metadata": {
    "name": "select_schema",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "USE DATABASE WORLD_HISTORY;\n",
    "CREATE SCHEMA IF NOT EXISTS SCHOOLS;\n",
    "USE SCHEMA SCHOOLS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b3f222-ccad-417b-b38e-39c24772cade",
   "metadata": {
    "name": "create_school_data_tables",
    "language": "sql"
   },
   "outputs": [],
   "source": "-- =====================================================================================\n-- SCHOOL DISTRICTS DATABASE - NORMALIZED DESIGN\n-- Create normalized tables for 5 biggest US cities school district data\n-- =====================================================================================\n\n-- School district data will be created directly in the public schema (no separate stage needed)\n\n-- =====================================================================================\n-- NORMALIZED TABLE STRUCTURES\n-- =====================================================================================\n\n-- 1. CITIES TABLE\n-- Contains the 5 biggest US cities by population\nCREATE OR REPLACE TABLE cities (\n    city_id INTEGER,\n    city_name VARCHAR(100),\n    state VARCHAR(50),\n    population INTEGER,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n);\n\n-- 2. SCHOOL DISTRICTS TABLE  \n-- Contains the largest school district in each city\nCREATE OR REPLACE TABLE school_districts (\n    district_id INTEGER,\n    district_name VARCHAR(200),\n    city_id INTEGER,\n    total_students INTEGER,\n    total_schools INTEGER,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n);\n\n-- 3. HIGH SCHOOLS TABLE\n-- Contains 3 high schools from each district (15 total)\nCREATE OR REPLACE TABLE high_schools (\n    school_id INTEGER,\n    school_name VARCHAR(200),\n    district_id INTEGER,\n    school_type VARCHAR(50), -- Regular, Magnet, Charter, etc.\n    enrollment INTEGER,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n);\n\n-- 4. TEACHERS TABLE\n-- Contains 3 world history teachers per school (45 total)\nCREATE OR REPLACE TABLE teachers (\n    teacher_id INTEGER,\n    teacher_name VARCHAR(100),\n    school_id INTEGER,\n    subject VARCHAR(50),\n    years_experience INTEGER,\n    education_level VARCHAR(50),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n);\n\n-- 5. CLASSES TABLE\n-- Contains 3 world history classes per school (45 total)\nCREATE OR REPLACE TABLE classes (\n    class_id INTEGER,\n    class_name VARCHAR(100),\n    teacher_id INTEGER,\n    school_id INTEGER,\n    period INTEGER,\n    room_number VARCHAR(20),\n    max_capacity INTEGER,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n);\n\n-- 6. STUDENTS TABLE\n-- Contains 30 students per class (1,350 total students)\nCREATE OR REPLACE TABLE students (\n    student_id INTEGER,\n    student_name VARCHAR(100),\n    class_id INTEGER,\n    grade_level INTEGER,\n    age INTEGER,\n    gender VARCHAR(10),\n    enrollment_date DATE,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n);\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3adf6d-8b01-4bf0-a984-86e606ac5a5a",
   "metadata": {
    "name": "insert_school_mock_data",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "\n",
    "-- =====================================================================================\n",
    "-- DATA INSERTION\n",
    "-- =====================================================================================\n",
    "\n",
    "-- Insert Cities Data\n",
    "INSERT INTO cities VALUES\n",
    "(1, 'New York City', 'New York', 8500000, CURRENT_TIMESTAMP()),\n",
    "(2, 'Los Angeles', 'California', 3900000, CURRENT_TIMESTAMP()),\n",
    "(3, 'Chicago', 'Illinois', 2700000, CURRENT_TIMESTAMP()),\n",
    "(4, 'Houston', 'Texas', 2300000, CURRENT_TIMESTAMP()),\n",
    "(5, 'Phoenix', 'Arizona', 1700000, CURRENT_TIMESTAMP());\n",
    "\n",
    "-- Insert School Districts Data\n",
    "INSERT INTO school_districts VALUES\n",
    "(1, 'New York City Department of Education', 1, 1100000, 1800, CURRENT_TIMESTAMP()),\n",
    "(2, 'Los Angeles Unified School District', 2, 565000, 1300, CURRENT_TIMESTAMP()),\n",
    "(3, 'Chicago Public Schools', 3, 330000, 650, CURRENT_TIMESTAMP()),\n",
    "(4, 'Houston Independent School District', 4, 190000, 280, CURRENT_TIMESTAMP()),\n",
    "(5, 'Phoenix Union High School District', 5, 28000, 22, CURRENT_TIMESTAMP());\n",
    "\n",
    "-- Insert High Schools Data (3 per district)\n",
    "INSERT INTO high_schools VALUES\n",
    "-- NYC Schools\n",
    "(1, 'Stuyvesant High School', 1, 'Specialized', 3300, CURRENT_TIMESTAMP()),\n",
    "(2, 'Bronx High School of Science', 1, 'Specialized', 3000, CURRENT_TIMESTAMP()),\n",
    "(3, 'Brooklyn Technical High School', 1, 'Specialized', 5400, CURRENT_TIMESTAMP()),\n",
    "\n",
    "-- LA Schools  \n",
    "(4, 'Garfield High School', 2, 'Regular', 2800, CURRENT_TIMESTAMP()),\n",
    "(5, 'Hollywood High School', 2, 'Magnet', 2100, CURRENT_TIMESTAMP()),\n",
    "(6, 'Lincoln High School', 2, 'Regular', 3200, CURRENT_TIMESTAMP()),\n",
    "\n",
    "-- Chicago Schools\n",
    "(7, 'Whitney Young Magnet High School', 3, 'Selective Enrollment', 2200, CURRENT_TIMESTAMP()),\n",
    "(8, 'Lane Tech College Prep High School', 3, 'Selective Enrollment', 4400, CURRENT_TIMESTAMP()),\n",
    "(9, 'Jones College Prep High School', 3, 'Selective Enrollment', 1800, CURRENT_TIMESTAMP()),\n",
    "\n",
    "-- Houston Schools\n",
    "(10, 'Bellaire High School', 4, 'Regular', 3600, CURRENT_TIMESTAMP()),\n",
    "(11, 'Lamar High School', 4, 'Regular', 3100, CURRENT_TIMESTAMP()),\n",
    "(12, 'Carnegie Vanguard High School', 4, 'Magnet', 1200, CURRENT_TIMESTAMP()),\n",
    "\n",
    "-- Phoenix Schools\n",
    "(13, 'Central High School', 5, 'Regular', 1800, CURRENT_TIMESTAMP()),\n",
    "(14, 'Skyline High School', 5, 'Regular', 2400, CURRENT_TIMESTAMP()),\n",
    "(15, 'Peoria High School', 5, 'Regular', 2900, CURRENT_TIMESTAMP());\n",
    "\n",
    "-- Insert Teachers Data (3 world history teachers per school)\n",
    "INSERT INTO teachers VALUES\n",
    "-- NYC Teachers (Schools 1-3)\n",
    "(1, 'Sarah Johnson', 1, 'World History', 12, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(2, 'Michael Chen', 1, 'World History', 8, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(3, 'Rebecca Martinez', 1, 'World History', 15, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(4, 'David Thompson', 2, 'World History', 10, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(5, 'Lisa Wang', 2, 'World History', 6, 'Bachelors', CURRENT_TIMESTAMP()),\n",
    "(6, 'James Rodriguez', 2, 'World History', 14, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(7, 'Amanda Foster', 3, 'World History', 9, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(8, 'Robert Kim', 3, 'World History', 11, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(9, 'Jennifer Lopez', 3, 'World History', 7, 'Bachelors', CURRENT_TIMESTAMP()),\n",
    "\n",
    "-- LA Teachers (Schools 4-6)\n",
    "(10, 'Carlos Gutierrez', 4, 'World History', 13, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(11, 'Michelle Davis', 4, 'World History', 5, 'Bachelors', CURRENT_TIMESTAMP()),\n",
    "(12, 'Anthony Wilson', 4, 'World History', 16, 'Doctorate', CURRENT_TIMESTAMP()),\n",
    "(13, 'Maria Hernandez', 5, 'World History', 8, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(14, 'Kevin Park', 5, 'World History', 12, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(15, 'Rachel Green', 5, 'World History', 4, 'Bachelors', CURRENT_TIMESTAMP()),\n",
    "(16, 'Daniel Lee', 6, 'World History', 10, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(17, 'Nicole Brown', 6, 'World History', 14, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(18, 'Steven Garcia', 6, 'World History', 7, 'Bachelors', CURRENT_TIMESTAMP()),\n",
    "\n",
    "-- Chicago Teachers (Schools 7-9)\n",
    "(19, 'Emily Anderson', 7, 'World History', 11, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(20, 'Matthew Taylor', 7, 'World History', 9, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(21, 'Ashley Miller', 7, 'World History', 6, 'Bachelors', CURRENT_TIMESTAMP()),\n",
    "(22, 'Brian Jackson', 8, 'World History', 15, 'Doctorate', CURRENT_TIMESTAMP()),\n",
    "(23, 'Samantha White', 8, 'World History', 8, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(24, 'Christopher Moore', 8, 'World History', 12, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(25, 'Jessica Clark', 9, 'World History', 5, 'Bachelors', CURRENT_TIMESTAMP()),\n",
    "(26, 'Ryan Lewis', 9, 'World History', 13, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(27, 'Lauren Adams', 9, 'World History', 10, 'Masters', CURRENT_TIMESTAMP()),\n",
    "\n",
    "-- Houston Teachers (Schools 10-12)\n",
    "(28, 'Mark Turner', 10, 'World History', 14, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(29, 'Stephanie Phillips', 10, 'World History', 7, 'Bachelors', CURRENT_TIMESTAMP()),\n",
    "(30, 'Joseph Campbell', 10, 'World History', 11, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(31, 'Melissa Parker', 11, 'World History', 9, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(32, 'William Evans', 11, 'World History', 16, 'Doctorate', CURRENT_TIMESTAMP()),\n",
    "(33, 'Kimberly Scott', 11, 'World History', 6, 'Bachelors', CURRENT_TIMESTAMP()),\n",
    "(34, 'Thomas Roberts', 12, 'World History', 12, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(35, 'Heather Carter', 12, 'World History', 8, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(36, 'Jason Mitchell', 12, 'World History', 10, 'Masters', CURRENT_TIMESTAMP()),\n",
    "\n",
    "-- Phoenix Teachers (Schools 13-15)\n",
    "(37, 'Andrea Perez', 13, 'World History', 13, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(38, 'Gregory Hall', 13, 'World History', 5, 'Bachelors', CURRENT_TIMESTAMP()),\n",
    "(39, 'Vanessa Young', 13, 'World History', 15, 'Doctorate', CURRENT_TIMESTAMP()),\n",
    "(40, 'Scott Hernandez', 14, 'World History', 9, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(41, 'Brittany King', 14, 'World History', 7, 'Bachelors', CURRENT_TIMESTAMP()),\n",
    "(42, 'Nathan Wright', 14, 'World History', 14, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(43, 'Crystal Lopez', 15, 'World History', 11, 'Masters', CURRENT_TIMESTAMP()),\n",
    "(44, 'Derek Hill', 15, 'World History', 6, 'Bachelors', CURRENT_TIMESTAMP()),\n",
    "(45, 'Tiffany Green', 15, 'World History', 12, 'Masters', CURRENT_TIMESTAMP());\n",
    "\n",
    "-- Insert Classes Data (3 world history classes per school, 1 per teacher)\n",
    "INSERT INTO classes VALUES\n",
    "-- NYC Classes (Schools 1-3)\n",
    "(1, 'World History Period 1', 1, 1, 1, 'A101', 30, CURRENT_TIMESTAMP()),\n",
    "(2, 'World History Period 3', 2, 1, 3, 'A102', 30, CURRENT_TIMESTAMP()),\n",
    "(3, 'World History Period 5', 3, 1, 5, 'A103', 30, CURRENT_TIMESTAMP()),\n",
    "(4, 'World History Period 2', 4, 2, 2, 'B201', 30, CURRENT_TIMESTAMP()),\n",
    "(5, 'World History Period 4', 5, 2, 4, 'B202', 30, CURRENT_TIMESTAMP()),\n",
    "(6, 'World History Period 6', 6, 2, 6, 'B203', 30, CURRENT_TIMESTAMP()),\n",
    "(7, 'World History Period 1', 7, 3, 1, 'C301', 30, CURRENT_TIMESTAMP()),\n",
    "(8, 'World History Period 3', 8, 3, 3, 'C302', 30, CURRENT_TIMESTAMP()),\n",
    "(9, 'World History Period 5', 9, 3, 5, 'C303', 30, CURRENT_TIMESTAMP()),\n",
    "\n",
    "-- LA Classes (Schools 4-6)\n",
    "(10, 'World History Period 2', 10, 4, 2, 'D401', 30, CURRENT_TIMESTAMP()),\n",
    "(11, 'World History Period 4', 11, 4, 4, 'D402', 30, CURRENT_TIMESTAMP()),\n",
    "(12, 'World History Period 6', 12, 4, 6, 'D403', 30, CURRENT_TIMESTAMP()),\n",
    "(13, 'World History Period 1', 13, 5, 1, 'E501', 30, CURRENT_TIMESTAMP()),\n",
    "(14, 'World History Period 3', 14, 5, 3, 'E502', 30, CURRENT_TIMESTAMP()),\n",
    "(15, 'World History Period 5', 15, 5, 5, 'E503', 30, CURRENT_TIMESTAMP()),\n",
    "(16, 'World History Period 2', 16, 6, 2, 'F601', 30, CURRENT_TIMESTAMP()),\n",
    "(17, 'World History Period 4', 17, 6, 4, 'F602', 30, CURRENT_TIMESTAMP()),\n",
    "(18, 'World History Period 6', 18, 6, 6, 'F603', 30, CURRENT_TIMESTAMP()),\n",
    "\n",
    "-- Chicago Classes (Schools 7-9)\n",
    "(19, 'World History Period 1', 19, 7, 1, 'G701', 30, CURRENT_TIMESTAMP()),\n",
    "(20, 'World History Period 3', 20, 7, 3, 'G702', 30, CURRENT_TIMESTAMP()),\n",
    "(21, 'World History Period 5', 21, 7, 5, 'G703', 30, CURRENT_TIMESTAMP()),\n",
    "(22, 'World History Period 2', 22, 8, 2, 'H801', 30, CURRENT_TIMESTAMP()),\n",
    "(23, 'World History Period 4', 23, 8, 4, 'H802', 30, CURRENT_TIMESTAMP()),\n",
    "(24, 'World History Period 6', 24, 8, 6, 'H803', 30, CURRENT_TIMESTAMP()),\n",
    "(25, 'World History Period 1', 25, 9, 1, 'I901', 30, CURRENT_TIMESTAMP()),\n",
    "(26, 'World History Period 3', 26, 9, 3, 'I902', 30, CURRENT_TIMESTAMP()),\n",
    "(27, 'World History Period 5', 27, 9, 5, 'I903', 30, CURRENT_TIMESTAMP()),\n",
    "\n",
    "-- Houston Classes (Schools 10-12)\n",
    "(28, 'World History Period 2', 28, 10, 2, 'J1001', 30, CURRENT_TIMESTAMP()),\n",
    "(29, 'World History Period 4', 29, 10, 4, 'J1002', 30, CURRENT_TIMESTAMP()),\n",
    "(30, 'World History Period 6', 30, 10, 6, 'J1003', 30, CURRENT_TIMESTAMP()),\n",
    "(31, 'World History Period 1', 31, 11, 1, 'K1101', 30, CURRENT_TIMESTAMP()),\n",
    "(32, 'World History Period 3', 32, 11, 3, 'K1102', 30, CURRENT_TIMESTAMP()),\n",
    "(33, 'World History Period 5', 33, 11, 5, 'K1103', 30, CURRENT_TIMESTAMP()),\n",
    "(34, 'World History Period 2', 34, 12, 2, 'L1201', 30, CURRENT_TIMESTAMP()),\n",
    "(35, 'World History Period 4', 35, 12, 4, 'L1202', 30, CURRENT_TIMESTAMP()),\n",
    "(36, 'World History Period 6', 36, 12, 6, 'L1203', 30, CURRENT_TIMESTAMP()),\n",
    "\n",
    "-- Phoenix Classes (Schools 13-15)\n",
    "(37, 'World History Period 1', 37, 13, 1, 'M1301', 30, CURRENT_TIMESTAMP()),\n",
    "(38, 'World History Period 3', 38, 13, 3, 'M1302', 30, CURRENT_TIMESTAMP()),\n",
    "(39, 'World History Period 5', 39, 13, 5, 'M1303', 30, CURRENT_TIMESTAMP()),\n",
    "(40, 'World History Period 2', 40, 14, 2, 'N1401', 30, CURRENT_TIMESTAMP()),\n",
    "(41, 'World History Period 4', 41, 14, 4, 'N1402', 30, CURRENT_TIMESTAMP()),\n",
    "(42, 'World History Period 6', 42, 14, 6, 'N1403', 30, CURRENT_TIMESTAMP()),\n",
    "(43, 'World History Period 1', 43, 15, 1, 'O1501', 30, CURRENT_TIMESTAMP()),\n",
    "(44, 'World History Period 3', 44, 15, 3, 'O1502', 30, CURRENT_TIMESTAMP()),\n",
    "(45, 'World History Period 5', 45, 15, 5, 'O1503', 30, CURRENT_TIMESTAMP());\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f050271b-55d7-4a04-89fb-619a2a87584e",
   "metadata": {
    "name": "generate_student_data",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "\n",
    "-- =====================================================================================\n",
    "-- GENERATE STUDENT DATA \n",
    "-- Note: This is a partial sample - the full dataset would have 30 students per class\n",
    "-- =====================================================================================\n",
    "\n",
    "-- Generate all 1,350 students with systematic unique naming pattern\n",
    "-- 45 classes √ó 30 students per class = 1,350 total students\n",
    "\n",
    "INSERT INTO students (student_id, student_name, class_id, grade_level, age, gender, enrollment_date, created_at)\n",
    "WITH student_generator AS (\n",
    "    SELECT \n",
    "        c.class_id,\n",
    "        sd.district_id,\n",
    "        hs.school_id,\n",
    "        ROW_NUMBER() OVER (ORDER BY c.class_id) as class_seq,\n",
    "        sd.district_name,\n",
    "        hs.school_name\n",
    "    FROM classes c\n",
    "    JOIN high_schools hs ON c.school_id = hs.school_id\n",
    "    JOIN school_districts sd ON hs.district_id = sd.district_id\n",
    "),\n",
    "student_numbers AS (\n",
    "    SELECT \n",
    "        SEQ4() as student_seq\n",
    "    FROM TABLE(GENERATOR(ROWCOUNT => 30))\n",
    "),\n",
    "all_students AS (\n",
    "    SELECT \n",
    "        ROW_NUMBER() OVER (ORDER BY sg.class_id, sn.student_seq) as student_id,\n",
    "        'Student_' || ROW_NUMBER() OVER (ORDER BY sg.class_id, sn.student_seq) || \n",
    "        '_D' || sg.district_id || \n",
    "        '_S' || sg.school_id || \n",
    "        '_C' || sg.class_id as student_name,\n",
    "        sg.class_id,\n",
    "        CASE WHEN UNIFORM(1,4,RANDOM()) = 1 THEN 9\n",
    "             WHEN UNIFORM(1,4,RANDOM()) = 2 THEN 10  \n",
    "             WHEN UNIFORM(1,4,RANDOM()) = 3 THEN 11\n",
    "             ELSE 12 END as grade_level,\n",
    "        CASE WHEN UNIFORM(1,5,RANDOM()) = 1 THEN 14\n",
    "             WHEN UNIFORM(1,5,RANDOM()) = 2 THEN 15\n",
    "             WHEN UNIFORM(1,5,RANDOM()) = 3 THEN 16\n",
    "             WHEN UNIFORM(1,5,RANDOM()) = 4 THEN 17\n",
    "             ELSE 18 END as age,\n",
    "        CASE WHEN UNIFORM(1,2,RANDOM()) = 1 THEN 'Male' ELSE 'Female' END as gender,\n",
    "        '2024-09-01'::DATE as enrollment_date\n",
    "    FROM student_generator sg\n",
    "    CROSS JOIN student_numbers sn\n",
    ")\n",
    "SELECT \n",
    "    student_id,\n",
    "    student_name, \n",
    "    class_id,\n",
    "    grade_level,\n",
    "    age,\n",
    "    gender,\n",
    "    enrollment_date,\n",
    "    CURRENT_TIMESTAMP()\n",
    "FROM all_students;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a49af7-45bc-42ab-8965-c7273d968fff",
   "metadata": {
    "name": "student_school_complete_vw",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "\n",
    "-- =====================================================================================\n",
    "-- CREATE DENORMALIZED VIEW\n",
    "-- Single view that joins all tables to show complete student hierarchy\n",
    "-- =====================================================================================\n",
    "\n",
    "CREATE OR REPLACE VIEW student_school_complete_vw AS\n",
    "SELECT \n",
    "    -- Student Information\n",
    "    s.student_id,\n",
    "    s.student_name,\n",
    "    s.grade_level,\n",
    "    s.age,\n",
    "    s.gender,\n",
    "    s.enrollment_date,\n",
    "    \n",
    "    -- Class Information\n",
    "    c.class_id,\n",
    "    c.class_name,\n",
    "    c.period,\n",
    "    c.room_number,\n",
    "    c.max_capacity,\n",
    "    \n",
    "    -- Teacher Information  \n",
    "    t.teacher_id,\n",
    "    t.teacher_name,\n",
    "    t.subject,\n",
    "    t.years_experience,\n",
    "    t.education_level,\n",
    "    \n",
    "    -- School Information\n",
    "    hs.school_id,\n",
    "    hs.school_name,\n",
    "    hs.school_type,\n",
    "    hs.enrollment AS school_enrollment,\n",
    "    \n",
    "    -- District Information\n",
    "    sd.district_id,\n",
    "    sd.district_name,\n",
    "    sd.total_students AS district_total_students,\n",
    "    sd.total_schools AS district_total_schools,\n",
    "    \n",
    "    -- City Information\n",
    "    ct.city_id,\n",
    "    ct.city_name,\n",
    "    ct.state,\n",
    "    ct.population AS city_population\n",
    "    \n",
    "FROM students s\n",
    "JOIN classes c ON s.class_id = c.class_id\n",
    "JOIN teachers t ON c.teacher_id = t.teacher_id\n",
    "JOIN high_schools hs ON c.school_id = hs.school_id\n",
    "JOIN school_districts sd ON hs.district_id = sd.district_id\n",
    "JOIN cities ct ON sd.city_id = ct.city_id;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e762bf3a-60f1-4782-9b47-2b37931e658d",
   "metadata": {
    "name": "create_test_tables",
    "language": "sql"
   },
   "outputs": [],
   "source": "-- Create tables specifically for the test schedule, questions, and responses\nCREATE TABLE IF NOT EXISTS WORLD_HISTORY_TEST_QUESTIONS (\n    QUESTION_ID NUMBER AUTOINCREMENT PRIMARY KEY,\n    FILEPATH STRING,\n    CHAPTER_NUMBER NUMBER,\n    PAGE_NUMBER NUMBER,\n    QUESTION_NUMBER NUMBER,\n    QUESTION_TEXT STRING,\n    OPTION_A STRING,\n    OPTION_B STRING,\n    OPTION_C STRING,\n    OPTION_D STRING,\n    CORRECT_ANSWER STRING,\n    DIFFICULTY_LEVEL STRING,\n    TOPIC STRING,\n    CREATED_TIMESTAMP TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n);\n\nCREATE TABLE IF NOT EXISTS TEST_SCHEDULE (\n    SCHEDULE_ID NUMBER AUTOINCREMENT,\n    CLASS_ID NUMBER,\n    TEACHER_ID NUMBER,\n    SCHOOL_ID NUMBER,\n    CHAPTER_NUMBER NUMBER,\n    TEST_ID NUMBER, -- Will link to CHAPTER_TESTS when tests are generated\n    SCHEDULED_DATE DATE,\n    SCHEDULED_WEEK NUMBER, -- Week number of academic year (1-40)\n    ACADEMIC_YEAR VARCHAR(20) DEFAULT '2024-2025',\n    TEST_STATUS VARCHAR(20) DEFAULT 'SCHEDULED',\n    CREATED_TIMESTAMP TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n);\n\nCREATE TABLE IF NOT EXISTS STUDENT_QUESTION_RESPONSES (\n    RESPONSE_ID INTEGER AUTOINCREMENT PRIMARY KEY,\n    TEST_RESULT_ID INTEGER,\n    QUESTION_ID INTEGER,\n    STUDENT_ID INTEGER,\n    STUDENT_ANSWER CHAR(1),\n    CORRECT_ANSWER CHAR(1),\n    IS_CORRECT BOOLEAN,\n    RESPONSE_TIME_SECONDS INTEGER,\n    CHAPTER_NUMBER INTEGER,\n    DIFFICULTY_LEVEL VARCHAR(20),\n    TOPIC VARCHAR(200),\n    CREATED_AT TIMESTAMP DEFAULT CURRENT_TIMESTAMP()\n);\n\n-- Doesn't need to be a Dynamic Table for the demo, but this showcases how it could be used in a real-world example\nCREATE DYNAMIC TABLE IF NOT EXISTS STUDENT_CHAPTER_TEST_RESULTS \n   TARGET_LAG = '10 minutes'\n   WAREHOUSE = WAREHOUSE_XL_G2\nAS\n    SELECT\n        STUDENT_ID,\n        CHAPTER_NUMBER,\n        COUNT(*) AS total_questions,\n        SUM(IFF(r.IS_CORRECT, 1, 0)) as correct_answers,\n        (correct_answers / total_questions) * 100 AS final_score_percent,\n        CASE \n        WHEN GREATEST(65, final_score_percent) >= 97 THEN 'A+'\n        WHEN GREATEST(65, final_score_percent) >= 93 THEN 'A'\n        WHEN GREATEST(65, final_score_percent) >= 90 THEN 'A-'\n        WHEN GREATEST(65, final_score_percent) >= 87 THEN 'B+'\n        WHEN GREATEST(65, final_score_percent) >= 83 THEN 'B'\n        WHEN GREATEST(65, final_score_percent) >= 80 THEN 'B-'\n        WHEN GREATEST(65, final_score_percent) >= 77 THEN 'C+'\n        WHEN GREATEST(65, final_score_percent) >= 73 THEN 'C'\n        WHEN GREATEST(65, final_score_percent) >= 70 THEN 'C-'\n        WHEN GREATEST(65, final_score_percent) >= 67 THEN 'D+'\n        WHEN GREATEST(65, final_score_percent) >= 65 THEN 'D'\n        ELSE 'F'\n    END as LETTER_GRADE\n    FROM\n        WORLD_HISTORY.SCHOOLS.STUDENT_QUESTION_RESPONSES r\n    GROUP BY\n        1, 2\n    ORDER BY\n        STUDENT_ID, CHAPTER_NUMBER;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a6f7fb-f8dd-4726-bf49-69c8b1992f4e",
   "metadata": {
    "name": "test_schedule_data",
    "language": "sql"
   },
   "outputs": [],
   "source": "-- Mock up a school calendar for date based analysis\nCREATE OR REPLACE TEMPORARY TABLE academic_calendar AS\nWITH date_series AS (\n    SELECT \n        DATEADD(WEEK, ROW_NUMBER() OVER (ORDER BY NULL) - 1, '2025-08-05'::DATE) as week_start_date,\n        ROW_NUMBER() OVER (ORDER BY NULL) as week_number\n    FROM TABLE(GENERATOR(ROWCOUNT => 45)) -- Generate 45 weeks to cover full academic year\n),\nacademic_weeks AS (\n    SELECT \n        week_start_date,\n        week_number,\n        MONTH(week_start_date) as month_num,\n        CASE \n            WHEN week_start_date BETWEEN '2024-12-23' AND '2025-01-06' THEN 'Winter Break'\n            WHEN week_start_date BETWEEN '2025-03-10' AND '2025-03-17' THEN 'Spring Break'\n            WHEN week_start_date BETWEEN '2025-11-25' AND '2025-11-29' THEN 'Thanksgiving Break'\n            WHEN DAYOFWEEK(week_start_date) IN (1, 7) THEN 'Weekend'\n            ELSE 'Regular School Week'\n        END as week_type\n    FROM date_series\n    WHERE week_start_date <= '2026-06-15' -- End of academic year\n)\nSELECT \n    week_number,\n    week_start_date,\n    week_type,\n    -- Adjust to use school days (Tuesday-Thursday for testing)\n    DATEADD(DAY, 2, week_start_date) as suggested_test_date -- Wednesday of each week\nFROM academic_weeks\nWHERE week_type = 'Regular School Week'\nORDER BY week_number;\n\n-- Show academic calendar\nSELECT \n    week_number,\n    week_start_date,\n    suggested_test_date,\n    week_type\nFROM academic_calendar\nLIMIT 40;\n\n\nINSERT INTO TEST_SCHEDULE (\n    CLASS_ID, TEACHER_ID, SCHOOL_ID, CHAPTER_NUMBER, \n    SCHEDULED_DATE, SCHEDULED_WEEK\n)\nWITH all_classes AS (\n    SELECT \n        c.CLASS_ID,\n        c.TEACHER_ID,\n        c.SCHOOL_ID\n    FROM CLASSES c\n),\nchapter_sequence AS (\n    SELECT \n        ROW_NUMBER() OVER (ORDER BY NULL) as chapter_number\n    FROM TABLE(GENERATOR(ROWCOUNT => 32)) -- 32 chapters\n),\nclass_schedules AS (\n    SELECT \n        ac.CLASS_ID,\n        ac.TEACHER_ID,\n        ac.SCHOOL_ID,\n        cs.chapter_number,\n        -- Randomly assign starting week for each class, then sequential weekly spacing\n        LEAST(40, GREATEST(1, \n            (HASH(ac.CLASS_ID, ac.TEACHER_ID) % 8) + 1 + (cs.chapter_number - 1)\n        )) as assigned_week\n    FROM all_classes ac\n    CROSS JOIN chapter_sequence cs\n),\nscheduled_tests AS (\n    SELECT \n        cls.CLASS_ID,\n        cls.TEACHER_ID,\n        cls.SCHOOL_ID,\n        cls.chapter_number,\n        cls.assigned_week,\n        COALESCE(cal.suggested_test_date, \n                DATEADD(WEEK, cls.assigned_week - 1, '2024-08-07')\n        ) as scheduled_date\n    FROM class_schedules cls\n    LEFT JOIN academic_calendar cal ON cls.assigned_week = cal.week_number\n    WHERE cls.assigned_week <= 40 -- Ensure we don't exceed academic year\n)\nSELECT \n    CLASS_ID,\n    TEACHER_ID,\n    SCHOOL_ID,\n    chapter_number,\n    scheduled_date,\n    assigned_week\nFROM scheduled_tests\nWHERE scheduled_date IS NOT NULL;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112411dc-68e2-4d87-ba7e-b4927b8d4b44",
   "metadata": {
    "name": "extract_questions_fn",
    "language": "sql"
   },
   "outputs": [],
   "source": "-- Python based function to do a simple text extraction of review questions and answers\n-- Note: Originally tried this with AI_COMPLETE and PARSE_DOCUMENT but neither was sufficient.\n-- To do: ~8/21 a new version of PARSE_DOCUMENT will be going GA and should be much better; evaluate that after it's available\nCREATE OR REPLACE FUNCTION EXTRACT_QUESTIONS(\"STAGED_FILE_PATH\" VARCHAR)\nRETURNS VARCHAR\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.12'\nPACKAGES = ('snowflake-snowpark-python','pypdf')\nHANDLER = 'extract_questions'\nAS '\nimport re\nimport json\nfrom pypdf import PdfReader\nfrom snowflake.snowpark.files import SnowflakeFile\n\ndef extract_questions(staged_file_path: str) -> str:\n    \"\"\"\n    Extracts numbered questions from a PDF file stored in a Snowflake stage.\n\n    Args:\n        staged_file_path: The path to the PDF file within a Snowflake stage.\n\n    Returns:\n        A JSON string representing a list of question objects,\n        each with a ''number'' and ''text'' key.\n    \"\"\"\n    try:\n        with SnowflakeFile.open(staged_file_path, ''rb'') as f:\n            reader = PdfReader(f)\n            full_text = \"\".join(page.extract_text() or \"\" for page in reader.pages)\n        \n        # Modified regex:\n        # (\\\\d+)      - Capturing group 1: one or more digits (the question number).\n        # \\\\.\\\\s       - A literal period followed by a space.\n        # (.*?)      - Capturing group 2: the question text (non-greedy).\n        # (?=...)    - Positive lookahead to stop at the next number or end of string.\n        found_questions = re.findall(r\"(\\\\d+)\\\\.\\\\s(.*?)(?=\\\\d+\\\\.\\\\s|\\\\Z)\", full_text, re.DOTALL)\n        \n        # Format the list of tuples [(''1'', ''text...''), (''2'', ''text...'')] into a list of dicts.\n        questions_list = [\n            {\"number\": int(number), \"text\": text.strip()}\n            for number, text in found_questions\n        ]\n        \n        # Return the list as a JSON string.\n        return json.dumps(questions_list)\n\n    except Exception as e:\n        return f\"Error processing file: {staged_file_path}. Details: {str(e)}\"\n';"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7f37f9-7471-4e5d-8007-955caa0fe815",
   "metadata": {
    "name": "extract_test_questions",
    "language": "sql"
   },
   "outputs": [],
   "source": "-- Find each page with \"standardized test practice\" and construct a url to that page and the following 2 pages\n-- Call the `extract_questions` function on each page\n-- No insert statement here because the next cell will call the results\n\nWITH test_assessment_start_page AS (\n  SELECT\n    page_id,\n    chapter_number,\n    page_number\n  FROM\n    world_history.public.world_history_rag\n  WHERE\n    content_type = 'RawText' AND page_number IS NOT NULL AND text_content iLIKE '%STANDARDIZED TEST PRACTICE%' \n    -- and chapter_number like 23\n    -- limit 5\n), \nASSESSMENT_PAGES AS (\n    SELECT\n      f.value :: STRING AS file_path,\n      chapter_number,\n      page_number\n    FROM\n      test_assessment_start_page,\n      LATERAL FLATTEN(\n        input => ARRAY_CONSTRUCT(\n          '/pages/chap' || LPAD(chapter_number, 2, '0') || '_page' || LPAD(page_number, 4, '0') || '.pdf',\n          '/pages/chap' || LPAD(chapter_number, 2, '0') || '_page' || LPAD(page_number + 1, 4, '0') || '.pdf',\n          '/pages/chap' || LPAD(chapter_number, 2, '0') || '_page' || LPAD(page_number + 2, 4, '0') || '.pdf'\n        )\n      ) f\n)\n\nselect \n    ap.file_path,\n    extract_questions(BUILD_SCOPED_FILE_URL(@world_history.public.pdf_documents, ap.file_path)) as extracted_questions,\n    ap.chapter_number,\n    ap.page_number\nFROM\n    assessment_pages ap\nGROUP BY\n    1, 3, 4;\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afec84a-b655-4ec8-aff5-f7eb5d72a497",
   "metadata": {
    "name": "chunk_test_questions",
    "language": "sql"
   },
   "outputs": [],
   "source": "-- Flatten the questions retreived in the {{extract_test_questions}} cell\nSELECT\n        eqc.file_path,\n        eqc.page_number,\n        eqc.chapter_number,\n        f.value:number::INT % 100 AS question_number,\n        f.value:text::STRING AS question_text\n    FROM\n        {{extract_test_questions}} eqc,\n        LATERAL FLATTEN(input => PARSE_JSON(eqc.extracted_questions)) f\n    WHERE\n        -- This WHERE clause is now the main filter for finding MCQs.\n        -- It's more robust to do this here than in the Python UDF.\n        -- Using LIKE without periods is slightly more forgiving.\n        question_text LIKE '%A %'\n        AND question_text LIKE '%B %'\n        AND question_text LIKE '%C %'\n        AND question_text LIKE '%D %'"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9150d7ee-a755-47dc-9d7e-73ce9f40c18e",
   "metadata": {
    "name": "parse_test_questions",
    "language": "sql"
   },
   "outputs": [],
   "source": "-- AI_COMPLETE to format the questions from {{chunk_test_questions}} in JSON, \n--- select the correct answer, assign it a topic and difficulty level \nSELECT\n    ctq.*,\n       try_parse_json(AI_COMPLETE(\n        'mixtral-8x7b',\n        'You are formatting test questions that have been extracted from a World History textbook page. \n        \n        TASK: Extract ALL multiple choice test questions from this textbook page, format it properly and supply the right answer.\n        \n        REQUIREMENTS:\n        1. Extract ONLY multiple choice questions (ignore essay, short answer, fill-in-the-blank)\n        2. Each question must have exactly 4 answer choices (A, B, C, D)\n        3. If the correct answer is provided, include it; if not, research the answer to find it.  Do not leave an answer blank or null.  If you cannot find an answer put \"C\"\n        4. Extract the topic/concept being tested based on other questions in the input.\n        5. Return results in this EXACT JSON format:\n        \n        {\n          \"extracted_questions\": [\n            {\n              \"question_text\": \"Which type of scientist uses fossils and artifacts to study early humans?\",\n              \"option_a\": \"Chemists\",\n              \"option_b\": \"Physicists\", \n              \"option_c\": \"Anthropologists\",\n              \"option_d\": \"Geologists\",\n              \"correct_answer\": \"C\",\n              \"topic\": \"Early Human Study\",\n              \"difficulty_level\": \"Medium\"\n            }\n          ],\n          \"extraction_notes\": \"Found X questions on page\",\n          \"page_info\": {\n            \"chapter_number\": ' || ctq.chapter_number || ',\n            \"page_number\": ' || ctq.page_number || ',\n            \"contains_test_questions\": true\n          }\n        }\n        \n        IMPORTANT RULES:\n        - Return ONLY valid JSON, no other text\n        - Use \"Easy\", \"Medium\", or \"Hard\" for difficulty_level\n        - If no test questions found, return empty array for extracted_questions\n        - If questions are incomplete or don\\'t have 4 options, skip them\n        - If any answers are more than one choice (eg \"C, D\"), skip them.  \n        - Extract the question text exactly as written, EXCEPT extract the specific options to the option_a, option_b, option_c or option_d field. Do NOT include the letter for the option in the value field. Ie if two of the potential anwsners are \"A Conscription\\nB War communism\\n\" then the JSON should only include the word that would fill in the blank => {\"option_a\": \"Conscription\", \"option_b\": \"War communism\"} \n        - If there is a fill in the blank (missing word), replace the empty space with \"_____\".\n\n        EXAMPLE:\n        INPUT TEXT: \"is the process of assembling troops and supplies to \\nget ready for war. \\nA Conscription\\nB War communism\\nC Armistice\\nD Mobilization\\n\"\n        DESIRED OUTPUT:\n            {\n              \"correct_answer\": \"D\",\n              \"difficulty_level\": \"Easy\",\n              \"option_a\": \"Conscription\",\n              \"option_b\": \"War communism\",\n              \"option_c\": \"Armistice\",\n              \"option_d\": \"Mobilization\",\n              \"question_number\": 3,\n              \"question_text\": \"_____ is the process of assembling troops and supplies to get ready for war.\",\n              \"topic\": \"War Preparation\"\n            },\n        \n        PAGE CONTENT:\n        ' || ctq.question_text\n    )) AS extraction_result\nFROM {{chunk_test_questions}} ctq"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8217e91-4d84-4ee0-952d-f42f46ac737c",
   "metadata": {
    "name": "insert_world_history_test_questions",
    "language": "sql"
   },
   "outputs": [],
   "source": "-- Finally, parse the json and insert the questions from {{parse_test_questions}} into the world_history_test_questions table\n\nINSERT INTO WORLD_HISTORY_TEST_QUESTIONS(\n    FILEPATH,\n    CHAPTER_NUMBER,\n    PAGE_NUMBER,\n    QUESTION_NUMBER,\n    QUESTION_TEXT,\n    OPTION_A,\n    OPTION_B,\n    OPTION_C,\n    OPTION_D,\n    CORRECT_ANSWER,\n    DIFFICULTY_LEVEL,\n    TOPIC\n)\n\nSELECT\n ptq.file_path,\n ptq.chapter_number,\n ptq.page_number,\n ptq.question_number,\n f.value:question_text as question_text,\n f.value:option_a as option_a,\n f.value:option_b as option_b,\n f.value:option_c as option_c,\n f.value:option_d as option_d,\n coalesce(f.value:correct_answer, 'C') as correct_answer,\n f.value:difficulty_level as difficulty_level,\n f.value:topic as topic,\nFROM {{parse_test_questions}} ptq,\n   LATERAL FLATTEN(input => ptq.extraction_result:extracted_questions) f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce1b863-edba-4114-8a03-1b943491f971",
   "metadata": {
    "name": "insert_student_question_responses",
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- This single INSERT statement generates and inserts all the mock student responses.\n",
    "-- It is designed to work with your table where RESPONSE_ID is AUTOINCREMENT\n",
    "-- and CREATED_AT has a DEFAULT value.\n",
    "\n",
    "INSERT INTO WORLD_HISTORY.SCHOOLS.STUDENT_QUESTION_RESPONSES (\n",
    "    TEST_RESULT_ID,\n",
    "    QUESTION_ID,\n",
    "    STUDENT_ID,\n",
    "    STUDENT_ANSWER,\n",
    "    CORRECT_ANSWER,\n",
    "    IS_CORRECT,\n",
    "    RESPONSE_TIME_SECONDS,\n",
    "    CHAPTER_NUMBER,\n",
    "    DIFFICULTY_LEVEL,\n",
    "    TOPIC\n",
    ")\n",
    "WITH\n",
    "-- CTE 1: Get a distinct list of all chapters that have questions.\n",
    "ALL_CHAPTERS AS (\n",
    "    SELECT DISTINCT CHAPTER_NUMBER\n",
    "    FROM WORLD_HISTORY.SCHOOLS.WORLD_HISTORY_TEST_QUESTIONS\n",
    "),\n",
    "\n",
    "-- CTE 2: Assign a target score (65-100) for each student for each chapter.\n",
    "TARGET_SCORES AS (\n",
    "    SELECT\n",
    "        s.STUDENT_ID,\n",
    "        c.CHAPTER_NUMBER,\n",
    "        -- Generate a score from a normal distribution with mean=82.5, stddev=5\n",
    "        -- and clamp the result between 65 and 100.\n",
    "        LEAST(100.0, GREATEST(65.0, NORMAL(82.5, 5, RANDOM()))) AS TARGET_PERCENT_CORRECT\n",
    "    FROM\n",
    "        WORLD_HISTORY.SCHOOLS.STUDENTS s\n",
    "    CROSS JOIN\n",
    "        ALL_CHAPTERS c\n",
    "),\n",
    "\n",
    "-- CTE 3: Get all questions for each chapter and find the total question count.\n",
    "CHAPTER_QUESTIONS AS (\n",
    "    SELECT\n",
    "        CHAPTER_NUMBER,\n",
    "        QUESTION_ID,\n",
    "        CORRECT_ANSWER,\n",
    "        DIFFICULTY_LEVEL,\n",
    "        TOPIC,\n",
    "        COUNT(QUESTION_ID) OVER (PARTITION BY CHAPTER_NUMBER) AS TOTAL_QUESTIONS_IN_CHAPTER\n",
    "    FROM\n",
    "        WORLD_HISTORY.SCHOOLS.WORLD_HISTORY_TEST_QUESTIONS\n",
    "),\n",
    "\n",
    "-- CTE 4: Combine scores and questions, then randomly rank questions for each student's test.\n",
    "FINAL_RESPONSES AS (\n",
    "    SELECT\n",
    "        ts.STUDENT_ID,\n",
    "        cq.QUESTION_ID,\n",
    "        cq.CHAPTER_NUMBER,\n",
    "        cq.CORRECT_ANSWER,\n",
    "        cq.DIFFICULTY_LEVEL,\n",
    "        cq.TOPIC,\n",
    "        -- Assign a random rank to each question within a student's chapter test attempt\n",
    "        ROW_NUMBER() OVER (PARTITION BY ts.STUDENT_ID, ts.CHAPTER_NUMBER ORDER BY UNIFORM(0,1,RANDOM())) as random_rank,\n",
    "        -- Calculate the exact number of questions that should be correct based on the target score\n",
    "        ROUND(cq.TOTAL_QUESTIONS_IN_CHAPTER * ts.TARGET_PERCENT_CORRECT / 100) AS num_to_be_correct\n",
    "    FROM\n",
    "        TARGET_SCORES ts\n",
    "    JOIN\n",
    "        CHAPTER_QUESTIONS cq ON ts.CHAPTER_NUMBER = cq.CHAPTER_NUMBER\n",
    ")\n",
    "\n",
    "-- Final SELECT to format and insert the data\n",
    "SELECT\n",
    "    NULL AS TEST_RESULT_ID, -- Populated with NULL as it's a separate entity\n",
    "    fr.QUESTION_ID,\n",
    "    fr.STUDENT_ID,\n",
    "    -- If the answer is correct, student_answer is the correct answer. Otherwise, NULL.\n",
    "    IFF((fr.random_rank <= fr.num_to_be_correct), fr.CORRECT_ANSWER, NULL) AS STUDENT_ANSWER,\n",
    "    fr.CORRECT_ANSWER,\n",
    "    (fr.random_rank <= fr.num_to_be_correct) AS IS_CORRECT,\n",
    "    UNIFORM(15, 90, RANDOM()) AS RESPONSE_TIME_SECONDS, -- Random response time: 15-90s\n",
    "    fr.CHAPTER_NUMBER,\n",
    "    fr.DIFFICULTY_LEVEL,\n",
    "    fr.TOPIC\n",
    "FROM\n",
    "    FINAL_RESPONSES fr;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd04464-b847-44ac-847e-a6f7195340ee",
   "metadata": {
    "name": "update_incorrect_answers",
    "language": "sql"
   },
   "outputs": [],
   "source": "-- Fun with creating incorrect test question answers\nUPDATE\n  STUDENT_QUESTION_RESPONSES\nSET CORRECT_ANSWER = UPPER(CORRECT_ANSWER);\n\nUPDATE\n  STUDENT_QUESTION_RESPONSES\nSET\n  STUDENT_ANSWER = CASE UPPER(CORRECT_ANSWER)\n    WHEN 'A' THEN (\n      SELECT VALUE FROM TABLE(FLATTEN(INPUT => ['B', 'C', 'D']::ARRAY)) ORDER BY RANDOM() LIMIT 1\n    )\n    WHEN 'B' THEN (\n      SELECT VALUE FROM TABLE(FLATTEN(INPUT => ['A', 'C', 'D']::ARRAY)) ORDER BY RANDOM() LIMIT 1\n    )\n    WHEN 'C' THEN (\n      SELECT VALUE FROM TABLE(FLATTEN(INPUT => ['A', 'B', 'D']::ARRAY)) ORDER BY RANDOM() LIMIT 1\n    )\n    WHEN 'D' THEN (\n      SELECT VALUE FROM TABLE(FLATTEN(INPUT => ['A', 'B', 'C']::ARRAY)) ORDER BY RANDOM() LIMIT 1\n    )\n  END\n  WHERE\n  STUDENT_ANSWER IS NULL or STUDENT_ANSWER = '';"
  },
  {
   "cell_type": "markdown",
   "id": "c123cde4-9101-4ecb-9354-45b5fe97bfa7",
   "metadata": {
    "name": "end_school_data_md",
    "collapsed": false
   },
   "source": "# End school data section"
  },
  {
   "cell_type": "markdown",
   "id": "7ad379e3-7bb4-496e-8037-4a12181805ba",
   "metadata": {
    "name": "upload_semantic_model_md",
    "collapsed": false
   },
   "source": [
    "# Upload Semantic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfda665-8946-4b58-98cb-99edca849e6d",
   "metadata": {
    "name": "create_config_files_stage",
    "language": "sql"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE STAGE WORLD_HISTORY.PUBLIC.CONFIG_FILES\n    ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE')\n    DIRECTORY = (\n        ENABLE = TRUE\n        AUTO_REFRESH = TRUE\n    );"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c50c0c8-ee41-4629-bdcf-842995b681b7",
   "metadata": {
    "name": "upload_cortex_analyst",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "session = get_active_session()\n",
    "\n",
    "# 1. Define the path to your YAML file\n",
    "local_file_path = \"world_history_analytics.yaml\"\n",
    "\n",
    "# 2. Define the target stage\n",
    "target_stage = \"@WORLD_HISTORY.PUBLIC.CONFIG_FILES\"\n",
    "\n",
    "# 3. Use the put command to upload the file\n",
    "put_result = session.file.put(local_file_path, target_stage, auto_compress= False, overwrite=True)\n",
    "\n",
    "# Print the result to confirm the upload\n",
    "print(f\"File uploaded successfully: {put_result[0].target} {put_result[0].status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b883b575-d7d7-4e1f-b07c-068e050813bc",
   "metadata": {
    "name": "finish_upload_semantic_model",
    "collapsed": false
   },
   "source": "# End Upload Semantic Model\n\nThe semantic model for Cortex Analyst will be available in `@WORLD_HISTORY.PUBLIC.CONFIG_FILES`"
  },
  {
   "cell_type": "markdown",
   "id": "b7b66ff0-d9fe-4eae-b31a-de7ba31e3f25",
   "metadata": {
    "name": "notebook_complete",
    "collapsed": false
   },
   "source": "# ‚úÖ All Done!  (Almost)\n\nEverything except the Snowflake Agent has been created.  The following are the same instructions in the `Instructions` cell.  \n\n# Agent Setup\nThis is how you can setup an Agent to use all of the services and tools to come up with accurate answers to complicated questions.\n\n## About\n- Display Name: World History Agent\n- Description: This agent has access to both information about schools, tests, grades, test questions, student responses (structured data) and a World History Textbook (unstructured data).  The content is related in the fact that the exams and questions and responses are based on the content from the World History Textbook.  Anyone can ask questions about content in the textbook, relate that back to student performance, and seamlessly use the agent to go back and forth between the different modalities.\n\n## Instructions\n- Response Instructions: Show any percentages as 0.00%.  If the question isn't extremely clear, ask for clarification.  You are an expert professor in World History.  You have the knowledge of 1060 pages of World History and access to student exam performance data.  Your keen observations, suggestions and insights will be highly prized.  Don't be afraid to make suggestions for how tests can be improved or how individual teachers, or schools, can teach the content differently.  \n- Sample Questions:\n    - Which is the first page that has references to other pages about the enlightment. What pages does it reference and what content is on those pages?\n    - I want to compare the military of Classical Athens to that of the late Roman Republic. Your primary method for finding the Roman comparison must be to execute a search for explicit connections starting from the pages discussing the Athenian military during the Persian Wars. First, summarize the Athenian model, then use the connection-finding tool to locate the relevant Roman content and provide the comparison.\n    - Analyze the policy of 'War Communism' implemented by the Bolsheviks during the Russian Civil War. First, summarize the policy's immediate historical context. Then, trace its ideological foundation by following any explicit cross-references in that chapter back to the introduction of Marxist theory earlier in the textbook.\n    - Compare the citizen-soldier model of Classical Athens during the Persian Wars with the professionalized army of the late Roman Republic. Begin by finding the section on the Persian Wars, summarize the chapter's discussion of the Athenian state and its military, and then use any direct textual cross-references to locate and analyze the author's comparison with the Roman military system.\n    - What were the hardest questions about the Roman Empire, which answer was chosen wrong the most, and what pages should students study to get more familiar with the content?\n    - How closely does the exam for Emerging Europe and the Byzantine Empire follow the textbook material?\n\n## Tools\n- Cortex Analyst: Add the World_History.public.config_files/world_history_semantic_model.yaml.  Let Cortex create the description.\n- Cortex Search: Add WORLD_HISTORY_QA.PUBLIC.WORLD_HISTORY_RAG_SEARCH\n   - Description: Returns vector based searches on the world history returning either pages, parts, page summaries, part summaries, or chapter summaries.\n   - ID Column: PDF_URL\n   - Title Column: ENHANCED_CITATION\n\n- Custom Tools\n    - Multihop_Search_Results.  Add WORLD_HISTORY.PUBLIC.MULTIHOP_SEARCH_RESULTS as a function.  \n        - page_id_param description: This is the page_id param that needs to be passed in the format of CHxx_Pyyyy.  Example: SELECT WORLD_HISTORY_QA.PUBLIC.MULTIHOP_SEARCH_RESULTS_FN('CH23_P0777');\n        - description: Use this tool to enrich context for a known page ID. When you have a specific page from a vector search, use this tool to retrieve the page summary, part summary, and chapter summary. This is best for answering questions about the broader theme, context, or significance of information found on a specific page.  This tool returns the connected pages (hops) for references.  It should be used to find if there are any connected edges.  Then move to the find_connected_edges tool to recursively follow those edges in the knowledge graph.\n    -Find_Connected_Edges. Add WORLD_HISTORY.PUBLIC.FIND_CONNECTED_PAGES as a function.\n        - max_hops description: This is the number of connections from the source page.  If the source page is page 10 and has a reference to page 20, that would be the first hop.  If page 20 has a reference to page 30 that's the 2nd hop.  Default to 2.\n        - starting_page_id description: This is the starting page id in the format \"CHxx_Pyyyy\".  Example \"CH23_P0772\".  It is a combination chapter and page number that we will get from the prior steps.\n        - description: Always use this tool _after_ multihop_search_results.  That tool tells you _if_ there are connected graph edges.  This tool then allows you to recursively follow the relationships of the material.  Use this tool to answer questions about explicit connections, direct links, or tracing a topic's influence across the textbook. It traverses the book's graph of 'see page...' cross-references. Prioritize this tool when a user asks to 'trace the origins of,' 'find the connection to,' 'see what this is linked to,' or analyze how the author explicitly compares two disparate topics.\n\n## Orchestration\nPlanning instructions\n```\nStep 1: Question Routing üö¶\nThe router should prioritize tools in order from most specialized to most general.\n\nIs the user asking to trace a connection or find an explicit link? (e.g., using words like \"trace,\" \"connect,\" \"link,\" \"cross-reference,\" \"compare to what the author links\").\n\nIf yes, prioritize the Multihop_Search_Results + Find_Connected_Pages tool path.  \n\nIs the user asking for the summary, context, or significance of a known topic? (e.g., \"Summarize the chapter on the Persian Wars\").\n\nIf yes, use the Cortex Search + Multihop_Search_Results Path.\n\nIs it a general knowledge question about the text? (e.g., \"Tell me about the Roman military\").\n\nIf yes, use the standard Cortex Search Path, potentially enriched with Multihop_Search_Results.\n\nIs it a question about structured data?\n\nIf yes, use the Cortex Analyst Path.\n\n-- ANY TIME the Multihop_Search_Results comes back with connected_pages to get more information.\n```"
  }
 ]
}